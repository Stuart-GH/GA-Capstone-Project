# Capstone Project
#### Summary
In this notebook I investigate whether categorising music based on language difficulty could be used to facilitate non-native language learning. I scraped song meta data on 50k songs from Spotify and corresponding lyrics from the QQ Music API. I applied a range of statistcal modelling techniques to the data as a practical application of the material covered on this course. 


## 1. Virtual Environment & Installing Packages

### 1.0 Virtual Environment

![image.png](attachment:image.png)

#### In terminal:
- Go to project directory and run: python3 -m venv env
- Note 'env' is name of sub folder where virtual environment will be located
- Activate the virtual environment: source env/bin/activate
- Confirm virtual environment is active by checking location of Python interpreter: source env/bin/activate
- It should be in the env directory
- To leave virtual environment: dactivate
- Set up new Kernel which uses env for packages: python3 -m ipykernel install --user --name=env
- Select env Kernel in Jupyter Interface

#### Activate virtual environment in terminal:

Note will see project name on left side of prompt:
((Spoty-Linguist-Project) ) (base) stuart@Cariss-MacBook-Pro Spoty-Linguist-Project % code goes here

Deactivate with: 'deactivate'

### 1.1 Packages

#### Install packages in virtual evironment in terminal and  import packages below


```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import random
import time
from tqdm import tqdm
import re
import sqlite3
from pandas.io import sql
import joblib

# Display settings
%config InlineBackend.figure_format = 'retina'
%matplotlib inline
pd.options.display.max_columns = False

# QQ Music API Package
from QQMusicAPI import QQMusic

# Spotipy API Package
import spotipy
from spotipy.oauth2 import SpotifyClientCredentials
from spotipy.oauth2 import SpotifyOAuth

# Mandarin NLP packages
from snownlp import SnowNLP
import jieba.posseg as pseg
import jieba
import hanzidentifier
import stopwordsiso
from stopwordsiso import stopwords

# NLP Using a count vectorizer TfidfVectorizer.
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, RidgeCV, LassoCV, ElasticNetCV, LogisticRegression, LogisticRegressionCV
from sklearn.metrics import plot_confusion_matrix, plot_roc_curve, plot_precision_recall_curve, roc_auc_score, average_precision_score
from sklearn.pipeline import make_pipeline
from sklearn.cluster import DBSCAN, KMeans, AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from sklearn.preprocessing import StandardScaler
from sklearn import metrics

# disable warnings to increase readability of notebook
import warnings

warnings.filterwarnings("ignore")
warnings.filterwarnings("ignore", category=DeprecationWarning)

# other
import ipywidgets as widgets
```

## 2. Data Collection

### 2.1. Spotify API Setup

![image.png](attachment:image.png)


```python
# slmsn Log in to spotipy API
client_id = 'c66cb687e0e84683a4e3ab8e8e434fb4'
client_secret = 'afcc61af42db4850a39bae2f80c7d22c'
spotipy_redirect = 'http://127.0.0.1:8888/'
username = 'stoooey'
device_id = '0d4063631663fbfd1c65bab8abb4d4cdd29a7ee3'
# # slout Log in to spotipy API
# client_id = 'd4326d78b2d546748e38e64999553ed7'
# client_secret = '322745c5243e4c20ac0f70c9e9f64aad'
# spotipy_redirect = 'http://127.0.0.1:8082/'
# username = '31vdm4nfavfv5cxazcd2agrhcxi4'
# device_id = '129053ff24523dd666330f953596e609249aad14'

scope = 'playlist-modify-private,playlist-modify-public,user-library-read,user-top-read,streaming, user-read-playback-state, user-read-recently-played'

token = SpotifyOAuth(username=username,
                     scope=scope,
                     client_id=client_id,
                     client_secret=client_secret,
                     redirect_uri=spotipy_redirect)
sp = spotipy.Spotify(auth_manager=token, requests_timeout=10, retries=10)
```


```python
# sp can then be used to access spotify API commands
```


```python
sp.artist_top_tracks('spotify:artist:7fIvjotigTGWqjIz6EP1i4')
```




    {'tracks': [{'album': {'album_type': 'album',
        'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/7fIvjotigTGWqjIz6EP1i4'},
          'href': 'https://api.spotify.com/v1/artists/7fIvjotigTGWqjIz6EP1i4',
          'id': '7fIvjotigTGWqjIz6EP1i4',
          'name': 'Four Tops',
          'type': 'artist',
          'uri': 'spotify:artist:7fIvjotigTGWqjIz6EP1i4'}],
        'external_urls': {'spotify': 'https://open.spotify.com/album/2jvkMPydCpLWwRothqJU1O'},
        'href': 'https://api.spotify.com/v1/albums/2jvkMPydCpLWwRothqJU1O',
        'id': '2jvkMPydCpLWwRothqJU1O',
        'images': [{'height': 640,
          'url': 'https://i.scdn.co/image/ab67616d0000b273c0fa62ae53b2aeae3766f13d',
          'width': 640},
         {'height': 300,
          'url': 'https://i.scdn.co/image/ab67616d00001e02c0fa62ae53b2aeae3766f13d',
          'width': 300},
         {'height': 64,
          'url': 'https://i.scdn.co/image/ab67616d00004851c0fa62ae53b2aeae3766f13d',
          'width': 64}],
        'name': 'Four Tops Second Album',
        'release_date': '1965',
        'release_date_precision': 'year',
        'total_tracks': 12,
        'type': 'album',
        'uri': 'spotify:album:2jvkMPydCpLWwRothqJU1O'},
       'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/7fIvjotigTGWqjIz6EP1i4'},
         'href': 'https://api.spotify.com/v1/artists/7fIvjotigTGWqjIz6EP1i4',
         'id': '7fIvjotigTGWqjIz6EP1i4',
         'name': 'Four Tops',
         'type': 'artist',
         'uri': 'spotify:artist:7fIvjotigTGWqjIz6EP1i4'}],
       'disc_number': 1,
       'duration_ms': 160280,
       'explicit': False,
       'external_ids': {'isrc': 'USMO16582593'},
       'external_urls': {'spotify': 'https://open.spotify.com/track/6b6IMqP565TbtFFZg9iFf3'},
       'href': 'https://api.spotify.com/v1/tracks/6b6IMqP565TbtFFZg9iFf3',
       'id': '6b6IMqP565TbtFFZg9iFf3',
       'is_local': False,
       'is_playable': True,
       'name': "I Can't Help Myself (Sugar Pie, Honey Bunch)",
       'popularity': 73,
       'preview_url': None,
       'track_number': 1,
       'type': 'track',
       'uri': 'spotify:track:6b6IMqP565TbtFFZg9iFf3'},
      {'album': {'album_type': 'album',
        'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/7fIvjotigTGWqjIz6EP1i4'},
          'href': 'https://api.spotify.com/v1/artists/7fIvjotigTGWqjIz6EP1i4',
          'id': '7fIvjotigTGWqjIz6EP1i4',
          'name': 'Four Tops',
          'type': 'artist',
          'uri': 'spotify:artist:7fIvjotigTGWqjIz6EP1i4'}],
        'external_urls': {'spotify': 'https://open.spotify.com/album/1pbPkhuest1I4RGpz3rWmC'},
        'href': 'https://api.spotify.com/v1/albums/1pbPkhuest1I4RGpz3rWmC',
        'id': '1pbPkhuest1I4RGpz3rWmC',
        'images': [{'height': 640,
          'url': 'https://i.scdn.co/image/ab67616d0000b27388e5b36b4749123a69b164e4',
          'width': 640},
         {'height': 300,
          'url': 'https://i.scdn.co/image/ab67616d00001e0288e5b36b4749123a69b164e4',
          'width': 300},
         {'height': 64,
          'url': 'https://i.scdn.co/image/ab67616d0000485188e5b36b4749123a69b164e4',
          'width': 64}],
        'name': 'Indestructible',
        'release_date': '1988-02-14',
        'release_date_precision': 'day',
        'total_tracks': 10,
        'type': 'album',
        'uri': 'spotify:album:1pbPkhuest1I4RGpz3rWmC'},
       'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/7fIvjotigTGWqjIz6EP1i4'},
         'href': 'https://api.spotify.com/v1/artists/7fIvjotigTGWqjIz6EP1i4',
         'id': '7fIvjotigTGWqjIz6EP1i4',
         'name': 'Four Tops',
         'type': 'artist',
         'uri': 'spotify:artist:7fIvjotigTGWqjIz6EP1i4'}],
       'disc_number': 1,
       'duration_ms': 274066,
       'explicit': False,
       'external_ids': {'isrc': 'USAR18800003'},
       'external_urls': {'spotify': 'https://open.spotify.com/track/7gfAYm7X5HMSuKcO4dy8Wi'},
       'href': 'https://api.spotify.com/v1/tracks/7gfAYm7X5HMSuKcO4dy8Wi',
       'id': '7gfAYm7X5HMSuKcO4dy8Wi',
       'is_local': False,
       'is_playable': True,
       'name': 'Loco in Acapulco',
       'popularity': 63,
       'preview_url': 'https://p.scdn.co/mp3-preview/afaaa4c2feac6264a4931148195d0d95fc9f98ca?cid=d4326d78b2d546748e38e64999553ed7',
       'track_number': 6,
       'type': 'track',
       'uri': 'spotify:track:7gfAYm7X5HMSuKcO4dy8Wi'},
      {'album': {'album_type': 'album',
        'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/7fIvjotigTGWqjIz6EP1i4'},
          'href': 'https://api.spotify.com/v1/artists/7fIvjotigTGWqjIz6EP1i4',
          'id': '7fIvjotigTGWqjIz6EP1i4',
          'name': 'Four Tops',
          'type': 'artist',
          'uri': 'spotify:artist:7fIvjotigTGWqjIz6EP1i4'}],
        'external_urls': {'spotify': 'https://open.spotify.com/album/6TPCze8g5Q3yCRYyA42qAm'},
        'href': 'https://api.spotify.com/v1/albums/6TPCze8g5Q3yCRYyA42qAm',
        'id': '6TPCze8g5Q3yCRYyA42qAm',
        'images': [{'height': 640,
          'url': 'https://i.scdn.co/image/ab67616d0000b2735060210758180bbc93db96de',
          'width': 640},
         {'height': 300,
          'url': 'https://i.scdn.co/image/ab67616d00001e025060210758180bbc93db96de',
          'width': 300},
         {'height': 64,
          'url': 'https://i.scdn.co/image/ab67616d000048515060210758180bbc93db96de',
          'width': 64}],
        'name': 'Four Tops',
        'release_date': '1965-01-01',
        'release_date_precision': 'day',
        'total_tracks': 11,
        'type': 'album',
        'uri': 'spotify:album:6TPCze8g5Q3yCRYyA42qAm'},
       'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/7fIvjotigTGWqjIz6EP1i4'},
         'href': 'https://api.spotify.com/v1/artists/7fIvjotigTGWqjIz6EP1i4',
         'id': '7fIvjotigTGWqjIz6EP1i4',
         'name': 'Four Tops',
         'type': 'artist',
         'uri': 'spotify:artist:7fIvjotigTGWqjIz6EP1i4'}],
       'disc_number': 1,
       'duration_ms': 166026,
       'explicit': False,
       'external_ids': {'isrc': 'USMO10111235'},
       'external_urls': {'spotify': 'https://open.spotify.com/track/3aCbwWCYCT3MJjZeUnlcp4'},
       'href': 'https://api.spotify.com/v1/tracks/3aCbwWCYCT3MJjZeUnlcp4',
       'id': '3aCbwWCYCT3MJjZeUnlcp4',
       'is_local': False,
       'is_playable': True,
       'name': 'Baby I Need Your Loving',
       'popularity': 62,
       'preview_url': None,
       'track_number': 1,
       'type': 'track',
       'uri': 'spotify:track:3aCbwWCYCT3MJjZeUnlcp4'},
      {'album': {'album_type': 'album',
        'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/7fIvjotigTGWqjIz6EP1i4'},
          'href': 'https://api.spotify.com/v1/artists/7fIvjotigTGWqjIz6EP1i4',
          'id': '7fIvjotigTGWqjIz6EP1i4',
          'name': 'Four Tops',
          'type': 'artist',
          'uri': 'spotify:artist:7fIvjotigTGWqjIz6EP1i4'}],
        'external_urls': {'spotify': 'https://open.spotify.com/album/4Jw0RycAqlXeAoymbc0CYp'},
        'href': 'https://api.spotify.com/v1/albums/4Jw0RycAqlXeAoymbc0CYp',
        'id': '4Jw0RycAqlXeAoymbc0CYp',
        'images': [{'height': 640,
          'url': 'https://i.scdn.co/image/ab67616d0000b27347d4f42187d70ddbaac864a2',
          'width': 640},
         {'height': 300,
          'url': 'https://i.scdn.co/image/ab67616d00001e0247d4f42187d70ddbaac864a2',
          'width': 300},
         {'height': 64,
          'url': 'https://i.scdn.co/image/ab67616d0000485147d4f42187d70ddbaac864a2',
          'width': 64}],
        'name': 'Reach Out',
        'release_date': '2004-01-01',
        'release_date_precision': 'day',
        'total_tracks': 12,
        'type': 'album',
        'uri': 'spotify:album:4Jw0RycAqlXeAoymbc0CYp'},
       'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/7fIvjotigTGWqjIz6EP1i4'},
         'href': 'https://api.spotify.com/v1/artists/7fIvjotigTGWqjIz6EP1i4',
         'id': '7fIvjotigTGWqjIz6EP1i4',
         'name': 'Four Tops',
         'type': 'artist',
         'uri': 'spotify:artist:7fIvjotigTGWqjIz6EP1i4'}],
       'disc_number': 1,
       'duration_ms': 178533,
       'explicit': False,
       'external_ids': {'isrc': 'USMO10119990'},
       'external_urls': {'spotify': 'https://open.spotify.com/track/6Pkj4nv5K53i64cLVgkVyY'},
       'href': 'https://api.spotify.com/v1/tracks/6Pkj4nv5K53i64cLVgkVyY',
       'id': '6Pkj4nv5K53i64cLVgkVyY',
       'is_local': False,
       'is_playable': True,
       'name': "Reach Out I'll Be There",
       'popularity': 59,
       'preview_url': None,
       'track_number': 1,
       'type': 'track',
       'uri': 'spotify:track:6Pkj4nv5K53i64cLVgkVyY'},
      {'album': {'album_type': 'album',
        'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/7fIvjotigTGWqjIz6EP1i4'},
          'href': 'https://api.spotify.com/v1/artists/7fIvjotigTGWqjIz6EP1i4',
          'id': '7fIvjotigTGWqjIz6EP1i4',
          'name': 'Four Tops',
          'type': 'artist',
          'uri': 'spotify:artist:7fIvjotigTGWqjIz6EP1i4'}],
        'external_urls': {'spotify': 'https://open.spotify.com/album/2jvkMPydCpLWwRothqJU1O'},
        'href': 'https://api.spotify.com/v1/albums/2jvkMPydCpLWwRothqJU1O',
        'id': '2jvkMPydCpLWwRothqJU1O',
        'images': [{'height': 640,
          'url': 'https://i.scdn.co/image/ab67616d0000b273c0fa62ae53b2aeae3766f13d',
          'width': 640},
         {'height': 300,
          'url': 'https://i.scdn.co/image/ab67616d00001e02c0fa62ae53b2aeae3766f13d',
          'width': 300},
         {'height': 64,
          'url': 'https://i.scdn.co/image/ab67616d00004851c0fa62ae53b2aeae3766f13d',
          'width': 64}],
        'name': 'Four Tops Second Album',
        'release_date': '1965',
        'release_date_precision': 'year',
        'total_tracks': 12,
        'type': 'album',
        'uri': 'spotify:album:2jvkMPydCpLWwRothqJU1O'},
       'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/7fIvjotigTGWqjIz6EP1i4'},
         'href': 'https://api.spotify.com/v1/artists/7fIvjotigTGWqjIz6EP1i4',
         'id': '7fIvjotigTGWqjIz6EP1i4',
         'name': 'Four Tops',
         'type': 'artist',
         'uri': 'spotify:artist:7fIvjotigTGWqjIz6EP1i4'}],
       'disc_number': 1,
       'duration_ms': 166400,
       'explicit': False,
       'external_ids': {'isrc': 'USMO10111236'},
       'external_urls': {'spotify': 'https://open.spotify.com/track/7d1ustAIlXG7ht5Q4IOv7s'},
       'href': 'https://api.spotify.com/v1/tracks/7d1ustAIlXG7ht5Q4IOv7s',
       'id': '7d1ustAIlXG7ht5Q4IOv7s',
       'is_local': False,
       'is_playable': True,
       'name': "It's The Same Old Song",
       'popularity': 56,
       'preview_url': None,
       'track_number': 5,
       'type': 'track',
       'uri': 'spotify:track:7d1ustAIlXG7ht5Q4IOv7s'},
      {'album': {'album_type': 'album',
        'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/7fIvjotigTGWqjIz6EP1i4'},
          'href': 'https://api.spotify.com/v1/artists/7fIvjotigTGWqjIz6EP1i4',
          'id': '7fIvjotigTGWqjIz6EP1i4',
          'name': 'Four Tops',
          'type': 'artist',
          'uri': 'spotify:artist:7fIvjotigTGWqjIz6EP1i4'}],
        'external_urls': {'spotify': 'https://open.spotify.com/album/0WwvY1uMFowKCrcFqF87o9'},
        'href': 'https://api.spotify.com/v1/albums/0WwvY1uMFowKCrcFqF87o9',
        'id': '0WwvY1uMFowKCrcFqF87o9',
        'images': [{'height': 640,
          'url': 'https://i.scdn.co/image/ab67616d0000b2730529944f736f1f6603990a1c',
          'width': 640},
         {'height': 300,
          'url': 'https://i.scdn.co/image/ab67616d00001e020529944f736f1f6603990a1c',
          'width': 300},
         {'height': 64,
          'url': 'https://i.scdn.co/image/ab67616d000048510529944f736f1f6603990a1c',
          'width': 64}],
        'name': 'Keeper Of The Castle',
        'release_date': '1972-01-01',
        'release_date_precision': 'day',
        'total_tracks': 12,
        'type': 'album',
        'uri': 'spotify:album:0WwvY1uMFowKCrcFqF87o9'},
       'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/7fIvjotigTGWqjIz6EP1i4'},
         'href': 'https://api.spotify.com/v1/artists/7fIvjotigTGWqjIz6EP1i4',
         'id': '7fIvjotigTGWqjIz6EP1i4',
         'name': 'Four Tops',
         'type': 'artist',
         'uri': 'spotify:artist:7fIvjotigTGWqjIz6EP1i4'}],
       'disc_number': 1,
       'duration_ms': 187773,
       'explicit': False,
       'external_ids': {'isrc': 'USMC17246883'},
       'external_urls': {'spotify': 'https://open.spotify.com/track/6q8NBpF6twALMb86FBpLgQ'},
       'href': 'https://api.spotify.com/v1/tracks/6q8NBpF6twALMb86FBpLgQ',
       'id': '6q8NBpF6twALMb86FBpLgQ',
       'is_local': False,
       'is_playable': True,
       'name': "Ain't No Woman (Like The One I've Got)",
       'popularity': 52,
       'preview_url': None,
       'track_number': 2,
       'type': 'track',
       'uri': 'spotify:track:6q8NBpF6twALMb86FBpLgQ'},
      {'album': {'album_type': 'album',
        'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/7fIvjotigTGWqjIz6EP1i4'},
          'href': 'https://api.spotify.com/v1/artists/7fIvjotigTGWqjIz6EP1i4',
          'id': '7fIvjotigTGWqjIz6EP1i4',
          'name': 'Four Tops',
          'type': 'artist',
          'uri': 'spotify:artist:7fIvjotigTGWqjIz6EP1i4'}],
        'external_urls': {'spotify': 'https://open.spotify.com/album/132ZigRToQWUtgMK4a6OSU'},
        'href': 'https://api.spotify.com/v1/albums/132ZigRToQWUtgMK4a6OSU',
        'id': '132ZigRToQWUtgMK4a6OSU',
        'images': [{'height': 640,
          'url': 'https://i.scdn.co/image/ab67616d0000b27319fd998669b2e94ea39a08b0',
          'width': 640},
         {'height': 300,
          'url': 'https://i.scdn.co/image/ab67616d00001e0219fd998669b2e94ea39a08b0',
          'width': 300},
         {'height': 64,
          'url': 'https://i.scdn.co/image/ab67616d0000485119fd998669b2e94ea39a08b0',
          'width': 64}],
        'name': 'Still Waters Run Deep',
        'release_date': '1970',
        'release_date_precision': 'year',
        'total_tracks': 10,
        'type': 'album',
        'uri': 'spotify:album:132ZigRToQWUtgMK4a6OSU'},
       'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/7fIvjotigTGWqjIz6EP1i4'},
         'href': 'https://api.spotify.com/v1/artists/7fIvjotigTGWqjIz6EP1i4',
         'id': '7fIvjotigTGWqjIz6EP1i4',
         'name': 'Four Tops',
         'type': 'artist',
         'uri': 'spotify:artist:7fIvjotigTGWqjIz6EP1i4'}],
       'disc_number': 1,
       'duration_ms': 194693,
       'explicit': False,
       'external_ids': {'isrc': 'USMO16784694'},
       'external_urls': {'spotify': 'https://open.spotify.com/track/5feUxmo1qphxusEo1N9GRt'},
       'href': 'https://api.spotify.com/v1/tracks/5feUxmo1qphxusEo1N9GRt',
       'id': '5feUxmo1qphxusEo1N9GRt',
       'is_local': False,
       'is_playable': True,
       'name': 'Still Water (Love)',
       'popularity': 46,
       'preview_url': None,
       'track_number': 1,
       'type': 'track',
       'uri': 'spotify:track:5feUxmo1qphxusEo1N9GRt'},
      {'album': {'album_type': 'album',
        'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/7fIvjotigTGWqjIz6EP1i4'},
          'href': 'https://api.spotify.com/v1/artists/7fIvjotigTGWqjIz6EP1i4',
          'id': '7fIvjotigTGWqjIz6EP1i4',
          'name': 'Four Tops',
          'type': 'artist',
          'uri': 'spotify:artist:7fIvjotigTGWqjIz6EP1i4'}],
        'external_urls': {'spotify': 'https://open.spotify.com/album/4Jw0RycAqlXeAoymbc0CYp'},
        'href': 'https://api.spotify.com/v1/albums/4Jw0RycAqlXeAoymbc0CYp',
        'id': '4Jw0RycAqlXeAoymbc0CYp',
        'images': [{'height': 640,
          'url': 'https://i.scdn.co/image/ab67616d0000b27347d4f42187d70ddbaac864a2',
          'width': 640},
         {'height': 300,
          'url': 'https://i.scdn.co/image/ab67616d00001e0247d4f42187d70ddbaac864a2',
          'width': 300},
         {'height': 64,
          'url': 'https://i.scdn.co/image/ab67616d0000485147d4f42187d70ddbaac864a2',
          'width': 64}],
        'name': 'Reach Out',
        'release_date': '2004-01-01',
        'release_date_precision': 'day',
        'total_tracks': 12,
        'type': 'album',
        'uri': 'spotify:album:4Jw0RycAqlXeAoymbc0CYp'},
       'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/7fIvjotigTGWqjIz6EP1i4'},
         'href': 'https://api.spotify.com/v1/artists/7fIvjotigTGWqjIz6EP1i4',
         'id': '7fIvjotigTGWqjIz6EP1i4',
         'name': 'Four Tops',
         'type': 'artist',
         'uri': 'spotify:artist:7fIvjotigTGWqjIz6EP1i4'}],
       'disc_number': 1,
       'duration_ms': 157640,
       'explicit': False,
       'external_ids': {'isrc': 'USMO10119991'},
       'external_urls': {'spotify': 'https://open.spotify.com/track/2sD2XkPogLt7keXttj7F9o'},
       'href': 'https://api.spotify.com/v1/tracks/2sD2XkPogLt7keXttj7F9o',
       'id': '2sD2XkPogLt7keXttj7F9o',
       'is_local': False,
       'is_playable': True,
       'name': 'Standing In The Shadows Of Love',
       'popularity': 46,
       'preview_url': None,
       'track_number': 8,
       'type': 'track',
       'uri': 'spotify:track:2sD2XkPogLt7keXttj7F9o'},
      {'album': {'album_type': 'album',
        'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/7fIvjotigTGWqjIz6EP1i4'},
          'href': 'https://api.spotify.com/v1/artists/7fIvjotigTGWqjIz6EP1i4',
          'id': '7fIvjotigTGWqjIz6EP1i4',
          'name': 'Four Tops',
          'type': 'artist',
          'uri': 'spotify:artist:7fIvjotigTGWqjIz6EP1i4'}],
        'external_urls': {'spotify': 'https://open.spotify.com/album/3xkgwc7evgKlM2euJZQo6i'},
        'href': 'https://api.spotify.com/v1/albums/3xkgwc7evgKlM2euJZQo6i',
        'id': '3xkgwc7evgKlM2euJZQo6i',
        'images': [{'height': 640,
          'url': 'https://i.scdn.co/image/ab67616d0000b273dcf98a7ef2d199cf4940a305',
          'width': 640},
         {'height': 300,
          'url': 'https://i.scdn.co/image/ab67616d00001e02dcf98a7ef2d199cf4940a305',
          'width': 300},
         {'height': 64,
          'url': 'https://i.scdn.co/image/ab67616d00004851dcf98a7ef2d199cf4940a305',
          'width': 64}],
        'name': 'Tonight',
        'release_date': '1981-01-01',
        'release_date_precision': 'day',
        'total_tracks': 9,
        'type': 'album',
        'uri': 'spotify:album:3xkgwc7evgKlM2euJZQo6i'},
       'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/7fIvjotigTGWqjIz6EP1i4'},
         'href': 'https://api.spotify.com/v1/artists/7fIvjotigTGWqjIz6EP1i4',
         'id': '7fIvjotigTGWqjIz6EP1i4',
         'name': 'Four Tops',
         'type': 'artist',
         'uri': 'spotify:artist:7fIvjotigTGWqjIz6EP1i4'}],
       'disc_number': 1,
       'duration_ms': 206053,
       'explicit': False,
       'external_ids': {'isrc': 'USPR38100076'},
       'external_urls': {'spotify': 'https://open.spotify.com/track/3FPaQv0qldYuF0XD0ziwT8'},
       'href': 'https://api.spotify.com/v1/tracks/3FPaQv0qldYuF0XD0ziwT8',
       'id': '3FPaQv0qldYuF0XD0ziwT8',
       'is_local': False,
       'is_playable': True,
       'name': 'When She Was My Girl',
       'popularity': 45,
       'preview_url': None,
       'track_number': 1,
       'type': 'track',
       'uri': 'spotify:track:3FPaQv0qldYuF0XD0ziwT8'},
      {'album': {'album_type': 'album',
        'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/7fIvjotigTGWqjIz6EP1i4'},
          'href': 'https://api.spotify.com/v1/artists/7fIvjotigTGWqjIz6EP1i4',
          'id': '7fIvjotigTGWqjIz6EP1i4',
          'name': 'Four Tops',
          'type': 'artist',
          'uri': 'spotify:artist:7fIvjotigTGWqjIz6EP1i4'}],
        'external_urls': {'spotify': 'https://open.spotify.com/album/3RQVbZ3zV5l9EOWDMyiwuR'},
        'href': 'https://api.spotify.com/v1/albums/3RQVbZ3zV5l9EOWDMyiwuR',
        'id': '3RQVbZ3zV5l9EOWDMyiwuR',
        'images': [{'height': 640,
          'url': 'https://i.scdn.co/image/ab67616d0000b273c92bc10c9587b72c6240ecd4',
          'width': 640},
         {'height': 300,
          'url': 'https://i.scdn.co/image/ab67616d00001e02c92bc10c9587b72c6240ecd4',
          'width': 300},
         {'height': 64,
          'url': 'https://i.scdn.co/image/ab67616d00004851c92bc10c9587b72c6240ecd4',
          'width': 64}],
        'name': 'Main Street People',
        'release_date': '1973-01-01',
        'release_date_precision': 'day',
        'total_tracks': 11,
        'type': 'album',
        'uri': 'spotify:album:3RQVbZ3zV5l9EOWDMyiwuR'},
       'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/7fIvjotigTGWqjIz6EP1i4'},
         'href': 'https://api.spotify.com/v1/artists/7fIvjotigTGWqjIz6EP1i4',
         'id': '7fIvjotigTGWqjIz6EP1i4',
         'name': 'Four Tops',
         'type': 'artist',
         'uri': 'spotify:artist:7fIvjotigTGWqjIz6EP1i4'}],
       'disc_number': 1,
       'duration_ms': 207146,
       'explicit': False,
       'external_ids': {'isrc': 'USMC17347851'},
       'external_urls': {'spotify': 'https://open.spotify.com/track/6DdWSQJNCQuQjXJQBHXTz0'},
       'href': 'https://api.spotify.com/v1/tracks/6DdWSQJNCQuQjXJQBHXTz0',
       'id': '6DdWSQJNCQuQjXJQBHXTz0',
       'is_local': False,
       'is_playable': True,
       'name': 'Are You Man Enough?',
       'popularity': 43,
       'preview_url': None,
       'track_number': 6,
       'type': 'track',
       'uri': 'spotify:track:6DdWSQJNCQuQjXJQBHXTz0'}]}




```python
# # E.g. access current user (me) most recently played song
sp.current_user_recently_played()
```




    {'items': [],
     'next': None,
     'cursors': None,
     'limit': 50,
     'href': 'https://api.spotify.com/v1/me/player/recently-played?limit=50'}



### 2.2. Scrape mandarin speaking artists from Spotify
![image-2.png](attachment:image-2.png)

I started with a playlist of 100 mandarin songs. I then found similar artists using the Spotify API's related artists function to find further artists. I repeated this process on each of the new artists that I found until I had c.2,000 artists. I could have scraped many more artists however I was limited by the number of requests I could send to the Spotify server and time available. The artists scraped were more than sufficient for the purposes on this project. 

_Footnote: In the below example code I use a starting playlist of just 3 tracks to reduce run time in this notebook. The full results were run previously and are imported at the end of this section._


```python
# I created a playlist in Spotify of 100 songs by various mandarin artists
# The below functions import the playlist and saves information about each track
# Including track name, artist, artist code,
# Playlist url is available in spotify (share playlist)


# Function to get the individual ids of each song in a playlist
def getTrackIDs(user, playlist_id):
    ids = []
    playlist = sp.user_playlist(user, playlist_id)
    for item in playlist['tracks']['items']:
        track = item['track']
        ids.append(track['id'])
    return ids


# Function to return information about a single track from its Spotify ID
def getTrackFeatures(id):
    try:
        meta = sp.track(id)
        features = sp.audio_features(id)
        name = meta['name']
        artist_href = meta['album']['artists'][0]['href']
        artist = meta['album']['artists'][0]['name']
        artist_uri = meta['artists'][0]['uri']
        artist_type = meta['artists'][0]['type']
        song_available_markets = meta['available_markets']
        song_duration_ms = meta['duration_ms']
        is_explicit = meta['explicit']
        song_href = meta['href']
        preview_url = meta['preview_url']
        track_number = meta['track_number']
        song_type = meta['type']
        length = meta['duration_ms']
        popularity = meta['popularity']
        album = meta['album']['name']
        album_type = meta['album']['album_type']
        album_artwork_link = meta['album']['images'][0]['url']
        album_tracks_num = meta['album']['total_tracks']
        album_uri = meta['album']['uri']
        album_release_date = meta['album']['release_date']
        acousticness = features[0]['acousticness']
        danceability = features[0]['danceability']
        energy = features[0]['energy']
        instrumentalness = features[0]['instrumentalness']
        liveness = features[0]['liveness']
        loudness = features[0]['loudness']
        speechiness = features[0]['speechiness']
        tempo = features[0]['tempo']
        time_signature = features[0]['time_signature']
        track = [
            name, artist_href, artist, artist_uri, artist_type,
            song_available_markets, song_duration_ms, is_explicit, song_href,
            preview_url, track_number, song_type, length, popularity, album,
            album_type, album_artwork_link, album_tracks_num, album_uri,
            album_release_date, acousticness, danceability, energy,
            instrumentalness, liveness, loudness, speechiness, tempo,
            time_signature
        ]
        return track
    except:
        pass


# Function to save information on every song in playlist in a dataframe
# This uses the two functions above
def grab_song_from_playlist(playlisturl, choose_playlist_name):
    # Spotify developer login credentials for API
    ids = getTrackIDs(username, playlisturl)
    tracks = []
    for i in range(len(ids)):
        track = getTrackFeatures(ids[i])
        tracks.append(track)
    # create dataset and save as csv
    df = pd.DataFrame(
        tracks,
        columns=[
            'name', 'artist_href', 'artist', 'artist_uri', 'artist_type',
            'song_available_markets', 'song_duration_ms', 'is_explicit',
            'song_href', 'preview_url', 'track_number', 'song_type', 'length',
            'popularity', 'album', 'album_type', 'album_artwork_link',
            'album_tracks_num', 'album_uri', 'album_release_date',
            'acousticness', 'danceability', 'energy', 'instrumentalness',
            'liveness', 'loudness', 'speechiness', 'tempo', 'time_signature'
        ])
    df.to_csv("%s.csv" % choose_playlist_name, sep=',')
    return df
```


```python
df_eg_playlist = grab_song_from_playlist(
    'spotify:playlist:3MfiGKuLbufkFtBfKOSkA0', 'Example_playlist')
df_eg_playlist
```

Songs and their meta data are saved from the playlist and shown in the above dataframe. This dataframe was then used as a starting point to find further artists.



```python
# Function to find similar artists to those included in original playlist
def scrape_similar_artists(starter_artist):
    # 1 Scrape similar artists and create df

    request = sp.artist_related_artists(starter_artist)
    name = []
    genres = []
    popularity = []
    followers = []
    artist_id = []
    artist_uri = []
    artist_href = []
    artist_images = []

    for ind, artists in enumerate(request['artists']):
        name.append(request['artists'][ind]['name'])
        genres.append(','.join(request['artists'][ind]['genres']))
        popularity.append(request['artists'][ind]['popularity'])
        followers.append(request['artists'][ind]['followers']['total'])
        artist_id.append(request['artists'][ind]['id'])
        artist_uri.append(request['artists'][ind]['uri'])
        artist_href.append(request['artists'][ind]['href'])
        # artist_images.append(request['artists'][ind]['images'][0]['url'])
        artist_images.append(np.nan)
    data = [
        name, genres, popularity, followers, artist_id, artist_uri,
        artist_href, artist_images
    ]
    df_artists = pd.DataFrame(data).T
    df_artists.columns = columns = [
        'name', 'genres', 'popularity', 'followers', 'artist_id', 'artist_uri',
        'artist_href', 'artist_images'
    ]
    df_artists = df_artists.sort_values(by='popularity', ascending=False)
    return df_artists
```


```python
# For each artist scrape similar artists and save the artist details in a new dataframe
df_eg_playlist_similar_artists = pd.DataFrame()
for ind, i in enumerate(df_eg_playlist['artist_uri']):
    try:
        df_eg_playlist_similar_artists = df_eg_playlist_similar_artists.append(
            scrape_similar_artists(i))
    except:
        pass

df_eg_playlist_similar_artists = df_eg_playlist_similar_artists.drop_duplicates(
    subset='artist_id', keep="last")
```


```python
df_eg_playlist_similar_artists.head(5)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>genres</th>
      <th>popularity</th>
      <th>followers</th>
      <th>artist_id</th>
      <th>artist_uri</th>
      <th>artist_href</th>
      <th>artist_images</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>16</th>
      <td>Sorry Youth</td>
      <td>chinese indie,taiwan indie,taiwan pop,taiwan p...</td>
      <td>40</td>
      <td>25239</td>
      <td>6c4IBMTcnFDhsKHXNSBBvp</td>
      <td>spotify:artist:6c4IBMTcnFDhsKHXNSBBvp</td>
      <td>https://api.spotify.com/v1/artists/6c4IBMTcnFD...</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>12</th>
      <td>Schoolgirl Byebye</td>
      <td>chinese indie,sinogaze,southern china indie,ta...</td>
      <td>39</td>
      <td>14966</td>
      <td>6kfcndVsu8F9Y5gL5xc717</td>
      <td>spotify:artist:6kfcndVsu8F9Y5gL5xc717</td>
      <td>https://api.spotify.com/v1/artists/6kfcndVsu8F...</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Zhao Lei</td>
      <td>chinese indie,chinese minyao</td>
      <td>35</td>
      <td>46576</td>
      <td>2KwZ9xnULczo0Z7Y7Bp57R</td>
      <td>spotify:artist:2KwZ9xnULczo0Z7Y7Bp57R</td>
      <td>https://api.spotify.com/v1/artists/2KwZ9xnULcz...</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>6</th>
      <td>盘尼西林乐队</td>
      <td>chinese indie</td>
      <td>33</td>
      <td>21184</td>
      <td>2oKqZCLOi18wxFFnDS6e25</td>
      <td>spotify:artist:2oKqZCLOi18wxFFnDS6e25</td>
      <td>https://api.spotify.com/v1/artists/2oKqZCLOi18...</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>8</th>
      <td>五條人</td>
      <td>chinese indie,chinese indie rock,chinese minya...</td>
      <td>31</td>
      <td>21686</td>
      <td>6NQwuxgsebzIzixLkAHDCT</td>
      <td>spotify:artist:6NQwuxgsebzIzixLkAHDCT</td>
      <td>https://api.spotify.com/v1/artists/6NQwuxgsebz...</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>




```python
df_eg_playlist_similar_artists.shape
```




    (48, 8)



In this example the origial playlist has been expanded from an inital number of 3 artists/tracks to 48 artists. I then used this to expand the search further by again looking for similar artists. For each new artist found I look for more similar artists.


```python
# Function to expand this search further.
# Takes data frame of artists made above and looks for similar artists
# It then looks for similar artists to those new artists found and so on
# Each sub tree is searched until no new artists are available
def scrape_similar_artists_expansion(df_artists):
    # for each of the similar artists found scrape similar artists of these too and repeat and until no more unique searches
    # use input df as master df
    master_df = df_artists
    searched_already_list = []
    count = 0
    artist_dict = {}
    for ind, uri in enumerate(master_df['artist_uri']):
        if count < 10000:
            count += 1
            if uri not in searched_already_list:
                artist_dict[ind] = scrape_similar_artists(uri)
                searched_already_list.append(uri)
    for i in artist_dict.keys():
        master_df = master_df.append(artist_dict[i])
    master_df = master_df.drop_duplicates(subset='artist_id', keep="last")
    master_df = master_df.sort_values(by='popularity', ascending=False)
    return master_df
```


```python
# # Run the code:
# df_eg_playlist_similar_artists_expand = scrape_similar_artists_expansion(df_eg_playlist_similar_artists)
```

Running all of the code from this section (3) resulted in a list of c. 2,000 mandarin speaking artists.


```python
artists_df = pd.read_csv(
    '/Users/stuart/Desktop/Spoty-Linguist-Project/Old Dfs/artists_df.csv',
    sep=',',
    index_col=0)
artists_df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>genres</th>
      <th>popularity</th>
      <th>followers</th>
      <th>artist_id</th>
      <th>artist_uri</th>
      <th>artist_href</th>
      <th>artist_images</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>5</th>
      <td>A Si</td>
      <td>chinese indie,chinese minyao,mandopop,taiwan i...</td>
      <td>47</td>
      <td>30483</td>
      <td>4yamiVzQPYBb02ceSu0jaI</td>
      <td>spotify:artist:4yamiVzQPYBb02ceSu0jaI</td>
      <td>https://api.spotify.com/v1/artists/4yamiVzQPYB...</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Deserts Chang</td>
      <td>chinese indie,mandopop,taiwan indie,taiwan pop...</td>
      <td>46</td>
      <td>111001</td>
      <td>7v9Il42LvvTeSfmf1bwfNx</td>
      <td>spotify:artist:7v9Il42LvvTeSfmf1bwfNx</td>
      <td>https://api.spotify.com/v1/artists/7v9Il42LvvT...</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>16</th>
      <td>老王樂隊</td>
      <td>chinese indie,mandopop,taiwan indie,taiwan pop</td>
      <td>46</td>
      <td>103339</td>
      <td>4MRQK5pLyNbcOW493n55iT</td>
      <td>spotify:artist:4MRQK5pLyNbcOW493n55iT</td>
      <td>https://api.spotify.com/v1/artists/4MRQK5pLyNb...</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>18</th>
      <td>Wang Feng</td>
      <td>c-pop,chinese singer-songwriter,classic mandop...</td>
      <td>44</td>
      <td>42463</td>
      <td>10LslMPb7P5j9L2QXGZBmM</td>
      <td>spotify:artist:10LslMPb7P5j9L2QXGZBmM</td>
      <td>https://api.spotify.com/v1/artists/10LslMPb7P5...</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Di Ma</td>
      <td>chinese indie,chinese minyao,chinese singer-so...</td>
      <td>42</td>
      <td>33600</td>
      <td>6INLZbPHXGj6ERrjFGPYD6</td>
      <td>spotify:artist:6INLZbPHXGj6ERrjFGPYD6</td>
      <td>https://api.spotify.com/v1/artists/6INLZbPHXGj...</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>




```python
artists_df.shape
```




    (2000, 8)



### 2.3. Get a list of albums for the artists scraped in 2.2.


```python
# Loop through all of the artists scraped in 3.0 and get a list of albums
# Here we run through only the first 2 artists as an example
empty_list_albums = []

for artist_uri in tqdm(artists_df['artist_uri'].iloc[:2]):
    album_uri_temp = sp.artist_albums(artist_uri)
    for ind, i in enumerate(album_uri_temp['items']):
        empty_list_albums.append(album_uri_temp['items'][ind]['uri'])
```

    100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 12.59it/s]


### 2.4. Get songs and meta data for each album


```python
# loop through all the albums and get track ids for each (top 2 only here)
empty_list_tracks3 = []
for album in tqdm(empty_list_albums[:2]):
    temp = sp.album_tracks(album)['items']
    for ind, track in enumerate(temp):
        try:
            empty_list_tracks3.append(
                sp.album_tracks(album)['items'][ind]['uri'])
        except:
            pass
```

    100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.71it/s]



```python
# Function to get song information from list of song ids
# This didnt work well with large dataframes
# def grab_song_from_ids(ids, choose_playlist_name):
#     # Spotify developer login credentials for API
#     sp = spotify_api_login()

#     tracks = []
#     for i in range(len(ids)):
#         # time.sleep(.5)
#         track = getTrackFeatures(ids[i])
#         tracks.append(track)

#     # create dataset
#     df = pd.DataFrame(tracks, columns = ['name', 'artist_href', 'artist','artist_uri','artist_type','song_available_markets','song_duration_ms','is_explicit','song_href','preview_url', 'track_number', 'song_type', 'length', 'popularity', 'album', 'album_type','album_artwork_link', 'album_tracks_num', 'album_uri', 'album_release_date', 'acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'loudness', 'speechiness', 'tempo', 'time_signature'])
#     df.to_csv("%s.csv"%choose_playlist_name, sep = ',')
#     return df
```


```python
# Loop through list of track IDs and get song information
tracks = []
for i in tqdm(empty_list_tracks3):
    try:
        track = getTrackFeatures(i)
        tracks.append(track)
    except:
        pass
```

    100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:01<00:00, 11.50it/s]



```python
# Loop through list of tracks and append to dataframe. Note that couldnt add altogether due to
# size issues.
df_tracks_master = pd.DataFrame(columns=[
    'name', 'artist_href', 'artist', 'artist_uri', 'artist_type',
    'song_available_markets', 'song_duration_ms', 'is_explicit', 'song_href',
    'preview_url', 'track_number', 'song_type', 'length', 'popularity',
    'album', 'album_type', 'album_artwork_link', 'album_tracks_num',
    'album_uri', 'album_release_date', 'acousticness', 'danceability',
    'energy', 'instrumentalness', 'liveness', 'loudness', 'speechiness',
    'tempo', 'time_signature'
])

for ind, i in enumerate(tracks):
    try:
        df_tracks_master.loc[ind] = i
    except:
        pass
```

Here's one I made earlier: the full dataframe of all tracks scraped


```python
df_tracks_master = pd.read_csv(
    '/Users/stuart/Desktop/Spoty-Linguist-Project/Old Dfs/df_tracks_master.csv',
    sep=',',
    index_col=0)
display(df_tracks_master.head())
df_tracks_master.shape
```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>artist_href</th>
      <th>artist</th>
      <th>artist_uri</th>
      <th>artist_type</th>
      <th>song_available_markets</th>
      <th>song_duration_ms</th>
      <th>is_explicit</th>
      <th>song_href</th>
      <th>preview_url</th>
      <th>...</th>
      <th>album_release_date</th>
      <th>acousticness</th>
      <th>danceability</th>
      <th>energy</th>
      <th>instrumentalness</th>
      <th>liveness</th>
      <th>loudness</th>
      <th>speechiness</th>
      <th>tempo</th>
      <th>time_signature</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>英雄 (Live)</td>
      <td>https://api.spotify.com/v1/artists/2elBjNSdBE2...</td>
      <td>Jay Chou</td>
      <td>spotify:artist:2elBjNSdBE2Y3f0j1mjrql</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>182040.0</td>
      <td>False</td>
      <td>https://api.spotify.com/v1/tracks/1i31BYKv1La3...</td>
      <td>https://p.scdn.co/mp3-preview/191037a026d9e098...</td>
      <td>...</td>
      <td>2019-11-01</td>
      <td>0.000172</td>
      <td>0.522</td>
      <td>0.908</td>
      <td>0.002500</td>
      <td>0.458</td>
      <td>-8.747</td>
      <td>0.0596</td>
      <td>118.987</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>雙截棍 (Live)</td>
      <td>https://api.spotify.com/v1/artists/2elBjNSdBE2...</td>
      <td>Jay Chou</td>
      <td>spotify:artist:2elBjNSdBE2Y3f0j1mjrql</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>145173.0</td>
      <td>False</td>
      <td>https://api.spotify.com/v1/tracks/77uwbOEaj6nM...</td>
      <td>https://p.scdn.co/mp3-preview/80c09c504a54344b...</td>
      <td>...</td>
      <td>2019-11-01</td>
      <td>0.002810</td>
      <td>0.461</td>
      <td>0.953</td>
      <td>0.000000</td>
      <td>0.424</td>
      <td>-6.440</td>
      <td>0.1360</td>
      <td>101.037</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>開不了口 (Live)</td>
      <td>https://api.spotify.com/v1/artists/2elBjNSdBE2...</td>
      <td>Jay Chou</td>
      <td>spotify:artist:2elBjNSdBE2Y3f0j1mjrql</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>272973.0</td>
      <td>False</td>
      <td>https://api.spotify.com/v1/tracks/5OR1Fbd7RSI1...</td>
      <td>https://p.scdn.co/mp3-preview/82a62689347391e7...</td>
      <td>...</td>
      <td>2019-11-01</td>
      <td>0.001760</td>
      <td>0.519</td>
      <td>0.612</td>
      <td>0.000002</td>
      <td>0.564</td>
      <td>-10.634</td>
      <td>0.0267</td>
      <td>139.944</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>床邊故事 (Live)</td>
      <td>https://api.spotify.com/v1/artists/2elBjNSdBE2...</td>
      <td>Jay Chou</td>
      <td>spotify:artist:2elBjNSdBE2Y3f0j1mjrql</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>220240.0</td>
      <td>False</td>
      <td>https://api.spotify.com/v1/tracks/7MmT3xzugKwe...</td>
      <td>https://p.scdn.co/mp3-preview/5316d5602f55d671...</td>
      <td>...</td>
      <td>2019-11-01</td>
      <td>0.039800</td>
      <td>0.640</td>
      <td>0.844</td>
      <td>0.000005</td>
      <td>0.122</td>
      <td>-9.645</td>
      <td>0.0539</td>
      <td>106.033</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>夜曲+竊愛 (Live)</td>
      <td>https://api.spotify.com/v1/artists/2elBjNSdBE2...</td>
      <td>Jay Chou</td>
      <td>spotify:artist:2elBjNSdBE2Y3f0j1mjrql</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>222013.0</td>
      <td>False</td>
      <td>https://api.spotify.com/v1/tracks/4vlzCpNsDFBa...</td>
      <td>https://p.scdn.co/mp3-preview/446b1630b52837ae...</td>
      <td>...</td>
      <td>2019-11-01</td>
      <td>0.028300</td>
      <td>0.538</td>
      <td>0.610</td>
      <td>0.005630</td>
      <td>0.386</td>
      <td>-10.513</td>
      <td>0.1680</td>
      <td>174.093</td>
      <td>4.0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 29 columns</p>
</div>





    (89689, 29)



Clean up the data:


```python
# Update column data types
strings_cols = [
    'name', 'artist_href', 'artist', 'artist_uri', 'artist_type',
    'song_available_markets', 'song_duration_ms', 'is_explicit', 'song_href',
    'preview_url', 'song_type', 'album', 'album_type', 'album_artwork_link',
    'album_uri'
]

for i in strings_cols:
    df_tracks_master[i] = df_tracks_master[i].astype("string")
df_tracks_master['album_release_date'] = pd.to_datetime(
    df_tracks_master.album_release_date)

df_tracks_master.dtypes
```




    name                              string
    artist_href                       string
    artist                            string
    artist_uri                        string
    artist_type                       string
    song_available_markets            string
    song_duration_ms                  string
    is_explicit                       string
    song_href                         string
    preview_url                       string
    track_number                     float64
    song_type                         string
    length                           float64
    popularity                       float64
    album                             string
    album_type                        string
    album_artwork_link                string
    album_tracks_num                 float64
    album_uri                         string
    album_release_date        datetime64[ns]
    acousticness                     float64
    danceability                     float64
    energy                           float64
    instrumentalness                 float64
    liveness                         float64
    loudness                         float64
    speechiness                      float64
    tempo                            float64
    time_signature                   float64
    dtype: object




```python
# Drop NAs
df_tracks_master_clean = df_tracks_master.dropna()
df_tracks_master_clean.shape
```




    (75029, 29)




```python
df_tracks_master_clean = df_tracks_master.drop_duplicates(subset='name',
                                                          keep='first')
df_tracks_master_clean.shape
```




    (54947, 29)




```python
df_tracks_master_clean = df_tracks_master_clean.drop_duplicates(
    subset=['acousticness', 'danceability', 'energy', 'tempo'], keep='first')
df_tracks_master_clean.shape
```




    (54326, 29)




```python
# Save the CSV to drive
df_tracks_master_clean.to_csv('df_tracks_master_clean.csv')
```

## 3. Lyric Scraping and Some SQL

### 3.1. Lyrics Scraper
.  |  .
- | - 
![image.png](attachment:image.png)| ![image-2.png](attachment:image-2.png) 

I used the QQ Music API to scrape lyrics for each of the songs I scraped in section 2. This was the most time consuming part of the project with the scraper running for more than 4 days. Surprisingly a majority of the songs I scraped had lyrics available. I encountered issues with the dataframe size becoming too large which was solved by using an SQL database to store information. Lyrics were downloaded in string format with time stamps generally included. I used regex to filter this information. As well as saving the primary dataframe (Spotify song information + lyrics) in SQL, I also saved each song in a unique table with the lyrics in one column and the time stamps in another. I found that I could use this later to print the lyrics of a song in time with the music (just for fun!).

I scraped lyrics for ~55,000 songs. The code below is blocked so that the scraper doesn't run again.


```python
# import random
# db_connection = sqlite3.connect('/Users/stuart/Desktop/Spoty-Linguist-Project/master_sql_db.db.sqlite')
# # Regex for extracting lyrics and time stamps fro QQ scrape
# lyric_r = re.compile(r'\[\d\d\:\d\d\.\d\d\](\w+|'')')
# time_r = re.compile(r'\[(\d\d\:\d\d\.\d\d)\]')

# df = df_tracks_master_clean.copy()
# df['lyrics'] = 'nan'
# df['time_stamp'] = 'nan'

# for ind, i in tqdm(enumerate(df.name)):

#     # create search string:
#     if df.artist.iloc[ind] == 'Various Artists': # if Various artist, just search for album and song name
#         search_text = df.name.iloc[ind] + " " + df.album.iloc[ind]
#     else:
#         search_text = df.artist.iloc[ind] + " " + df.name.iloc[ind]
#     try:
#         search_result = QQMusic.search(search_text).data[0]
#         lyric = search_result.lyric
#         lyric.extract()
#         lyric = lyric.lyric

#         # add lyrics and timestamps to main dataframe
#         lyric_list = re.findall(lyric_r, lyric)
#         time_list = re.findall(time_r, lyric)
#         lyric_string = ','.join(lyric_list)
#         lyric_time = ','.join(time_list)
#         df.iloc[ind,29] = lyric_string
#         df.iloc[ind, 30] = lyric_time

#         if not lyric_list:
#             df.iloc[ind,29] = lyric
# #         # save to csv
# #         df.to_csv('df_tracks_master_lyrics.csv')
# # save to sql:
#         df.to_sql(name = 'master', con = db_connection, if_exists = 'replace', index = False)

#     except:
#         pass

#     try:
#         # create a dataframe for each song and save in sql
#         df_temp = pd.DataFrame({'time':time_list,'lyrics':lyric_list})
#         name_temp = df.iloc[ind,0]
#         df_temp.to_sql(name = name_temp, con = db_connection, if_exists = 'replace', index = False)
#     except:
#         pass

#     time.sleep(random.uniform(1,4))
```


```python
# Import the results from the final dataframe table in SQL
db_connection = sqlite3.connect(
    '/Users/stuart/Desktop/Spoty-Linguist-Project/master_sql_db.db.sqlite')
df_tracks_master_clean_lyrics = sql.read_sql('SELECT * FROM master',
                                             con=db_connection)
df_tracks_master_clean_lyrics.shape
```




    (54326, 31)



### 3.2. Add Genre of songs (from Spotify)


```python
df_tracks_master_clean_lyrics_genre = df_tracks_master_clean_lyrics.merge(
    artists_df[['artist_uri',
                'genres']], on='artist_uri', how='left').drop_duplicates()
df_tracks_master_clean_lyrics_genre.to_csv(
    'df_tracks_master_clean_lyrics_genre.csv')
```


```python
def add_genre_0(genre_0):
    try:
        genre_reg = re.compile(r'\w+')
        genre_list = re.findall(genre_reg, genre_0)
        return genre_list[0]
    except:
        return np.nan


def add_genre_1(genre_1):
    try:
        genre_reg = re.compile(r'\w+')
        genre_list = re.findall(genre_reg, genre_1)
        return genre_list[1]
    except:
        return np.nan


def add_genre_2(genre_2):
    try:
        genre_reg = re.compile(r'\w+')
        genre_list = re.findall(genre_reg, genre_2)
        return genre_list[2]
    except:
        return np.nan
```


```python
df_tracks_master_clean_lyrics_genre[
    'genre_0'] = df_tracks_master_clean_lyrics_genre['genres'].apply(
        add_genre_0)
df_tracks_master_clean_lyrics_genre[
    'genre_1'] = df_tracks_master_clean_lyrics_genre['genres'].apply(
        add_genre_1)
df_tracks_master_clean_lyrics_genre[
    'genre_2'] = df_tracks_master_clean_lyrics_genre['genres'].apply(
        add_genre_2)
```


```python
df_tracks_master_clean_lyrics_genre['genre_0'].isnull().sum()
```




    34497




```python
df_tracks_master_clean_lyrics_genre.to_csv(
    'df_tracks_master_clean_lyrics_genre')
```


```python
df_tracks_master_clean_lyrics_genre = pd.read_csv(
    '/Users/stuart/Desktop/Spoty-Linguist-Project/df_tracks_master_clean_lyrics_genre.csv',
    sep=',',
    index_col=0)
```

At this stage 768 lyrics are null. None of the artists are null. 

## 4. Language Difficulty Scoring

#### 4.1. HSK Scores
Many people studying mandarin will follow the HSK testing system. In this system the language learning process is divided into 6 grades (HSK 1 to HSK 6). In each stage the amount of characters /words a student is required to recognise increases:

- HSK1: 300 characters / 500 words
- HSK2: 600 characters / 1300 words
- HSK3: 900 characters / 2300 words
- HSK4: 1200 characters / 3300 words
- HSK5: 1500 characters / 4300 words
- HSK6 1800 characters / 5500 words

Using this system I gave each song a score for each HSK level to show what percentage of the lyrics would be known by a person at that level. I utilised CSV files of mandarin vocab at each HSK level。

Chinese words can be made of one or more characters. For example 'I' is formed by one character 我, where as library is formed by 3 character 图书馆. I therefore  used 'SnowNLP' a mandarin NLP Python package to tokenise the lyrics.


```python
# SnowNLP example:
print('I met my friends in the library')
print('我在图书馆遇到了我的朋友')
SnowNLP(u'我在图书馆遇到了我的朋友').words
```

    I met my friends in the library
    我在图书馆遇到了我的朋友





    ['我', '在', '图书馆', '遇到', '了', '我', '的', '朋友']




```python
# Import HSK Lists: CSV files with words at each HSK level
csv_file_hsk1 = "/Users/stuart/Desktop/GA2/DSI20-lessons/projects/project-capstone/SL1/hsk_csv-master/hsk1.csv"
hsk1 = pd.read_csv(csv_file_hsk1, sep=',', header=None)
hsk1.columns = ['mandarin', 'pinyin', 'english']

csv_file_hsk2 = "/Users/stuart/Desktop/GA2/DSI20-lessons/projects/project-capstone/SL1/hsk_csv-master/hsk2.csv"
hsk2 = pd.read_csv(csv_file_hsk2, sep=',', header=None)
hsk2.columns = ['mandarin', 'pinyin', 'english']

csv_file_hsk3 = "/Users/stuart/Desktop/GA2/DSI20-lessons/projects/project-capstone/SL1/hsk_csv-master/hsk3.csv"
hsk3 = pd.read_csv(csv_file_hsk3, sep=',', header=None)
hsk3.columns = ['mandarin', 'pinyin', 'english']

csv_file_hsk4 = "/Users/stuart/Desktop/GA2/DSI20-lessons/projects/project-capstone/SL1/hsk_csv-master/hsk4.csv"
hsk4 = pd.read_csv(csv_file_hsk4, sep=',', header=None)
hsk4.columns = ['mandarin', 'pinyin', 'english']

csv_file_hsk5 = "/Users/stuart/Desktop/GA2/DSI20-lessons/projects/project-capstone/SL1/hsk_csv-master/hsk5.csv"
hsk5 = pd.read_csv(csv_file_hsk5, sep=',', header=None)
hsk5.columns = ['mandarin', 'pinyin', 'english']

csv_file_hsk6 = "/Users/stuart/Desktop/GA2/DSI20-lessons/projects/project-capstone/SL1/hsk_csv-master/hsk6.csv"
hsk6 = pd.read_csv(csv_file_hsk6, sep=',', header=None)
hsk6.columns = ['mandarin', 'pinyin', 'english']
```


```python
# expand hsk lists to also include individual characters
def expand_hsk(x):
    elist = []
    for i in x.mandarin:
        for j in i:
            elist.append(j)
    temp = pd.DataFrame(elist).drop_duplicates()
    temp['pinyin'] = np.nan
    temp['english'] = np.nan
    temp.columns = ['mandarin', 'pinyin', 'english']
    x = x.merge(temp, how='outer', on='mandarin')
    x = x.drop(columns=['pinyin_y', 'english_y'])
    x.columns = ['mandarin', 'pinyin', 'english']
    return x


hsk1 = expand_hsk(hsk1)
hsk2 = expand_hsk(hsk2)
hsk3 = expand_hsk(hsk3)
hsk4 = expand_hsk(hsk4)
hsk5 = expand_hsk(hsk5)
hsk6 = expand_hsk(hsk6)
```


```python
# Create a column that labels each row as HSK level x
hsk1['hsk'] = 1
hsk2['hsk'] = 2
hsk3['hsk'] = 3
hsk4['hsk'] = 4
hsk5['hsk'] = 5
hsk6['hsk'] = 6
```


```python
# HSK1 should include all HSK1, HSK6 should include HSK1-HSK6
hsk2 = hsk2.append(hsk1)
hsk3 = hsk3.append(hsk2)
hsk4 = hsk4.append(hsk3)
hsk5 = hsk5.append(hsk4)
hsk6 = hsk6.append(hsk5)
```


```python
# Create a master HSK DF which includes all vocab and their HSK level
hsk6 = hsk6.sort_values(by='hsk')
hsk_master = hsk6.copy()
```


```python
# Drop any duplicates
hsk_master = hsk_master.drop_duplicates('mandarin')
```


```python
# Function take text and return df with HSK scores for each word or individual character:


def text_hsk_split(text):
    # tokenise the text: extract words (typically 1-2 characters)
    from snownlp import SnowNLP
    text = SnowNLP(text)
    lyrics_tokenised_df = pd.DataFrame(list(text.tags),
                                       columns=['mandarin', 'type'])
    # drop rows where there is only a comma and also drop duplicates:
    lyrics_tokenised_df = lyrics_tokenised_df[
        lyrics_tokenised_df["mandarin"].replace(
            ",", np.nan).notnull()].drop_duplicates()

    # add hsk levels to the df by left merging with HSK master df
    lyrics_tokenised_hsk_df = pd.merge(lyrics_tokenised_df,
                                       hsk_master,
                                       on="mandarin",
                                       how='left')

    # # for all nan values, break down into individual character and add to list
    lyrics_tokenised_hsk_nan_df = lyrics_tokenised_hsk_df[
        lyrics_tokenised_hsk_df['hsk'].isnull(
        )]  # filter for all that were hsk nan
    lyrics_tokenised_hsk_nan_df = lyrics_tokenised_hsk_nan_df[
        'mandarin'].str.extract(r'(\w)')  # extract indivual characters
    lyrics_tokenised_hsk_nan_df.columns = ['mandarin']
    lyrics_tokenised_hsk_nan_df = pd.merge(lyrics_tokenised_hsk_nan_df,
                                           hsk_master,
                                           on="mandarin",
                                           how='left')  # add hsk info
    lyrics_tokenised_hsk_nan_df = lyrics_tokenised_hsk_nan_df.drop_duplicates()
    lyrics_tokenised_hsk_not_nan_df = lyrics_tokenised_hsk_df[
        lyrics_tokenised_hsk_df['hsk'].notnull(
        )]  # filter original df for all that were not nan

    lyrics_tokenised_hsk_final_df = lyrics_tokenised_hsk_not_nan_df.append(
        lyrics_tokenised_hsk_nan_df)
    return lyrics_tokenised_hsk_final_df
```


```python
def check_hsk_levels(lyrics_tokenised_hsk_final_df):
    try:
        df = lyrics_tokenised_hsk_final_df.copy(
        )  # function to take df and return score

        hsk1_per = df[(df.hsk == 1)].shape[0] / df.hsk.shape[0]
        hsk2_per = df[(df.hsk == 2) | (df.hsk == 1)].shape[0] / df.hsk.shape[0]
        hsk3_per = df[(df.hsk == 3) | (df.hsk == 2) |
                      (df.hsk == 1)].shape[0] / df.hsk.shape[0]
        hsk4_per = df[(df.hsk == 4) | (df.hsk == 3) | (df.hsk == 2) |
                      (df.hsk == 1)].shape[0] / df.hsk.shape[0]
        hsk5_per = df[(df.hsk == 5) | (df.hsk == 4) | (df.hsk == 3) |
                      (df.hsk == 2) | (df.hsk == 1)].shape[0] / df.hsk.shape[0]
        hsk6_per = df[(df.hsk == 6) | (df.hsk == 5) | (df.hsk == 4) |
                      (df.hsk == 3) | (df.hsk == 2) |
                      (df.hsk == 1)].shape[0] / df.hsk.shape[0]
        results = [hsk1_per, hsk2_per, hsk3_per, hsk4_per, hsk5_per, hsk6_per]
        return results
    except:
        return [np.nan for i in range(6)]  # 6 nan values
```


```python
# Filter out any other non-chinese characters from the lyrics
def getChinese(context):
    filtrate = re.compile(u'[^\u4E00-\u9FA5]')  # non-Chinese unicode range
    context = filtrate.sub(r'', context)  # remove all non-Chinese characters
    return context
```


```python
# Filter out songs with less than 8 characters
df_tracks_master_clean2_lyrics__genre = df_tracks_master_clean_lyrics_genre.copy(
)
df_tracks_master_clean2_lyrics__genre[
    'lyrics'] = df_tracks_master_clean2_lyrics__genre['lyrics'].astype(str)
df_tracks_master_clean2_lyrics__genre[
    'lyrics'] = df_tracks_master_clean2_lyrics__genre['lyrics'].apply(
        getChinese)
df_tracks_master_clean2_lyrics__genre[
    'lyrics_len'] = df_tracks_master_clean2_lyrics__genre['lyrics'].apply(
        lambda x: len(x))
df_tracks_master_clean2_lyrics__genre = df_tracks_master_clean2_lyrics__genre[
    df_tracks_master_clean2_lyrics__genre['lyrics_len'] >= 8]
```


```python
df_tracks_master_clean2_lyrics__genre = df_tracks_master_clean2_lyrics__genre.drop_duplicates(
    'lyrics')
```


```python
df_tracks_master_clean2_lyrics__genre.shape
```




    (35942, 33)




```python
df_tracks_master_clean2_lyrics_genre_hsk = df_tracks_master_clean2_lyrics__genre.copy(
)
# Run the above two functions into the main dataframe:
for ind, i in tqdm(enumerate(df_tracks_master_clean2_lyrics_genre_hsk.lyrics)):
    try:
        temp = text_hsk_split(i)
        temp = check_hsk_levels(temp)
    except:
        temp = [np.nan for i in range(6)]  # 6 nan values
        pass

    index_temp = df_tracks_master_clean2_lyrics_genre_hsk.index[ind]

    df_tracks_master_clean2_lyrics_genre_hsk.loc[index_temp, 'hsk1'] = temp[0]
    df_tracks_master_clean2_lyrics_genre_hsk.loc[index_temp, 'hsk2'] = temp[1]
    df_tracks_master_clean2_lyrics_genre_hsk.loc[index_temp, 'hsk3'] = temp[2]
    df_tracks_master_clean2_lyrics_genre_hsk.loc[index_temp, 'hsk4'] = temp[3]
    df_tracks_master_clean2_lyrics_genre_hsk.loc[index_temp, 'hsk5'] = temp[4]
    df_tracks_master_clean2_lyrics_genre_hsk.loc[index_temp, 'hsk6'] = temp[5]

df_tracks_master_clean2_lyrics_genre_hsk = df_tracks_master_clean2_lyrics_genre_hsk.drop_duplicates(
    'name').copy()
df_tracks_master_clean2_lyrics_genre_hsk.to_csv(
    'df_tracks_master_clean2_lyrics_genre_hsk.csv')
```

    35942it [1:28:18,  6.78it/s]



```python
df_tracks_master_clean2_lyrics_genre_hsk.shape
```




    (35942, 39)




```python
# Assign a HSK level to each song based on the vocabulary that is covered at that HSK level
df_tracks_master_clean2_lyrics_genre_hsk['hsk_level'] = 6
df_tracks_master_clean2_lyrics_genre_hsk['hsk_level'][
    df_tracks_master_clean2_lyrics_genre_hsk['hsk5'] >= 0.8] = 5
df_tracks_master_clean2_lyrics_genre_hsk['hsk_level'][
    df_tracks_master_clean2_lyrics_genre_hsk['hsk4'] >= 0.8] = 4
df_tracks_master_clean2_lyrics_genre_hsk['hsk_level'][
    df_tracks_master_clean2_lyrics_genre_hsk['hsk3'] >= 0.8] = 3
df_tracks_master_clean2_lyrics_genre_hsk['hsk_level'][
    df_tracks_master_clean2_lyrics_genre_hsk['hsk2'] >= 0.75] = 2
df_tracks_master_clean2_lyrics_genre_hsk['hsk_level'][
    df_tracks_master_clean2_lyrics_genre_hsk['hsk1'] >= 0.75] = 1

# Create binary classification for each song. E.g. is it suitable for HSK 2 learner at 80% coverage? Yes/ No
df_tracks_master_clean2_lyrics_genre_hsk['hsk_level_6_80p'] = 1
df_tracks_master_clean2_lyrics_genre_hsk['hsk_level_5_80p'] = 0
df_tracks_master_clean2_lyrics_genre_hsk['hsk_level_4_80p'] = 0
df_tracks_master_clean2_lyrics_genre_hsk['hsk_level_3_80p'] = 0
df_tracks_master_clean2_lyrics_genre_hsk['hsk_level_2_80p'] = 0
df_tracks_master_clean2_lyrics_genre_hsk['hsk_level_1_80p'] = 0

df_tracks_master_clean2_lyrics_genre_hsk['hsk_level_5_80p'][
    df_tracks_master_clean2_lyrics_genre_hsk['hsk5'] >= 0.8] = 1
df_tracks_master_clean2_lyrics_genre_hsk['hsk_level_4_80p'][
    df_tracks_master_clean2_lyrics_genre_hsk['hsk4'] >= 0.8] = 1
df_tracks_master_clean2_lyrics_genre_hsk['hsk_level_3_80p'][
    df_tracks_master_clean2_lyrics_genre_hsk['hsk3'] >= 0.8] = 1
df_tracks_master_clean2_lyrics_genre_hsk['hsk_level_2_80p'][
    df_tracks_master_clean2_lyrics_genre_hsk['hsk2'] >= 0.8] = 1
df_tracks_master_clean2_lyrics_genre_hsk['hsk_level_1_80p'][
    df_tracks_master_clean2_lyrics_genre_hsk['hsk1'] >= 0.8] = 1

df_tracks_master_clean2_lyrics_genre_hsk['hsk_level_5_75p'] = 0
df_tracks_master_clean2_lyrics_genre_hsk['hsk_level_4_75p'] = 0
df_tracks_master_clean2_lyrics_genre_hsk['hsk_level_3_75p'] = 0
df_tracks_master_clean2_lyrics_genre_hsk['hsk_level_2_75p'] = 0
df_tracks_master_clean2_lyrics_genre_hsk['hsk_level_1_75p'] = 0

df_tracks_master_clean2_lyrics_genre_hsk['hsk_level_5_75p'][
    df_tracks_master_clean2_lyrics_genre_hsk['hsk5'] >= 0.75] = 1
df_tracks_master_clean2_lyrics_genre_hsk['hsk_level_4_75p'][
    df_tracks_master_clean2_lyrics_genre_hsk['hsk4'] >= 0.75] = 1
df_tracks_master_clean2_lyrics_genre_hsk['hsk_level_3_75p'][
    df_tracks_master_clean2_lyrics_genre_hsk['hsk3'] >= 0.75] = 1
df_tracks_master_clean2_lyrics_genre_hsk['hsk_level_2_75p'][
    df_tracks_master_clean2_lyrics_genre_hsk['hsk2'] >= 0.75] = 1
df_tracks_master_clean2_lyrics_genre_hsk['hsk_level_1_75p'][
    df_tracks_master_clean2_lyrics_genre_hsk['hsk1'] >= 0.75] = 1

display(df_tracks_master_clean2_lyrics_genre_hsk['hsk_level'].value_counts(
    normalize=False))
df_tracks_master_clean2_lyrics_genre_hsk['hsk_level'].value_counts(
    normalize=True)
```


    5    20883
    4    12142
    6     1780
    3     1037
    2       90
    1        6
    Name: hsk_level, dtype: int64





    5    0.581084
    4    0.337860
    6    0.049530
    3    0.028855
    2    0.002504
    1    0.000167
    Name: hsk_level, dtype: float64




```python
df_tracks_master_clean2_lyrics_genre_hsk.to_csv(
    'df_tracks_master_clean2_lyrics_genre_hsk.csv')
```


```python
df_tracks_master_clean2_lyrics_genre_hsk = pd.read_csv(
    '/Users/stuart/Desktop/Spoty-Linguist-Project/df_tracks_master_clean2_lyrics_genre_hsk.csv',
    sep=',',
    index_col=0)
df_tracks_master_clean2_lyrics_genre_hsk.shape
```




    (35938, 54)




```python
df_tracks_master_clean2_lyrics_genre_hsk.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>artist_href</th>
      <th>artist</th>
      <th>artist_uri</th>
      <th>artist_type</th>
      <th>song_available_markets</th>
      <th>song_duration_ms</th>
      <th>is_explicit</th>
      <th>song_href</th>
      <th>preview_url</th>
      <th>track_number</th>
      <th>song_type</th>
      <th>length</th>
      <th>popularity</th>
      <th>album</th>
      <th>album_type</th>
      <th>album_artwork_link</th>
      <th>album_tracks_num</th>
      <th>album_uri</th>
      <th>album_release_date</th>
      <th>acousticness</th>
      <th>danceability</th>
      <th>energy</th>
      <th>instrumentalness</th>
      <th>liveness</th>
      <th>loudness</th>
      <th>speechiness</th>
      <th>tempo</th>
      <th>time_signature</th>
      <th>lyrics</th>
      <th>time_stamp</th>
      <th>genres</th>
      <th>lyrics_len</th>
      <th>hsk1</th>
      <th>hsk2</th>
      <th>hsk3</th>
      <th>hsk4</th>
      <th>hsk5</th>
      <th>hsk6</th>
      <th>hsk_level</th>
      <th>hsk_level_6_80p</th>
      <th>hsk_level_5_80p</th>
      <th>hsk_level_4_80p</th>
      <th>hsk_level_3_80p</th>
      <th>hsk_level_2_80p</th>
      <th>hsk_level_1_80p</th>
      <th>hsk_level_5_75p</th>
      <th>hsk_level_4_75p</th>
      <th>hsk_level_3_75p</th>
      <th>hsk_level_2_75p</th>
      <th>hsk_level_1_75p</th>
      <th>genre_0</th>
      <th>genre_1</th>
      <th>genre_2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>英雄 (Live)</td>
      <td>https://api.spotify.com/v1/artists/2elBjNSdBE2...</td>
      <td>Jay Chou</td>
      <td>spotify:artist:2elBjNSdBE2Y3f0j1mjrql</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>182040.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/1i31BYKv1La3...</td>
      <td>https://p.scdn.co/mp3-preview/191037a026d9e098...</td>
      <td>1.0</td>
      <td>track</td>
      <td>182040.0</td>
      <td>31.0</td>
      <td>周杰倫地表最強世界巡迴演唱會</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273494cdb...</td>
      <td>25.0</td>
      <td>spotify:album:5lqE2qT3nGLW6FGoJYfwUT</td>
      <td>2019-11-01</td>
      <td>0.000172</td>
      <td>0.522</td>
      <td>0.908</td>
      <td>0.002500</td>
      <td>0.458</td>
      <td>-8.747</td>
      <td>0.0596</td>
      <td>118.987</td>
      <td>4.0</td>
      <td>英雄词曲人生不是一个人的游戏一起奋斗一起超越一起杀吧兄弟好战好胜战胜逆命管他天赋够不够我们都...</td>
      <td>00:00.00,00:10.81,00:21.62,00:32.43,00:34.37,0...</td>
      <td>NaN</td>
      <td>288</td>
      <td>0.280303</td>
      <td>0.469697</td>
      <td>0.583333</td>
      <td>0.666667</td>
      <td>0.856061</td>
      <td>0.946970</td>
      <td>5</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>雙截棍 (Live)</td>
      <td>https://api.spotify.com/v1/artists/2elBjNSdBE2...</td>
      <td>Jay Chou</td>
      <td>spotify:artist:2elBjNSdBE2Y3f0j1mjrql</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>145173.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/77uwbOEaj6nM...</td>
      <td>https://p.scdn.co/mp3-preview/80c09c504a54344b...</td>
      <td>2.0</td>
      <td>track</td>
      <td>145173.0</td>
      <td>30.0</td>
      <td>周杰倫地表最強世界巡迴演唱會</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273494cdb...</td>
      <td>25.0</td>
      <td>spotify:album:5lqE2qT3nGLW6FGoJYfwUT</td>
      <td>2019-11-01</td>
      <td>0.002810</td>
      <td>0.461</td>
      <td>0.953</td>
      <td>0.000000</td>
      <td>0.424</td>
      <td>-6.440</td>
      <td>0.1360</td>
      <td>101.037</td>
      <td>4.0</td>
      <td>双截棍词曲岩烧店的烟味弥漫店里面的妈妈桑教拳脚武术的老板练铁沙掌硬底子功夫最擅长还会金钟罩铁...</td>
      <td>00:00.00,00:09.79,00:19.58,00:29.37,00:31.72,0...</td>
      <td>NaN</td>
      <td>370</td>
      <td>0.320000</td>
      <td>0.460000</td>
      <td>0.670000</td>
      <td>0.810000</td>
      <td>0.870000</td>
      <td>0.980000</td>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>開不了口 (Live)</td>
      <td>https://api.spotify.com/v1/artists/2elBjNSdBE2...</td>
      <td>Jay Chou</td>
      <td>spotify:artist:2elBjNSdBE2Y3f0j1mjrql</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>272973.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/5OR1Fbd7RSI1...</td>
      <td>https://p.scdn.co/mp3-preview/82a62689347391e7...</td>
      <td>3.0</td>
      <td>track</td>
      <td>272973.0</td>
      <td>32.0</td>
      <td>周杰倫地表最強世界巡迴演唱會</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273494cdb...</td>
      <td>25.0</td>
      <td>spotify:album:5lqE2qT3nGLW6FGoJYfwUT</td>
      <td>2019-11-01</td>
      <td>0.001760</td>
      <td>0.519</td>
      <td>0.612</td>
      <td>0.000002</td>
      <td>0.564</td>
      <td>-10.634</td>
      <td>0.0267</td>
      <td>139.944</td>
      <td>4.0</td>
      <td>开不了口词曲才离开没多久就开始担心今天的你过得好不好整个画面是你嘴嘟嘟那可爱的模样还有在你身...</td>
      <td>00:00.00,00:24.00,00:48.00,01:12.01,01:14.58,0...</td>
      <td>NaN</td>
      <td>299</td>
      <td>0.404762</td>
      <td>0.595238</td>
      <td>0.833333</td>
      <td>0.892857</td>
      <td>0.928571</td>
      <td>0.988095</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>床邊故事 (Live)</td>
      <td>https://api.spotify.com/v1/artists/2elBjNSdBE2...</td>
      <td>Jay Chou</td>
      <td>spotify:artist:2elBjNSdBE2Y3f0j1mjrql</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>220240.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/7MmT3xzugKwe...</td>
      <td>https://p.scdn.co/mp3-preview/5316d5602f55d671...</td>
      <td>4.0</td>
      <td>track</td>
      <td>220240.0</td>
      <td>29.0</td>
      <td>周杰倫地表最強世界巡迴演唱會</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273494cdb...</td>
      <td>25.0</td>
      <td>spotify:album:5lqE2qT3nGLW6FGoJYfwUT</td>
      <td>2019-11-01</td>
      <td>0.039800</td>
      <td>0.640</td>
      <td>0.844</td>
      <td>0.000005</td>
      <td>0.122</td>
      <td>-9.645</td>
      <td>0.0539</td>
      <td>106.033</td>
      <td>4.0</td>
      <td>床边故事词曲从前从前有只猫头鹰它站在屋顶屋顶后面一遍森林森林很安静安静的钢琴在大厅阁楼里仔细...</td>
      <td>00:00.00,00:06.45,00:12.91,00:19.37,00:20.74,0...</td>
      <td>NaN</td>
      <td>346</td>
      <td>0.368852</td>
      <td>0.500000</td>
      <td>0.655738</td>
      <td>0.803279</td>
      <td>0.901639</td>
      <td>0.950820</td>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>4</th>
      <td>夜曲+竊愛 (Live)</td>
      <td>https://api.spotify.com/v1/artists/2elBjNSdBE2...</td>
      <td>Jay Chou</td>
      <td>spotify:artist:2elBjNSdBE2Y3f0j1mjrql</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>222013.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/4vlzCpNsDFBa...</td>
      <td>https://p.scdn.co/mp3-preview/446b1630b52837ae...</td>
      <td>5.0</td>
      <td>track</td>
      <td>222013.0</td>
      <td>41.0</td>
      <td>周杰倫地表最強世界巡迴演唱會</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273494cdb...</td>
      <td>25.0</td>
      <td>spotify:album:5lqE2qT3nGLW6FGoJYfwUT</td>
      <td>2019-11-01</td>
      <td>0.028300</td>
      <td>0.538</td>
      <td>0.610</td>
      <td>0.005630</td>
      <td>0.386</td>
      <td>-10.513</td>
      <td>0.1680</td>
      <td>174.093</td>
      <td>4.0</td>
      <td>夜曲夜曲词曲窃爱词曲一群嗜血的蚂蚁被腐肉所吸引我面无表情看孤独的风景失去你失去你当鸽子不再象...</td>
      <td>00:00.00,00:03.37,00:03.70,00:04.45,00:05.10,0...</td>
      <td>NaN</td>
      <td>414</td>
      <td>0.233333</td>
      <td>0.306667</td>
      <td>0.526667</td>
      <td>0.680000</td>
      <td>0.813333</td>
      <td>0.933333</td>
      <td>5</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>



## 5. Data Cleaning

Some final data cleaning including removing genres that are not in scope. Removal of corrupt lyrics etc.


```python
df_tracks_master_clean2_lyrics_genre_hsk[[
    'hsk1', 'hsk2', 'hsk3', 'hsk4', 'hsk5', 'hsk6'
]].describe()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>hsk1</th>
      <th>hsk2</th>
      <th>hsk3</th>
      <th>hsk4</th>
      <th>hsk5</th>
      <th>hsk6</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>35938.000000</td>
      <td>35938.000000</td>
      <td>35938.000000</td>
      <td>35938.000000</td>
      <td>35938.000000</td>
      <td>35938.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.313522</td>
      <td>0.460375</td>
      <td>0.632173</td>
      <td>0.769797</td>
      <td>0.896004</td>
      <td>0.976780</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.073362</td>
      <td>0.088865</td>
      <td>0.091038</td>
      <td>0.080770</td>
      <td>0.059733</td>
      <td>0.039831</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.100000</td>
      <td>0.333333</td>
      <td>0.363636</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.266667</td>
      <td>0.405809</td>
      <td>0.577465</td>
      <td>0.724138</td>
      <td>0.867647</td>
      <td>0.970297</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>0.309258</td>
      <td>0.459184</td>
      <td>0.633333</td>
      <td>0.775000</td>
      <td>0.903614</td>
      <td>0.985507</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>0.357143</td>
      <td>0.515152</td>
      <td>0.689655</td>
      <td>0.822581</td>
      <td>0.934211</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>0.857143</td>
      <td>0.923077</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>




```python
# delete as lyrics are corrupt
df_tracks_master_clean2_lyrics_genre_hsk = df_tracks_master_clean2_lyrics_genre_hsk.drop(
    index=[178755, 53570, 240, 84568])
```


```python
df_tracks_master_clean2_lyrics_genre_hsk[[
    'hsk1', 'hsk2', 'hsk3', 'hsk4', 'hsk5', 'hsk6'
]].describe()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>hsk1</th>
      <th>hsk2</th>
      <th>hsk3</th>
      <th>hsk4</th>
      <th>hsk5</th>
      <th>hsk6</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>35938.000000</td>
      <td>35938.000000</td>
      <td>35938.000000</td>
      <td>35938.000000</td>
      <td>35938.000000</td>
      <td>35938.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.313522</td>
      <td>0.460375</td>
      <td>0.632173</td>
      <td>0.769797</td>
      <td>0.896004</td>
      <td>0.976780</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.073362</td>
      <td>0.088865</td>
      <td>0.091038</td>
      <td>0.080770</td>
      <td>0.059733</td>
      <td>0.039831</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.100000</td>
      <td>0.333333</td>
      <td>0.363636</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.266667</td>
      <td>0.405809</td>
      <td>0.577465</td>
      <td>0.724138</td>
      <td>0.867647</td>
      <td>0.970297</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>0.309258</td>
      <td>0.459184</td>
      <td>0.633333</td>
      <td>0.775000</td>
      <td>0.903614</td>
      <td>0.985507</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>0.357143</td>
      <td>0.515152</td>
      <td>0.689655</td>
      <td>0.822581</td>
      <td>0.934211</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>0.857143</td>
      <td>0.923077</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>




```python
df_tracks_master_clean2_lyrics_genre_hsk['genre_0'][
    df_tracks_master_clean2_lyrics_genre_hsk.genre_0 == 'c'] = 'c-pop'
```


```python
df_tracks_master_clean2_lyrics_genre_hsk['genre_1'][
    df_tracks_master_clean2_lyrics_genre_hsk.genre_1 == 'r'] = 'chinese r&b'
```


```python
df_tracks_master_clean2_lyrics_genre_hsk['genre_1'][
    df_tracks_master_clean2_lyrics_genre_hsk['genre_1'] ==
    'viral'] = 'chinese_viral_pop'
```


```python
df_tracks_master_clean2_lyrics_genre_hsk[
    df_tracks_master_clean2_lyrics_genre_hsk['genre_0'].isnull()]
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>artist_href</th>
      <th>artist</th>
      <th>artist_uri</th>
      <th>artist_type</th>
      <th>song_available_markets</th>
      <th>song_duration_ms</th>
      <th>is_explicit</th>
      <th>song_href</th>
      <th>preview_url</th>
      <th>track_number</th>
      <th>song_type</th>
      <th>length</th>
      <th>popularity</th>
      <th>album</th>
      <th>album_type</th>
      <th>album_artwork_link</th>
      <th>album_tracks_num</th>
      <th>album_uri</th>
      <th>album_release_date</th>
      <th>acousticness</th>
      <th>danceability</th>
      <th>energy</th>
      <th>instrumentalness</th>
      <th>liveness</th>
      <th>loudness</th>
      <th>speechiness</th>
      <th>tempo</th>
      <th>time_signature</th>
      <th>lyrics</th>
      <th>time_stamp</th>
      <th>genres</th>
      <th>lyrics_len</th>
      <th>hsk1</th>
      <th>hsk2</th>
      <th>hsk3</th>
      <th>hsk4</th>
      <th>hsk5</th>
      <th>hsk6</th>
      <th>hsk_level</th>
      <th>hsk_level_6_80p</th>
      <th>hsk_level_5_80p</th>
      <th>hsk_level_4_80p</th>
      <th>hsk_level_3_80p</th>
      <th>hsk_level_2_80p</th>
      <th>hsk_level_1_80p</th>
      <th>hsk_level_5_75p</th>
      <th>hsk_level_4_75p</th>
      <th>hsk_level_3_75p</th>
      <th>hsk_level_2_75p</th>
      <th>hsk_level_1_75p</th>
      <th>genre_0</th>
      <th>genre_1</th>
      <th>genre_2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>英雄 (Live)</td>
      <td>https://api.spotify.com/v1/artists/2elBjNSdBE2...</td>
      <td>Jay Chou</td>
      <td>spotify:artist:2elBjNSdBE2Y3f0j1mjrql</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>182040.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/1i31BYKv1La3...</td>
      <td>https://p.scdn.co/mp3-preview/191037a026d9e098...</td>
      <td>1.0</td>
      <td>track</td>
      <td>182040.0</td>
      <td>31.0</td>
      <td>周杰倫地表最強世界巡迴演唱會</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273494cdb...</td>
      <td>25.0</td>
      <td>spotify:album:5lqE2qT3nGLW6FGoJYfwUT</td>
      <td>2019-11-01</td>
      <td>0.000172</td>
      <td>0.522</td>
      <td>0.908</td>
      <td>0.002500</td>
      <td>0.4580</td>
      <td>-8.747</td>
      <td>0.0596</td>
      <td>118.987</td>
      <td>4.0</td>
      <td>英雄词曲人生不是一个人的游戏一起奋斗一起超越一起杀吧兄弟好战好胜战胜逆命管他天赋够不够我们都...</td>
      <td>00:00.00,00:10.81,00:21.62,00:32.43,00:34.37,0...</td>
      <td>NaN</td>
      <td>288</td>
      <td>0.280303</td>
      <td>0.469697</td>
      <td>0.583333</td>
      <td>0.666667</td>
      <td>0.856061</td>
      <td>0.946970</td>
      <td>5</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>雙截棍 (Live)</td>
      <td>https://api.spotify.com/v1/artists/2elBjNSdBE2...</td>
      <td>Jay Chou</td>
      <td>spotify:artist:2elBjNSdBE2Y3f0j1mjrql</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>145173.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/77uwbOEaj6nM...</td>
      <td>https://p.scdn.co/mp3-preview/80c09c504a54344b...</td>
      <td>2.0</td>
      <td>track</td>
      <td>145173.0</td>
      <td>30.0</td>
      <td>周杰倫地表最強世界巡迴演唱會</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273494cdb...</td>
      <td>25.0</td>
      <td>spotify:album:5lqE2qT3nGLW6FGoJYfwUT</td>
      <td>2019-11-01</td>
      <td>0.002810</td>
      <td>0.461</td>
      <td>0.953</td>
      <td>0.000000</td>
      <td>0.4240</td>
      <td>-6.440</td>
      <td>0.1360</td>
      <td>101.037</td>
      <td>4.0</td>
      <td>双截棍词曲岩烧店的烟味弥漫店里面的妈妈桑教拳脚武术的老板练铁沙掌硬底子功夫最擅长还会金钟罩铁...</td>
      <td>00:00.00,00:09.79,00:19.58,00:29.37,00:31.72,0...</td>
      <td>NaN</td>
      <td>370</td>
      <td>0.320000</td>
      <td>0.460000</td>
      <td>0.670000</td>
      <td>0.810000</td>
      <td>0.870000</td>
      <td>0.980000</td>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>開不了口 (Live)</td>
      <td>https://api.spotify.com/v1/artists/2elBjNSdBE2...</td>
      <td>Jay Chou</td>
      <td>spotify:artist:2elBjNSdBE2Y3f0j1mjrql</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>272973.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/5OR1Fbd7RSI1...</td>
      <td>https://p.scdn.co/mp3-preview/82a62689347391e7...</td>
      <td>3.0</td>
      <td>track</td>
      <td>272973.0</td>
      <td>32.0</td>
      <td>周杰倫地表最強世界巡迴演唱會</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273494cdb...</td>
      <td>25.0</td>
      <td>spotify:album:5lqE2qT3nGLW6FGoJYfwUT</td>
      <td>2019-11-01</td>
      <td>0.001760</td>
      <td>0.519</td>
      <td>0.612</td>
      <td>0.000002</td>
      <td>0.5640</td>
      <td>-10.634</td>
      <td>0.0267</td>
      <td>139.944</td>
      <td>4.0</td>
      <td>开不了口词曲才离开没多久就开始担心今天的你过得好不好整个画面是你嘴嘟嘟那可爱的模样还有在你身...</td>
      <td>00:00.00,00:24.00,00:48.00,01:12.01,01:14.58,0...</td>
      <td>NaN</td>
      <td>299</td>
      <td>0.404762</td>
      <td>0.595238</td>
      <td>0.833333</td>
      <td>0.892857</td>
      <td>0.928571</td>
      <td>0.988095</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>床邊故事 (Live)</td>
      <td>https://api.spotify.com/v1/artists/2elBjNSdBE2...</td>
      <td>Jay Chou</td>
      <td>spotify:artist:2elBjNSdBE2Y3f0j1mjrql</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>220240.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/7MmT3xzugKwe...</td>
      <td>https://p.scdn.co/mp3-preview/5316d5602f55d671...</td>
      <td>4.0</td>
      <td>track</td>
      <td>220240.0</td>
      <td>29.0</td>
      <td>周杰倫地表最強世界巡迴演唱會</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273494cdb...</td>
      <td>25.0</td>
      <td>spotify:album:5lqE2qT3nGLW6FGoJYfwUT</td>
      <td>2019-11-01</td>
      <td>0.039800</td>
      <td>0.640</td>
      <td>0.844</td>
      <td>0.000005</td>
      <td>0.1220</td>
      <td>-9.645</td>
      <td>0.0539</td>
      <td>106.033</td>
      <td>4.0</td>
      <td>床边故事词曲从前从前有只猫头鹰它站在屋顶屋顶后面一遍森林森林很安静安静的钢琴在大厅阁楼里仔细...</td>
      <td>00:00.00,00:06.45,00:12.91,00:19.37,00:20.74,0...</td>
      <td>NaN</td>
      <td>346</td>
      <td>0.368852</td>
      <td>0.500000</td>
      <td>0.655738</td>
      <td>0.803279</td>
      <td>0.901639</td>
      <td>0.950820</td>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>4</th>
      <td>夜曲+竊愛 (Live)</td>
      <td>https://api.spotify.com/v1/artists/2elBjNSdBE2...</td>
      <td>Jay Chou</td>
      <td>spotify:artist:2elBjNSdBE2Y3f0j1mjrql</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>222013.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/4vlzCpNsDFBa...</td>
      <td>https://p.scdn.co/mp3-preview/446b1630b52837ae...</td>
      <td>5.0</td>
      <td>track</td>
      <td>222013.0</td>
      <td>41.0</td>
      <td>周杰倫地表最強世界巡迴演唱會</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273494cdb...</td>
      <td>25.0</td>
      <td>spotify:album:5lqE2qT3nGLW6FGoJYfwUT</td>
      <td>2019-11-01</td>
      <td>0.028300</td>
      <td>0.538</td>
      <td>0.610</td>
      <td>0.005630</td>
      <td>0.3860</td>
      <td>-10.513</td>
      <td>0.1680</td>
      <td>174.093</td>
      <td>4.0</td>
      <td>夜曲夜曲词曲窃爱词曲一群嗜血的蚂蚁被腐肉所吸引我面无表情看孤独的风景失去你失去你当鸽子不再象...</td>
      <td>00:00.00,00:03.37,00:03.70,00:04.45,00:05.10,0...</td>
      <td>NaN</td>
      <td>414</td>
      <td>0.233333</td>
      <td>0.306667</td>
      <td>0.526667</td>
      <td>0.680000</td>
      <td>0.813333</td>
      <td>0.933333</td>
      <td>5</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>180168</th>
      <td>載歌載舞/詐肚痛</td>
      <td>https://api.spotify.com/v1/artists/14CRx1HAm3E...</td>
      <td>合輯</td>
      <td>spotify:artist:7iGIX7gvr9SPKdvzdsAFqD</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>66000.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/5hf55x6xKmJG...</td>
      <td>https://p.scdn.co/mp3-preview/cd5ff24297dc1e4b...</td>
      <td>10.0</td>
      <td>track</td>
      <td>66000.0</td>
      <td>1.0</td>
      <td>新不了情電影原聲帶</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273e5db8a...</td>
      <td>13.0</td>
      <td>spotify:album:4dvMzvDaWgkJcg0fSoAcqz</td>
      <td>1993-11-17</td>
      <td>0.301000</td>
      <td>0.661</td>
      <td>0.492</td>
      <td>0.000000</td>
      <td>0.3190</td>
      <td>-9.348</td>
      <td>0.0457</td>
      <td>108.936</td>
      <td>4.0</td>
      <td>载歌载舞曲女你行动型似阿飞男我其实唔系阿飞女尽地学轻松共游戏合尽地学轻松共游戏女男女合</td>
      <td>00:00.10,00:00.30,00:00.40,00:05.40,00:07.38,0...</td>
      <td>NaN</td>
      <td>43</td>
      <td>0.250000</td>
      <td>0.375000</td>
      <td>0.625000</td>
      <td>0.750000</td>
      <td>0.916667</td>
      <td>0.958333</td>
      <td>5</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>180169</th>
      <td>補鑊佬</td>
      <td>https://api.spotify.com/v1/artists/14CRx1HAm3E...</td>
      <td>合輯</td>
      <td>spotify:artist:3kqGWMGSayGUtYrAah9Quh</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>98000.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/3AVpVRnTSgY4...</td>
      <td>https://p.scdn.co/mp3-preview/099a86956e02e186...</td>
      <td>11.0</td>
      <td>track</td>
      <td>98000.0</td>
      <td>0.0</td>
      <td>新不了情電影原聲帶</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273e5db8a...</td>
      <td>13.0</td>
      <td>spotify:album:4dvMzvDaWgkJcg0fSoAcqz</td>
      <td>1993-11-17</td>
      <td>0.657000</td>
      <td>0.699</td>
      <td>0.423</td>
      <td>0.000008</td>
      <td>0.0698</td>
      <td>-8.917</td>
      <td>0.0365</td>
      <td>116.121</td>
      <td>4.0</td>
      <td>补镬佬词曲日日我夫君上街温野做奴奴在家理细矛但求两夫妻有乜野都做通街将镬补姜补铜煲日夜甘温得...</td>
      <td>00:00.00,00:04.47,00:08.94,00:13.02,00:17.19,0...</td>
      <td>NaN</td>
      <td>169</td>
      <td>0.207792</td>
      <td>0.311688</td>
      <td>0.467532</td>
      <td>0.688312</td>
      <td>0.818182</td>
      <td>0.909091</td>
      <td>5</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>180171</th>
      <td>黎明在望</td>
      <td>https://api.spotify.com/v1/artists/14CRx1HAm3E...</td>
      <td>合輯</td>
      <td>spotify:artist:2DnSZXdPcLmYSHiNgLPGJH</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>181000.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/7gvoDP1cyqK1...</td>
      <td>https://p.scdn.co/mp3-preview/5a1102619b97d1cf...</td>
      <td>13.0</td>
      <td>track</td>
      <td>181000.0</td>
      <td>4.0</td>
      <td>新不了情電影原聲帶</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273e5db8a...</td>
      <td>13.0</td>
      <td>spotify:album:4dvMzvDaWgkJcg0fSoAcqz</td>
      <td>1993-11-17</td>
      <td>0.961000</td>
      <td>0.341</td>
      <td>0.298</td>
      <td>0.000275</td>
      <td>0.3260</td>
      <td>-10.524</td>
      <td>0.0356</td>
      <td>100.783</td>
      <td>4.0</td>
      <td>黎明在望词曲忆起往事无限唏嘘感慨一生饱受挫败痛苦历风霜未怨命运有泪自抹干每在风雨夜拥抱慰藉时...</td>
      <td>00:00.00,00:06.64,00:10.03,00:21.25,00:31.93,0...</td>
      <td>NaN</td>
      <td>180</td>
      <td>0.200000</td>
      <td>0.280000</td>
      <td>0.430000</td>
      <td>0.670000</td>
      <td>0.860000</td>
      <td>0.970000</td>
      <td>5</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>180330</th>
      <td>The Racket</td>
      <td>https://api.spotify.com/v1/artists/6azt2KNXenl...</td>
      <td>Old Fashion</td>
      <td>spotify:artist:6azt2KNXenlXO7FCKGtE6R</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>202066.0</td>
      <td>1.0</td>
      <td>https://api.spotify.com/v1/tracks/5gznD9kw8Csp...</td>
      <td>https://p.scdn.co/mp3-preview/b729d4e6e416d2d7...</td>
      <td>1.0</td>
      <td>track</td>
      <td>202066.0</td>
      <td>0.0</td>
      <td>What Were We Thinking</td>
      <td>single</td>
      <td>https://i.scdn.co/image/ab67616d0000b2734c334d...</td>
      <td>4.0</td>
      <td>spotify:album:5XxtJqZY3wHEZDQgZ7jLpB</td>
      <td>2016-07-14</td>
      <td>0.000672</td>
      <td>0.555</td>
      <td>0.656</td>
      <td>0.022000</td>
      <td>0.6680</td>
      <td>-3.966</td>
      <td>0.0331</td>
      <td>104.974</td>
      <td>4.0</td>
      <td>浮士德词曲我脑袋摇了几下变得像个莎士比亚从没写过那么多情话你让想学吉他走路的时候想象你咬小指...</td>
      <td>00:00.00,00:10.16,00:20.33,00:30.49,00:32.02,0...</td>
      <td>NaN</td>
      <td>503</td>
      <td>0.307692</td>
      <td>0.505495</td>
      <td>0.642857</td>
      <td>0.763736</td>
      <td>0.901099</td>
      <td>0.983516</td>
      <td>5</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>180331</th>
      <td>Caroline</td>
      <td>https://api.spotify.com/v1/artists/6azt2KNXenl...</td>
      <td>Old Fashion</td>
      <td>spotify:artist:6azt2KNXenlXO7FCKGtE6R</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>207200.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/1WzCE4IUfk7G...</td>
      <td>https://p.scdn.co/mp3-preview/82a422055377af16...</td>
      <td>2.0</td>
      <td>track</td>
      <td>207200.0</td>
      <td>0.0</td>
      <td>What Were We Thinking</td>
      <td>single</td>
      <td>https://i.scdn.co/image/ab67616d0000b2734c334d...</td>
      <td>4.0</td>
      <td>spotify:album:5XxtJqZY3wHEZDQgZ7jLpB</td>
      <td>2016-07-14</td>
      <td>0.000008</td>
      <td>0.485</td>
      <td>0.834</td>
      <td>0.016400</td>
      <td>0.1180</td>
      <td>-4.901</td>
      <td>0.0344</td>
      <td>140.011</td>
      <td>4.0</td>
      <td>词曲没人能你想不想要回到你的过去你的歌词太简单了而我像能承受多少的赞美就能承受多少诋毁我待在...</td>
      <td>00:00.00,00:08.00,00:16.00,00:24.00,00:26.54,0...</td>
      <td>NaN</td>
      <td>399</td>
      <td>0.410256</td>
      <td>0.544872</td>
      <td>0.628205</td>
      <td>0.782051</td>
      <td>0.910256</td>
      <td>0.967949</td>
      <td>5</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
<p>21495 rows × 54 columns</p>
</div>




```python
sp.artist('potify:artist:2elBjNSdBE2Y3f0j1mjrql')
```




    {'external_urls': {'spotify': 'https://open.spotify.com/artist/2elBjNSdBE2Y3f0j1mjrql'},
     'followers': {'href': None, 'total': 2737866},
     'genres': ['c-pop', 'mandopop', 'taiwan pop', 'zhongguo feng'],
     'href': 'https://api.spotify.com/v1/artists/2elBjNSdBE2Y3f0j1mjrql',
     'id': '2elBjNSdBE2Y3f0j1mjrql',
     'images': [{'height': 640,
       'url': 'https://i.scdn.co/image/ab6761610000e5eba9989f6b9885c7aedc29c557',
       'width': 640},
      {'height': 320,
       'url': 'https://i.scdn.co/image/ab67616100005174a9989f6b9885c7aedc29c557',
       'width': 320},
      {'height': 160,
       'url': 'https://i.scdn.co/image/ab6761610000f178a9989f6b9885c7aedc29c557',
       'width': 160}],
     'name': 'Jay Chou',
     'popularity': 73,
     'type': 'artist',
     'uri': 'spotify:artist:2elBjNSdBE2Y3f0j1mjrql'}




```python
list_of_artists = df_tracks_master_clean2_lyrics_genre_hsk.artist_uri.drop_duplicates(
)
```


```python
list_of_artists
```




    0         spotify:artist:2elBjNSdBE2Y3f0j1mjrql
    8         spotify:artist:1xCrrnnj9xif5G0y3ie5dM
    73        spotify:artist:6TwanpBr3fIrx6ITQM9kc0
    75        spotify:artist:2iJUPCerX1OiPRuQXsStZH
    83        spotify:artist:3LyN3dzJjv35T1XcDysnZG
                              ...                  
    180160    spotify:artist:0pxtsY8RWW1IClFhKRHPqU
    180163    spotify:artist:2DnSZXdPcLmYSHiNgLPGJH
    180168    spotify:artist:7iGIX7gvr9SPKdvzdsAFqD
    180169    spotify:artist:3kqGWMGSayGUtYrAah9Quh
    180330    spotify:artist:6azt2KNXenlXO7FCKGtE6R
    Name: artist_uri, Length: 2199, dtype: object



Fix genres as previous work had too many errors:


```python
df_tracks_master_clean2_lyrics_genre_hsk_test = df_tracks_master_clean2_lyrics_genre_hsk.copy(
)
```


```python
df_tracks_master_clean2_lyrics_genre_hsk_test['Genre0']
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>artist_href</th>
      <th>artist</th>
      <th>artist_uri</th>
      <th>artist_type</th>
      <th>song_available_markets</th>
      <th>song_duration_ms</th>
      <th>is_explicit</th>
      <th>song_href</th>
      <th>preview_url</th>
      <th>track_number</th>
      <th>song_type</th>
      <th>length</th>
      <th>popularity</th>
      <th>album</th>
      <th>album_type</th>
      <th>album_artwork_link</th>
      <th>album_tracks_num</th>
      <th>album_uri</th>
      <th>album_release_date</th>
      <th>acousticness</th>
      <th>danceability</th>
      <th>energy</th>
      <th>instrumentalness</th>
      <th>liveness</th>
      <th>loudness</th>
      <th>speechiness</th>
      <th>tempo</th>
      <th>time_signature</th>
      <th>lyrics</th>
      <th>time_stamp</th>
      <th>genres</th>
      <th>lyrics_len</th>
      <th>hsk1</th>
      <th>hsk2</th>
      <th>hsk3</th>
      <th>hsk4</th>
      <th>hsk5</th>
      <th>hsk6</th>
      <th>hsk_level</th>
      <th>hsk_level_6_80p</th>
      <th>hsk_level_5_80p</th>
      <th>hsk_level_4_80p</th>
      <th>hsk_level_3_80p</th>
      <th>hsk_level_2_80p</th>
      <th>hsk_level_1_80p</th>
      <th>hsk_level_5_75p</th>
      <th>hsk_level_4_75p</th>
      <th>hsk_level_3_75p</th>
      <th>hsk_level_2_75p</th>
      <th>hsk_level_1_75p</th>
      <th>genre_0</th>
      <th>genre_1</th>
      <th>genre_2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>英雄 (Live)</td>
      <td>https://api.spotify.com/v1/artists/2elBjNSdBE2...</td>
      <td>Jay Chou</td>
      <td>spotify:artist:2elBjNSdBE2Y3f0j1mjrql</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>182040.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/1i31BYKv1La3...</td>
      <td>https://p.scdn.co/mp3-preview/191037a026d9e098...</td>
      <td>1.0</td>
      <td>track</td>
      <td>182040.0</td>
      <td>31.0</td>
      <td>周杰倫地表最強世界巡迴演唱會</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273494cdb...</td>
      <td>25.0</td>
      <td>spotify:album:5lqE2qT3nGLW6FGoJYfwUT</td>
      <td>2019-11-01</td>
      <td>0.000172</td>
      <td>0.522</td>
      <td>0.908</td>
      <td>0.002500</td>
      <td>0.4580</td>
      <td>-8.747</td>
      <td>0.0596</td>
      <td>118.987</td>
      <td>4.0</td>
      <td>英雄词曲人生不是一个人的游戏一起奋斗一起超越一起杀吧兄弟好战好胜战胜逆命管他天赋够不够我们都...</td>
      <td>00:00.00,00:10.81,00:21.62,00:32.43,00:34.37,0...</td>
      <td>NaN</td>
      <td>288</td>
      <td>0.280303</td>
      <td>0.469697</td>
      <td>0.583333</td>
      <td>0.666667</td>
      <td>0.856061</td>
      <td>0.946970</td>
      <td>5</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>雙截棍 (Live)</td>
      <td>https://api.spotify.com/v1/artists/2elBjNSdBE2...</td>
      <td>Jay Chou</td>
      <td>spotify:artist:2elBjNSdBE2Y3f0j1mjrql</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>145173.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/77uwbOEaj6nM...</td>
      <td>https://p.scdn.co/mp3-preview/80c09c504a54344b...</td>
      <td>2.0</td>
      <td>track</td>
      <td>145173.0</td>
      <td>30.0</td>
      <td>周杰倫地表最強世界巡迴演唱會</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273494cdb...</td>
      <td>25.0</td>
      <td>spotify:album:5lqE2qT3nGLW6FGoJYfwUT</td>
      <td>2019-11-01</td>
      <td>0.002810</td>
      <td>0.461</td>
      <td>0.953</td>
      <td>0.000000</td>
      <td>0.4240</td>
      <td>-6.440</td>
      <td>0.1360</td>
      <td>101.037</td>
      <td>4.0</td>
      <td>双截棍词曲岩烧店的烟味弥漫店里面的妈妈桑教拳脚武术的老板练铁沙掌硬底子功夫最擅长还会金钟罩铁...</td>
      <td>00:00.00,00:09.79,00:19.58,00:29.37,00:31.72,0...</td>
      <td>NaN</td>
      <td>370</td>
      <td>0.320000</td>
      <td>0.460000</td>
      <td>0.670000</td>
      <td>0.810000</td>
      <td>0.870000</td>
      <td>0.980000</td>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>開不了口 (Live)</td>
      <td>https://api.spotify.com/v1/artists/2elBjNSdBE2...</td>
      <td>Jay Chou</td>
      <td>spotify:artist:2elBjNSdBE2Y3f0j1mjrql</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>272973.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/5OR1Fbd7RSI1...</td>
      <td>https://p.scdn.co/mp3-preview/82a62689347391e7...</td>
      <td>3.0</td>
      <td>track</td>
      <td>272973.0</td>
      <td>32.0</td>
      <td>周杰倫地表最強世界巡迴演唱會</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273494cdb...</td>
      <td>25.0</td>
      <td>spotify:album:5lqE2qT3nGLW6FGoJYfwUT</td>
      <td>2019-11-01</td>
      <td>0.001760</td>
      <td>0.519</td>
      <td>0.612</td>
      <td>0.000002</td>
      <td>0.5640</td>
      <td>-10.634</td>
      <td>0.0267</td>
      <td>139.944</td>
      <td>4.0</td>
      <td>开不了口词曲才离开没多久就开始担心今天的你过得好不好整个画面是你嘴嘟嘟那可爱的模样还有在你身...</td>
      <td>00:00.00,00:24.00,00:48.00,01:12.01,01:14.58,0...</td>
      <td>NaN</td>
      <td>299</td>
      <td>0.404762</td>
      <td>0.595238</td>
      <td>0.833333</td>
      <td>0.892857</td>
      <td>0.928571</td>
      <td>0.988095</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>床邊故事 (Live)</td>
      <td>https://api.spotify.com/v1/artists/2elBjNSdBE2...</td>
      <td>Jay Chou</td>
      <td>spotify:artist:2elBjNSdBE2Y3f0j1mjrql</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>220240.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/7MmT3xzugKwe...</td>
      <td>https://p.scdn.co/mp3-preview/5316d5602f55d671...</td>
      <td>4.0</td>
      <td>track</td>
      <td>220240.0</td>
      <td>29.0</td>
      <td>周杰倫地表最強世界巡迴演唱會</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273494cdb...</td>
      <td>25.0</td>
      <td>spotify:album:5lqE2qT3nGLW6FGoJYfwUT</td>
      <td>2019-11-01</td>
      <td>0.039800</td>
      <td>0.640</td>
      <td>0.844</td>
      <td>0.000005</td>
      <td>0.1220</td>
      <td>-9.645</td>
      <td>0.0539</td>
      <td>106.033</td>
      <td>4.0</td>
      <td>床边故事词曲从前从前有只猫头鹰它站在屋顶屋顶后面一遍森林森林很安静安静的钢琴在大厅阁楼里仔细...</td>
      <td>00:00.00,00:06.45,00:12.91,00:19.37,00:20.74,0...</td>
      <td>NaN</td>
      <td>346</td>
      <td>0.368852</td>
      <td>0.500000</td>
      <td>0.655738</td>
      <td>0.803279</td>
      <td>0.901639</td>
      <td>0.950820</td>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>4</th>
      <td>夜曲+竊愛 (Live)</td>
      <td>https://api.spotify.com/v1/artists/2elBjNSdBE2...</td>
      <td>Jay Chou</td>
      <td>spotify:artist:2elBjNSdBE2Y3f0j1mjrql</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>222013.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/4vlzCpNsDFBa...</td>
      <td>https://p.scdn.co/mp3-preview/446b1630b52837ae...</td>
      <td>5.0</td>
      <td>track</td>
      <td>222013.0</td>
      <td>41.0</td>
      <td>周杰倫地表最強世界巡迴演唱會</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273494cdb...</td>
      <td>25.0</td>
      <td>spotify:album:5lqE2qT3nGLW6FGoJYfwUT</td>
      <td>2019-11-01</td>
      <td>0.028300</td>
      <td>0.538</td>
      <td>0.610</td>
      <td>0.005630</td>
      <td>0.3860</td>
      <td>-10.513</td>
      <td>0.1680</td>
      <td>174.093</td>
      <td>4.0</td>
      <td>夜曲夜曲词曲窃爱词曲一群嗜血的蚂蚁被腐肉所吸引我面无表情看孤独的风景失去你失去你当鸽子不再象...</td>
      <td>00:00.00,00:03.37,00:03.70,00:04.45,00:05.10,0...</td>
      <td>NaN</td>
      <td>414</td>
      <td>0.233333</td>
      <td>0.306667</td>
      <td>0.526667</td>
      <td>0.680000</td>
      <td>0.813333</td>
      <td>0.933333</td>
      <td>5</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>180168</th>
      <td>載歌載舞/詐肚痛</td>
      <td>https://api.spotify.com/v1/artists/14CRx1HAm3E...</td>
      <td>合輯</td>
      <td>spotify:artist:7iGIX7gvr9SPKdvzdsAFqD</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>66000.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/5hf55x6xKmJG...</td>
      <td>https://p.scdn.co/mp3-preview/cd5ff24297dc1e4b...</td>
      <td>10.0</td>
      <td>track</td>
      <td>66000.0</td>
      <td>1.0</td>
      <td>新不了情電影原聲帶</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273e5db8a...</td>
      <td>13.0</td>
      <td>spotify:album:4dvMzvDaWgkJcg0fSoAcqz</td>
      <td>1993-11-17</td>
      <td>0.301000</td>
      <td>0.661</td>
      <td>0.492</td>
      <td>0.000000</td>
      <td>0.3190</td>
      <td>-9.348</td>
      <td>0.0457</td>
      <td>108.936</td>
      <td>4.0</td>
      <td>载歌载舞曲女你行动型似阿飞男我其实唔系阿飞女尽地学轻松共游戏合尽地学轻松共游戏女男女合</td>
      <td>00:00.10,00:00.30,00:00.40,00:05.40,00:07.38,0...</td>
      <td>NaN</td>
      <td>43</td>
      <td>0.250000</td>
      <td>0.375000</td>
      <td>0.625000</td>
      <td>0.750000</td>
      <td>0.916667</td>
      <td>0.958333</td>
      <td>5</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>180169</th>
      <td>補鑊佬</td>
      <td>https://api.spotify.com/v1/artists/14CRx1HAm3E...</td>
      <td>合輯</td>
      <td>spotify:artist:3kqGWMGSayGUtYrAah9Quh</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>98000.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/3AVpVRnTSgY4...</td>
      <td>https://p.scdn.co/mp3-preview/099a86956e02e186...</td>
      <td>11.0</td>
      <td>track</td>
      <td>98000.0</td>
      <td>0.0</td>
      <td>新不了情電影原聲帶</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273e5db8a...</td>
      <td>13.0</td>
      <td>spotify:album:4dvMzvDaWgkJcg0fSoAcqz</td>
      <td>1993-11-17</td>
      <td>0.657000</td>
      <td>0.699</td>
      <td>0.423</td>
      <td>0.000008</td>
      <td>0.0698</td>
      <td>-8.917</td>
      <td>0.0365</td>
      <td>116.121</td>
      <td>4.0</td>
      <td>补镬佬词曲日日我夫君上街温野做奴奴在家理细矛但求两夫妻有乜野都做通街将镬补姜补铜煲日夜甘温得...</td>
      <td>00:00.00,00:04.47,00:08.94,00:13.02,00:17.19,0...</td>
      <td>NaN</td>
      <td>169</td>
      <td>0.207792</td>
      <td>0.311688</td>
      <td>0.467532</td>
      <td>0.688312</td>
      <td>0.818182</td>
      <td>0.909091</td>
      <td>5</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>180171</th>
      <td>黎明在望</td>
      <td>https://api.spotify.com/v1/artists/14CRx1HAm3E...</td>
      <td>合輯</td>
      <td>spotify:artist:2DnSZXdPcLmYSHiNgLPGJH</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>181000.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/7gvoDP1cyqK1...</td>
      <td>https://p.scdn.co/mp3-preview/5a1102619b97d1cf...</td>
      <td>13.0</td>
      <td>track</td>
      <td>181000.0</td>
      <td>4.0</td>
      <td>新不了情電影原聲帶</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273e5db8a...</td>
      <td>13.0</td>
      <td>spotify:album:4dvMzvDaWgkJcg0fSoAcqz</td>
      <td>1993-11-17</td>
      <td>0.961000</td>
      <td>0.341</td>
      <td>0.298</td>
      <td>0.000275</td>
      <td>0.3260</td>
      <td>-10.524</td>
      <td>0.0356</td>
      <td>100.783</td>
      <td>4.0</td>
      <td>黎明在望词曲忆起往事无限唏嘘感慨一生饱受挫败痛苦历风霜未怨命运有泪自抹干每在风雨夜拥抱慰藉时...</td>
      <td>00:00.00,00:06.64,00:10.03,00:21.25,00:31.93,0...</td>
      <td>NaN</td>
      <td>180</td>
      <td>0.200000</td>
      <td>0.280000</td>
      <td>0.430000</td>
      <td>0.670000</td>
      <td>0.860000</td>
      <td>0.970000</td>
      <td>5</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>180330</th>
      <td>The Racket</td>
      <td>https://api.spotify.com/v1/artists/6azt2KNXenl...</td>
      <td>Old Fashion</td>
      <td>spotify:artist:6azt2KNXenlXO7FCKGtE6R</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>202066.0</td>
      <td>1.0</td>
      <td>https://api.spotify.com/v1/tracks/5gznD9kw8Csp...</td>
      <td>https://p.scdn.co/mp3-preview/b729d4e6e416d2d7...</td>
      <td>1.0</td>
      <td>track</td>
      <td>202066.0</td>
      <td>0.0</td>
      <td>What Were We Thinking</td>
      <td>single</td>
      <td>https://i.scdn.co/image/ab67616d0000b2734c334d...</td>
      <td>4.0</td>
      <td>spotify:album:5XxtJqZY3wHEZDQgZ7jLpB</td>
      <td>2016-07-14</td>
      <td>0.000672</td>
      <td>0.555</td>
      <td>0.656</td>
      <td>0.022000</td>
      <td>0.6680</td>
      <td>-3.966</td>
      <td>0.0331</td>
      <td>104.974</td>
      <td>4.0</td>
      <td>浮士德词曲我脑袋摇了几下变得像个莎士比亚从没写过那么多情话你让想学吉他走路的时候想象你咬小指...</td>
      <td>00:00.00,00:10.16,00:20.33,00:30.49,00:32.02,0...</td>
      <td>NaN</td>
      <td>503</td>
      <td>0.307692</td>
      <td>0.505495</td>
      <td>0.642857</td>
      <td>0.763736</td>
      <td>0.901099</td>
      <td>0.983516</td>
      <td>5</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>180331</th>
      <td>Caroline</td>
      <td>https://api.spotify.com/v1/artists/6azt2KNXenl...</td>
      <td>Old Fashion</td>
      <td>spotify:artist:6azt2KNXenlXO7FCKGtE6R</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>207200.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/1WzCE4IUfk7G...</td>
      <td>https://p.scdn.co/mp3-preview/82a422055377af16...</td>
      <td>2.0</td>
      <td>track</td>
      <td>207200.0</td>
      <td>0.0</td>
      <td>What Were We Thinking</td>
      <td>single</td>
      <td>https://i.scdn.co/image/ab67616d0000b2734c334d...</td>
      <td>4.0</td>
      <td>spotify:album:5XxtJqZY3wHEZDQgZ7jLpB</td>
      <td>2016-07-14</td>
      <td>0.000008</td>
      <td>0.485</td>
      <td>0.834</td>
      <td>0.016400</td>
      <td>0.1180</td>
      <td>-4.901</td>
      <td>0.0344</td>
      <td>140.011</td>
      <td>4.0</td>
      <td>词曲没人能你想不想要回到你的过去你的歌词太简单了而我像能承受多少的赞美就能承受多少诋毁我待在...</td>
      <td>00:00.00,00:08.00,00:16.00,00:24.00,00:26.54,0...</td>
      <td>NaN</td>
      <td>399</td>
      <td>0.410256</td>
      <td>0.544872</td>
      <td>0.628205</td>
      <td>0.782051</td>
      <td>0.910256</td>
      <td>0.967949</td>
      <td>5</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
<p>35938 rows × 54 columns</p>
</div>




```python
len(list_of_artists)
```




    2199




```python
artists_genre_df = pd.DataFrame()
for i in tqdm(list_of_artists):
    empty_list = []
    empty_list.append(i)
    genres_list = sp.artist(i)['genres']
    [empty_list.append(j) for j in genres_list]
    artists_genre_df = artists_genre_df.append([empty_list])
artists_genre_df
```

    100%|███████████████████████████████████████████████████████████████████████████████| 2199/2199 [03:35<00:00, 10.19it/s]





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>spotify:artist:2elBjNSdBE2Y3f0j1mjrql</td>
      <td>c-pop</td>
      <td>mandopop</td>
      <td>taiwan pop</td>
      <td>zhongguo feng</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>0</th>
      <td>spotify:artist:1xCrrnnj9xif5G0y3ie5dM</td>
      <td>chinese r&amp;b</td>
      <td>mandopop</td>
      <td>taiwan pop</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>0</th>
      <td>spotify:artist:6TwanpBr3fIrx6ITQM9kc0</td>
      <td>chinese soundtrack</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>0</th>
      <td>spotify:artist:2iJUPCerX1OiPRuQXsStZH</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>0</th>
      <td>spotify:artist:3LyN3dzJjv35T1XcDysnZG</td>
      <td>mandopop</td>
      <td>taiwan pop</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>0</th>
      <td>spotify:artist:0pxtsY8RWW1IClFhKRHPqU</td>
      <td>chinese minyao</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>0</th>
      <td>spotify:artist:2DnSZXdPcLmYSHiNgLPGJH</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>0</th>
      <td>spotify:artist:7iGIX7gvr9SPKdvzdsAFqD</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>0</th>
      <td>spotify:artist:3kqGWMGSayGUtYrAah9Quh</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>0</th>
      <td>spotify:artist:6azt2KNXenlXO7FCKGtE6R</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
<p>2199 rows × 8 columns</p>
</div>




```python
artists_genre_df.columns = [
    'artist_uri', 'Genre1', 'Genre2', 'Genre3', 'Genre4', 'Genre5', 'Genre6',
    'Genre7'
]
```


```python
df_tracks_master_clean2_lyrics_genre_hsk = df_tracks_master_clean2_lyrics_genre_hsk.merge(
    artists_genre_df, on='artist_uri', how='left')
```


```python
df_tracks_master_clean2_lyrics_genre_hsk.to_csv(
    'df_tracks_master_clean2_lyrics_genre_hsk_v2.csv')
```


```python
df_tracks_master_clean2_lyrics_genre_hsk_v2 = pd.read_csv(
    '/Users/stuart/Desktop/Spoty-Linguist-Project/df_tracks_master_clean2_lyrics_genre_hsk_v2.csv',
    sep=',',
    index_col=0)
```


```python
df_tracks_master_clean2_lyrics_genre_hsk_v2.Genre1.value_counts()
```




    c-pop                 16816
    chinese indie          4673
    mandopop               4473
    cantopop               1330
    classic mandopop       1114
                          ...  
    sinogaze                  1
    english indie rock        1
    acoustic pop              1
    classic j-pop             1
    belgian dnb               1
    Name: Genre1, Length: 99, dtype: int64




```python
df_tracks_master_clean3_lyrics_genre_hsk_v2 = df_tracks_master_clean2_lyrics_genre_hsk_v2[
    (df_tracks_master_clean2_lyrics_genre_hsk_v2.Genre1 != 'indian metal')
    & (df_tracks_master_clean2_lyrics_genre_hsk_v2.Genre1 != 'japanese piano')
    & (df_tracks_master_clean2_lyrics_genre_hsk_v2.Genre1 != 'danish pop') &
    (df_tracks_master_clean2_lyrics_genre_hsk_v2.Genre1 !=
     'english indie rock') &
    (df_tracks_master_clean2_lyrics_genre_hsk_v2.Genre1 != 'belgian dnb')]
df_tracks_master_clean3_lyrics_genre_hsk_v2 = df_tracks_master_clean3_lyrics_genre_hsk_v2[
    (df_tracks_master_clean3_lyrics_genre_hsk_v2.Genre2 != 'russian dance pop')
    & (df_tracks_master_clean3_lyrics_genre_hsk_v2.Genre2 !=
       'japanese punk rock') &
    (df_tracks_master_clean3_lyrics_genre_hsk_v2.Genre2 != 'korean pop') &
    (df_tracks_master_clean3_lyrics_genre_hsk_v2.Genre2 != 'japanese r&b')]
df_tracks_master_clean3_lyrics_genre_hsk_v2 = df_tracks_master_clean3_lyrics_genre_hsk_v2[
    (df_tracks_master_clean3_lyrics_genre_hsk_v2.Genre3 != 'nordic folk metal')
    & (df_tracks_master_clean3_lyrics_genre_hsk_v2.Genre3 !=
       'japanese jazz fusion')]
```

I also want to urls that can open the song in a browser rather than in the api. Update the api columns to allow this:


```python
def modify_url(x):
    url_search = re.compile(r'(?<=tracks).*')
    url_clean = 'https://open.spotify.com/track'
    url_clean = url_clean + re.findall(url_search, x)[0]
    return url_clean
```


```python
df_tracks_master_clean3_lyrics_genre_hsk_v2[
    'weblink'] = df_tracks_master_clean3_lyrics_genre_hsk_v2['song_href']
df_tracks_master_clean3_lyrics_genre_hsk_v2[
    'weblink'] = df_tracks_master_clean3_lyrics_genre_hsk_v2['weblink'].apply(
        modify_url)
df_tracks_master_clean3_lyrics_genre_hsk_v2['weblink'][:10]
```




    0    https://open.spotify.com/track/1i31BYKv1La3wVC...
    1    https://open.spotify.com/track/77uwbOEaj6nMy6T...
    2    https://open.spotify.com/track/5OR1Fbd7RSI18Oo...
    3    https://open.spotify.com/track/7MmT3xzugKweEte...
    4    https://open.spotify.com/track/4vlzCpNsDFBaW99...
    5    https://open.spotify.com/track/2KrXsknYKiyTDFt...
    6    https://open.spotify.com/track/4WXwCqReqMXALrY...
    7    https://open.spotify.com/track/3M1YMhbHFUO1tzy...
    8    https://open.spotify.com/track/50Njtg1yk7ODjC0...
    9    https://open.spotify.com/track/2aOYHzYlLY80ju0...
    Name: weblink, dtype: object




```python
df_tracks_master_clean3_lyrics_genre_hsk_v2['hsk_level'].value_counts(
    normalize=True)
```




    5    0.578575
    4    0.306562
    2    0.055455
    6    0.049330
    3    0.008574
    1    0.001503
    Name: hsk_level, dtype: float64



I also want to urls that can open the song in a browser rather than in the api. Update the api columns to allow this:


```python
def modify_url(x):
    url_search = re.compile(r'(?<=tracks).*')
    url_clean = 'https://open.spotify.com/track'
    url_clean = url_clean + re.findall(url_search, x)[0]
    return url_clean
```


```python
df_tracks_master_clean3_lyrics_genre_hsk_v2[
    'weblink'] = df_tracks_master_clean3_lyrics_genre_hsk_v2['song_href']
df_tracks_master_clean3_lyrics_genre_hsk_v2[
    'weblink'] = df_tracks_master_clean3_lyrics_genre_hsk_v2['weblink'].apply(
        modify_url)
df_tracks_master_clean3_lyrics_genre_hsk_v2['weblink'][:10]
```


```python
df_tracks_master_clean3_lyrics_genre_hsk_v2.to_csv(
    'df_tracks_master_clean3_lyrics_genre_hsk_v2.csv')
```


```python
df_tracks_master_clean3_lyrics_genre_hsk_v2 = pd.read_csv(
    '/Users/stuart/Desktop/Spoty-Linguist-Project/df_tracks_master_clean3_lyrics_genre_hsk_v2.csv',
    sep=',',
    index_col=0)
```

## 7. Data Analysis

### 7.1. HSK Coverage
The KDE plots below demonstrate the distribution of song comprehension for each HSK level. At the lowest HSK levels 1 and 2 comprehension is low with HSK 1 / HSK 2 vocabulary covering on average 30% / 46% of lyrics in a song respectively. There are very few songs that are accessible at an 80% comprehension level (HSK 1: total of 4 songs, HSK 2: 32 songs). Nonethless, these outliers could be valuable to learners at this level where accessible authentic material is extremely limited.

Of course comprehension rates increase with HSK level and therefore the number of songs available at an 80% comprehenion increases too. At HSK 3 the number of songs available at 80% comprehension spikes to over 1000+ and at HSK 4 to 13,000+ and finally to 34,000+ for both HSK 5 and 6. This shows that even at HSK 3 and HSK 4 levels there is a large corpus of music that should be accessible to learners.

![image.png](attachment:image.png)


```python
df_tracks_master_clean3_lyrics_genre_hsk_v2 = pd.read_csv(
    '/Users/stuart/Desktop/Spoty-Linguist-Project/df_tracks_master_clean3_lyrics_genre_hsk_v2.csv',
    sep=',',
    index_col=0)
```


```python
# Create a dataframe with the number of songs available at each HSK Level assuming 80% coverage
hsk6_count = df_tracks_master_clean3_lyrics_genre_hsk_v2[
    'hsk_level_6_80p'].sum()
hsk5_count = df_tracks_master_clean3_lyrics_genre_hsk_v2[
    'hsk_level_5_80p'].sum()
hsk4_count = df_tracks_master_clean3_lyrics_genre_hsk_v2[
    'hsk_level_4_80p'].sum()
hsk3_count = df_tracks_master_clean3_lyrics_genre_hsk_v2[
    'hsk_level_3_80p'].sum()
hsk2_count = df_tracks_master_clean3_lyrics_genre_hsk_v2[
    'hsk_level_2_80p'].sum()
hsk1_count = df_tracks_master_clean3_lyrics_genre_hsk_v2[
    'hsk_level_1_80p'].sum()

hsk_count_df = pd.DataFrame(
    [hsk6_count, hsk5_count, hsk4_count, hsk3_count, hsk2_count, hsk1_count],
    index=['hsk6', 'hsk5', 'hsk4', 'hsk3', 'hsk2', 'hsk1'],
    columns=['Count'])
hsk_count_df = hsk_count_df.sort_values(by='Count')

# Plot the results
fig1, ax1 = plt.subplots(ncols=2, nrows=2, figsize=(16, 12))
fig1.tight_layout(h_pad=7, w_pad=4)
sns.kdeplot(df_tracks_master_clean3_lyrics_genre_hsk_v2['hsk1'],
            ax=ax1[0][0],
            fill=True)
sns.kdeplot(df_tracks_master_clean3_lyrics_genre_hsk_v2['hsk2'],
            ax=ax1[0][0],
            fill=True)
sns.kdeplot(df_tracks_master_clean3_lyrics_genre_hsk_v2['hsk3'],
            ax=ax1[0][0],
            fill=True)
sns.kdeplot(df_tracks_master_clean3_lyrics_genre_hsk_v2['hsk4'],
            ax=ax1[0][0],
            fill=True)
sns.kdeplot(df_tracks_master_clean3_lyrics_genre_hsk_v2['hsk5'],
            ax=ax1[0][0],
            fill=True)
sns.kdeplot(df_tracks_master_clean3_lyrics_genre_hsk_v2['hsk6'],
            ax=ax1[0][0],
            fill=True)
ax1[0][0].legend(['hsk1', 'hsk2', 'hsk3', 'hsk4', 'hsk5', 'hsk6'])
ax1[0][0].set_title('HSK Coverage (1-6)')

sns.kdeplot(df_tracks_master_clean3_lyrics_genre_hsk_v2['hsk1'],
            ax=ax1[0][1],
            fill=True)
sns.kdeplot(df_tracks_master_clean3_lyrics_genre_hsk_v2['hsk2'],
            ax=ax1[0][1],
            fill=True)
sns.kdeplot(df_tracks_master_clean3_lyrics_genre_hsk_v2['hsk3'],
            ax=ax1[0][1],
            fill=True)
sns.kdeplot(df_tracks_master_clean3_lyrics_genre_hsk_v2['hsk4'],
            ax=ax1[0][1],
            fill=True)
ax1[0][1].legend(['hsk1', 'hsk2', 'hsk3', 'hsk4'])
ax1[0][1].set_title('HSK Coverage (1-4))')

sns.barplot(data=hsk_count_df, x=hsk_count_df.index, y='Count', ax=ax1[1][0])
sns.barplot(data=hsk_count_df.iloc[:4],
            x=hsk_count_df.iloc[:4].index,
            y='Count',
            ax=ax1[1][1])

ax1[1][0].set_title('HSK Material Accessibility (1-6)')
ax1[1][1].set_title('HSK Material Accessibility (1-4)')
fig1.show()
```


    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_113_0.png)
    



```python
df_tracks_master_clean3_lyrics_genre_hsk_v2[[
    'hsk1', 'hsk2', 'hsk3', 'hsk4', 'hsk5', 'hsk6'
]].describe()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>hsk1</th>
      <th>hsk2</th>
      <th>hsk3</th>
      <th>hsk4</th>
      <th>hsk5</th>
      <th>hsk6</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>35921.000000</td>
      <td>35921.000000</td>
      <td>35921.000000</td>
      <td>35921.000000</td>
      <td>35921.000000</td>
      <td>35921.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.313556</td>
      <td>0.460414</td>
      <td>0.632219</td>
      <td>0.769857</td>
      <td>0.896059</td>
      <td>0.976837</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.073309</td>
      <td>0.088806</td>
      <td>0.090968</td>
      <td>0.080649</td>
      <td>0.059587</td>
      <td>0.039618</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.100000</td>
      <td>0.333333</td>
      <td>0.363636</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.266667</td>
      <td>0.405941</td>
      <td>0.577465</td>
      <td>0.724138</td>
      <td>0.867647</td>
      <td>0.970339</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>0.309278</td>
      <td>0.459184</td>
      <td>0.633333</td>
      <td>0.775000</td>
      <td>0.903614</td>
      <td>0.985507</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>0.357143</td>
      <td>0.515152</td>
      <td>0.689655</td>
      <td>0.822581</td>
      <td>0.934211</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>0.857143</td>
      <td>0.923077</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>




```python
df_tracks_master_clean3_lyrics_genre_hsk_v2.shape
```




    (35921, 62)



### 7.2. Tableau
![image.png](attachment:image.png)
![image-2.png](attachment:image-2.png)

https://prod-uk-a.online.tableau.com/#/site/spottylinguist/workbooks/76696/views

### 7.3. Word Frequency Analysis
#### With stop words included:
![image.png](attachment:image.png)

#### With stop words excluded:
![image-2.png](attachment:image-2.png)


```python
# Import Jieba to tokenize lyrics
import jieba.posseg as pseg
import jieba
```


```python
# Tokenizer using JieBa to account for Mandarin
def man_token_jie(x):
    return jieba.lcut(x)
```


```python
lyrics = df_tracks_master_clean3_lyrics_genre_hsk_v2.lyrics

# Tokenize lyrics
cvec = CountVectorizer(tokenizer=man_token_jie, max_features=10000)
X_all = cvec.fit_transform(lyrics)
```


```python
X_all_df = pd.DataFrame(X_all.toarray(), columns =cvec.get_feature_names() )
```


```python
# pd.options.display.max_columns = None
import seaborn as sns
sns.set_style("darkgrid",{"font.sans-serif":['simhei', 'Arial']})
pd.options.display.max_rows = None

sns.set(rc={'figure.figsize':(14,20)})
sns.set
# set Chinese as font for plot
plt.rcParams['font.family'] = ['Heiti TC']

plot_data = X_all_df.sum().sort_values(ascending=False)[0:40]
plot = sns.barplot(x = plot_data.index, y = plot_data);
plot.tick_params(axis='x', rotation=0)
```


    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_123_0.png)
    


#### 7.3.1. With Stop Words Removed


```python
stopwords(["zh"])
```




    {'、',
     '。',
     '〈',
     '〉',
     '《',
     '》',
     '一',
     '一个',
     '一些',
     '一何',
     '一切',
     '一则',
     '一方面',
     '一旦',
     '一来',
     '一样',
     '一种',
     '一般',
     '一转眼',
     '七',
     '万一',
     '三',
     '上',
     '上下',
     '下',
     '不',
     '不仅',
     '不但',
     '不光',
     '不单',
     '不只',
     '不外乎',
     '不如',
     '不妨',
     '不尽',
     '不尽然',
     '不得',
     '不怕',
     '不惟',
     '不成',
     '不拘',
     '不料',
     '不是',
     '不比',
     '不然',
     '不特',
     '不独',
     '不管',
     '不至于',
     '不若',
     '不论',
     '不过',
     '不问',
     '与',
     '与其',
     '与其说',
     '与否',
     '与此同时',
     '且',
     '且不说',
     '且说',
     '两者',
     '个',
     '个别',
     '中',
     '临',
     '为',
     '为了',
     '为什么',
     '为何',
     '为止',
     '为此',
     '为着',
     '乃',
     '乃至',
     '乃至于',
     '么',
     '之',
     '之一',
     '之所以',
     '之类',
     '乌乎',
     '乎',
     '乘',
     '九',
     '也',
     '也好',
     '也罢',
     '了',
     '二',
     '二来',
     '于',
     '于是',
     '于是乎',
     '云云',
     '云尔',
     '五',
     '些',
     '亦',
     '人',
     '人们',
     '人家',
     '什',
     '什么',
     '什么样',
     '今',
     '介于',
     '仍',
     '仍旧',
     '从',
     '从此',
     '从而',
     '他',
     '他人',
     '他们',
     '他们们',
     '以',
     '以上',
     '以为',
     '以便',
     '以免',
     '以及',
     '以故',
     '以期',
     '以来',
     '以至',
     '以至于',
     '以致',
     '们',
     '任',
     '任何',
     '任凭',
     '会',
     '似的',
     '但',
     '但凡',
     '但是',
     '何',
     '何以',
     '何况',
     '何处',
     '何时',
     '余外',
     '作为',
     '你',
     '你们',
     '使',
     '使得',
     '例如',
     '依',
     '依据',
     '依照',
     '便于',
     '俺',
     '俺们',
     '倘',
     '倘使',
     '倘或',
     '倘然',
     '倘若',
     '借',
     '借傥然',
     '假使',
     '假如',
     '假若',
     '做',
     '像',
     '儿',
     '先不先',
     '光',
     '光是',
     '全体',
     '全部',
     '八',
     '六',
     '兮',
     '共',
     '关于',
     '关于具体地说',
     '其',
     '其一',
     '其中',
     '其二',
     '其他',
     '其余',
     '其它',
     '其次',
     '具体地说',
     '具体说来',
     '兼之',
     '内',
     '再',
     '再其次',
     '再则',
     '再有',
     '再者',
     '再者说',
     '再说',
     '冒',
     '冲',
     '况且',
     '几',
     '几时',
     '凡',
     '凡是',
     '凭',
     '凭借',
     '出于',
     '出来',
     '分',
     '分别',
     '则',
     '则甚',
     '别',
     '别人',
     '别处',
     '别是',
     '别的',
     '别管',
     '别说',
     '到',
     '前后',
     '前此',
     '前者',
     '加之',
     '加以',
     '区',
     '即',
     '即令',
     '即使',
     '即便',
     '即如',
     '即或',
     '即若',
     '却',
     '去',
     '又',
     '又及',
     '及',
     '及其',
     '及至',
     '反之',
     '反而',
     '反过来',
     '反过来说',
     '受到',
     '另',
     '另一方面',
     '另外',
     '另悉',
     '只',
     '只当',
     '只怕',
     '只是',
     '只有',
     '只消',
     '只要',
     '只限',
     '叫',
     '叮咚',
     '可',
     '可以',
     '可是',
     '可见',
     '各',
     '各个',
     '各位',
     '各种',
     '各自',
     '同',
     '同时',
     '后',
     '后者',
     '向',
     '向使',
     '向着',
     '吓',
     '吗',
     '否则',
     '吧',
     '吧哒',
     '含',
     '吱',
     '呀',
     '呃',
     '呕',
     '呗',
     '呜',
     '呜呼',
     '呢',
     '呵',
     '呵呵',
     '呸',
     '呼哧',
     '咋',
     '和',
     '咚',
     '咦',
     '咧',
     '咱',
     '咱们',
     '咳',
     '哇',
     '哈',
     '哈哈',
     '哉',
     '哎',
     '哎呀',
     '哎哟',
     '哗',
     '哟',
     '哦',
     '哩',
     '哪',
     '哪个',
     '哪些',
     '哪儿',
     '哪天',
     '哪年',
     '哪怕',
     '哪样',
     '哪边',
     '哪里',
     '哼',
     '哼唷',
     '唉',
     '唯有',
     '啊',
     '啐',
     '啥',
     '啦',
     '啪达',
     '啷当',
     '喂',
     '喏',
     '喔唷',
     '喽',
     '嗡',
     '嗡嗡',
     '嗬',
     '嗯',
     '嗳',
     '嘎',
     '嘎登',
     '嘘',
     '嘛',
     '嘻',
     '嘿',
     '嘿嘿',
     '四',
     '因',
     '因为',
     '因了',
     '因此',
     '因着',
     '因而',
     '固然',
     '在',
     '在下',
     '在于',
     '地',
     '基于',
     '处在',
     '多',
     '多么',
     '多少',
     '大',
     '大家',
     '她',
     '她们',
     '好',
     '如',
     '如上',
     '如上所述',
     '如下',
     '如何',
     '如其',
     '如同',
     '如是',
     '如果',
     '如此',
     '如若',
     '始而',
     '孰料',
     '孰知',
     '宁',
     '宁可',
     '宁愿',
     '宁肯',
     '它',
     '它们',
     '对',
     '对于',
     '对待',
     '对方',
     '对比',
     '将',
     '小',
     '尔',
     '尔后',
     '尔尔',
     '尚且',
     '就',
     '就是',
     '就是了',
     '就是说',
     '就算',
     '就要',
     '尽',
     '尽管',
     '尽管如此',
     '岂但',
     '己',
     '已',
     '已矣',
     '巴',
     '巴巴',
     '年',
     '并',
     '并且',
     '庶乎',
     '庶几',
     '开外',
     '开始',
     '归',
     '归齐',
     '当',
     '当地',
     '当然',
     '当着',
     '彼',
     '彼时',
     '彼此',
     '往',
     '待',
     '很',
     '得',
     '得了',
     '怎',
     '怎么',
     '怎么办',
     '怎么样',
     '怎奈',
     '怎样',
     '总之',
     '总的来看',
     '总的来说',
     '总的说来',
     '总而言之',
     '恰恰相反',
     '您',
     '惟其',
     '慢说',
     '我',
     '我们',
     '或',
     '或则',
     '或是',
     '或曰',
     '或者',
     '截至',
     '所',
     '所以',
     '所在',
     '所幸',
     '所有',
     '才',
     '才能',
     '打',
     '打从',
     '把',
     '抑或',
     '拿',
     '按',
     '按照',
     '换句话说',
     '换言之',
     '据',
     '据此',
     '接着',
     '故',
     '故此',
     '故而',
     '旁人',
     '无',
     '无宁',
     '无论',
     '既',
     '既往',
     '既是',
     '既然',
     '日',
     '时',
     '时候',
     '是',
     '是以',
     '是的',
     '更',
     '曾',
     '替',
     '替代',
     '最',
     '月',
     '有',
     '有些',
     '有关',
     '有及',
     '有时',
     '有的',
     '望',
     '朝',
     '朝着',
     '本',
     '本人',
     '本地',
     '本着',
     '本身',
     '来',
     '来着',
     '来自',
     '来说',
     '极了',
     '果然',
     '果真',
     '某',
     '某个',
     '某些',
     '某某',
     '根据',
     '欤',
     '正值',
     '正如',
     '正巧',
     '正是',
     '此',
     '此地',
     '此处',
     '此外',
     '此时',
     '此次',
     '此间',
     '毋宁',
     '每',
     '每当',
     '比',
     '比及',
     '比如',
     '比方',
     '没奈何',
     '沿',
     '沿着',
     '漫说',
     '点',
     '焉',
     '然则',
     '然后',
     '然而',
     '照',
     '照着',
     '犹且',
     '犹自',
     '甚且',
     '甚么',
     '甚或',
     '甚而',
     '甚至',
     '甚至于',
     '用',
     '用来',
     '由',
     '由于',
     '由是',
     '由此',
     '由此可见',
     '的',
     '的确',
     '的话',
     '直到',
     '相对而言',
     '省得',
     '看',
     '眨眼',
     '着',
     '着呢',
     '矣',
     '矣乎',
     '矣哉',
     '离',
     '秒',
     '称',
     '竟而',
     '第',
     '等',
     '等到',
     '等等',
     '简言之',
     '管',
     '类如',
     '紧接着',
     '纵',
     '纵令',
     '纵使',
     '纵然',
     '经',
     '经过',
     '结果',
     '给',
     '继之',
     '继后',
     '继而',
     '综上所述',
     '罢了',
     '者',
     '而',
     '而且',
     '而况',
     '而后',
     '而外',
     '而已',
     '而是',
     '而言',
     '能',
     '能否',
     '腾',
     '自',
     '自个儿',
     '自从',
     '自各儿',
     '自后',
     '自家',
     '自己',
     '自打',
     '自身',
     '至',
     '至于',
     '至今',
     '至若',
     '致',
     '般的',
     '若',
     '若夫',
     '若是',
     '若果',
     '若非',
     '莫不然',
     '莫如',
     '莫若',
     '虽',
     '虽则',
     '虽然',
     '虽说',
     '被',
     '要',
     '要不',
     '要不是',
     '要不然',
     '要么',
     '要是',
     '譬喻',
     '譬如',
     '让',
     '许多',
     '论',
     '设使',
     '设或',
     '设若',
     '诚如',
     '诚然',
     '该',
     '说',
     '说来',
     '请',
     '诸',
     '诸位',
     '诸如',
     '谁',
     '谁人',
     '谁料',
     '谁知',
     '贼死',
     '赖以',
     '赶',
     '起',
     '起见',
     '趁',
     '趁着',
     '越是',
     '距',
     '跟',
     '较',
     '较之',
     '边',
     '过',
     '还',
     '还是',
     '还有',
     '还要',
     '这',
     '这一来',
     '这个',
     '这么',
     '这么些',
     '这么样',
     '这么点儿',
     '这些',
     '这会儿',
     '这儿',
     '这就是说',
     '这时',
     '这样',
     '这次',
     '这般',
     '这边',
     '这里',
     '进而',
     '连',
     '连同',
     '逐步',
     '通过',
     '遵循',
     '遵照',
     '那',
     '那个',
     '那么',
     '那么些',
     '那么样',
     '那些',
     '那会儿',
     '那儿',
     '那时',
     '那样',
     '那般',
     '那边',
     '那里',
     '都',
     '鄙人',
     '鉴于',
     '针对',
     '阿',
     '除',
     '除了',
     '除外',
     '除开',
     '除此之外',
     '除非',
     '随',
     '随后',
     '随时',
     '随着',
     '难道说',
     '零',
     '非',
     '非但',
     '非徒',
     '非特',
     '非独',
     '靠',
     '顺',
     '顺着',
     '首先',
     '︿',
     '！',
     '＃',
     '＄',
     '％',
     '＆',
     '（',
     '）',
     '＊',
     '＋',
     '，',
     '０',
     '１',
     '２',
     '３',
     '４',
     '５',
     '６',
     '７',
     '８',
     '９',
     '：',
     '；',
     '＜',
     '＞',
     '？',
     '＠',
     '［',
     '］',
     '｛',
     '｜',
     '｝',
     '～',
     '￥'}




```python
cvec = CountVectorizer(tokenizer=man_token_jie,
                       max_features=10000,
                       stop_words=stopwords(["zh"]))

X_all = cvec.fit_transform(lyrics)
X_all_df = pd.DataFrame(X_all.toarray(), columns =cvec.get_feature_names() )
sns.set_style("darkgrid",{"font.sans-serif":['simhei', 'Arial']})
pd.options.display.max_rows = None

sns.set(rc={'figure.figsize':(14,20)})
sns.set
# set Chinese as font for plot
plt.rcParams['font.family'] = ['Heiti TC']

plot_data = X_all_df.sum().sort_values(ascending=False)[0:40]
plot = sns.barplot(x = plot_data.index, y = plot_data);
plot.tick_params(axis='x', rotation=0)
```


    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_126_0.png)
    


Inspect top 75 words. Not surprisingly love is the number one? I wonder if this is the same for english songs?

Some of these should be exluded: 
- 词曲 (word): included in lyrics in error during scraping.
- 编曲 (compose: as above
- 制作 (manufacture): as above (I think in this context it is referring to the record label
- 想: many meanings therefore will exclude
- 里: as above
- 心: heart and mind, emotive, keep

The rest look ok.

Some of the top words (roughly translated):
爱  love
没有 not having
不要 must not
爱情 romance
永远 foreve, eternal
一起 together, the same place
快乐 happiness
梦 dreams
不能 not able
时间 time


```python
X_all_df.sum().sort_values(ascending=False)[:75]
```




    爱     49268
    词曲    31187
    想     24105
    没有    21429
    世界    12576
    里     12058
    心     11965
    走     11922
    不要    11745
    知道    10710
    没     10253
    爱情    10078
    编曲     9650
    不会     9386
    太      9132
    永远     8672
    一起     8443
    快乐     8145
    梦      7928
    不能     7242
    时间     6918
    寂寞     6856
    一天     6641
    听      6586
    笑      6294
    感觉     6229
    不再     6180
    幸福     5991
    已经     5990
    真的     5917
    不想     5866
    制作     5805
    现在     5764
    离开     5643
    心里     5565
    也许     5539
    美丽     5472
    需要     5376
    最后     5264
    心中     5166
    回忆     5099
    相信     5099
    今天     5087
    想要     5065
    喜欢     5046
    一次     5022
    陪      4965
    一生     4944
    懂      4824
    过去     4820
    曾经     4784
    是否     4725
    未来     4671
    温柔     4619
    眼泪     4555
    我会     4530
    问      4471
    混音     4459
    身边     4396
    唱      4394
    带      4376
    其实     4301
    找      4296
    生活     4287
    生命     4229
    忘记     4220
    一点     4176
    拥抱     4157
    总是     4129
    一直     4127
    怕      4060
    失去     4014
    明白     4003
    希望     3975
    等待     3952
    dtype: int64




```python
# these look ok too
X_all_df.sum().sort_values(ascending=False)[75:150]
```




    忘     3934
    记得    3918
    天空    3878
    朋友    3863
    哭     3841
    我要    3799
    原来    3779
    无法    3684
    人生    3663
    讲     3610
    明天    3607
    手     3542
    不到    3538
    告诉    3480
    孤单    3464
    变成    3438
    慢慢    3377
    自由    3342
    能够    3311
    以后    3307
    每个    3279
    路     3254
    拥有    3242
    故事    3227
    未     3226
    女     3181
    眼睛    3171
    想起    3167
    男     3142
    继续    3137
    不必    3124
    应该    3121
    错     3121
    愿     3114
    看见    3110
    愿意    3105
    看着    3058
    声     2988
    思念    2968
    喔     2967
    改变    2962
    变     2958
    快     2935
    音乐    2933
    发现    2933
    再见    2914
    风     2910
    有人    2907
    青春    2905
    留下    2890
    伤心    2869
    吉他    2833
    放弃    2811
    微笑    2805
    地方    2789
    终于    2783
    温暖    2764
    真     2754
    觉得    2752
    感情    2751
    回头    2746
    期待    2726
    话     2711
    害怕    2677
    总     2667
    越     2642
    吻     2640
    飞     2638
    从前    2613
    轻轻    2612
    面对    2610
    孤独    2609
    难     2579
    习惯    2570
    似     2568
    dtype: int64




```python
updated_stopwords = list(stopwords(["zh"]))
updated_stopwords.extend(['词曲','编曲','制作','想','里'])
updated_stopwords = set(updated_stopwords)
```




    {'、',
     '。',
     '〈',
     '〉',
     '《',
     '》',
     '一',
     '一个',
     '一些',
     '一何',
     '一切',
     '一则',
     '一方面',
     '一旦',
     '一来',
     '一样',
     '一种',
     '一般',
     '一转眼',
     '七',
     '万一',
     '三',
     '上',
     '上下',
     '下',
     '不',
     '不仅',
     '不但',
     '不光',
     '不单',
     '不只',
     '不外乎',
     '不如',
     '不妨',
     '不尽',
     '不尽然',
     '不得',
     '不怕',
     '不惟',
     '不成',
     '不拘',
     '不料',
     '不是',
     '不比',
     '不然',
     '不特',
     '不独',
     '不管',
     '不至于',
     '不若',
     '不论',
     '不过',
     '不问',
     '与',
     '与其',
     '与其说',
     '与否',
     '与此同时',
     '且',
     '且不说',
     '且说',
     '两者',
     '个',
     '个别',
     '中',
     '临',
     '为',
     '为了',
     '为什么',
     '为何',
     '为止',
     '为此',
     '为着',
     '乃',
     '乃至',
     '乃至于',
     '么',
     '之',
     '之一',
     '之所以',
     '之类',
     '乌乎',
     '乎',
     '乘',
     '九',
     '也',
     '也好',
     '也罢',
     '了',
     '二',
     '二来',
     '于',
     '于是',
     '于是乎',
     '云云',
     '云尔',
     '五',
     '些',
     '亦',
     '人',
     '人们',
     '人家',
     '什',
     '什么',
     '什么样',
     '今',
     '介于',
     '仍',
     '仍旧',
     '从',
     '从此',
     '从而',
     '他',
     '他人',
     '他们',
     '他们们',
     '以',
     '以上',
     '以为',
     '以便',
     '以免',
     '以及',
     '以故',
     '以期',
     '以来',
     '以至',
     '以至于',
     '以致',
     '们',
     '任',
     '任何',
     '任凭',
     '会',
     '似的',
     '但',
     '但凡',
     '但是',
     '何',
     '何以',
     '何况',
     '何处',
     '何时',
     '余外',
     '作为',
     '你',
     '你们',
     '使',
     '使得',
     '例如',
     '依',
     '依据',
     '依照',
     '便于',
     '俺',
     '俺们',
     '倘',
     '倘使',
     '倘或',
     '倘然',
     '倘若',
     '借',
     '借傥然',
     '假使',
     '假如',
     '假若',
     '做',
     '像',
     '儿',
     '先不先',
     '光',
     '光是',
     '全体',
     '全部',
     '八',
     '六',
     '兮',
     '共',
     '关于',
     '关于具体地说',
     '其',
     '其一',
     '其中',
     '其二',
     '其他',
     '其余',
     '其它',
     '其次',
     '具体地说',
     '具体说来',
     '兼之',
     '内',
     '再',
     '再其次',
     '再则',
     '再有',
     '再者',
     '再者说',
     '再说',
     '冒',
     '冲',
     '况且',
     '几',
     '几时',
     '凡',
     '凡是',
     '凭',
     '凭借',
     '出于',
     '出来',
     '分',
     '分别',
     '则',
     '则甚',
     '别',
     '别人',
     '别处',
     '别是',
     '别的',
     '别管',
     '别说',
     '到',
     '制作',
     '前后',
     '前此',
     '前者',
     '加之',
     '加以',
     '区',
     '即',
     '即令',
     '即使',
     '即便',
     '即如',
     '即或',
     '即若',
     '却',
     '去',
     '又',
     '又及',
     '及',
     '及其',
     '及至',
     '反之',
     '反而',
     '反过来',
     '反过来说',
     '受到',
     '另',
     '另一方面',
     '另外',
     '另悉',
     '只',
     '只当',
     '只怕',
     '只是',
     '只有',
     '只消',
     '只要',
     '只限',
     '叫',
     '叮咚',
     '可',
     '可以',
     '可是',
     '可见',
     '各',
     '各个',
     '各位',
     '各种',
     '各自',
     '同',
     '同时',
     '后',
     '后者',
     '向',
     '向使',
     '向着',
     '吓',
     '吗',
     '否则',
     '吧',
     '吧哒',
     '含',
     '吱',
     '呀',
     '呃',
     '呕',
     '呗',
     '呜',
     '呜呼',
     '呢',
     '呵',
     '呵呵',
     '呸',
     '呼哧',
     '咋',
     '和',
     '咚',
     '咦',
     '咧',
     '咱',
     '咱们',
     '咳',
     '哇',
     '哈',
     '哈哈',
     '哉',
     '哎',
     '哎呀',
     '哎哟',
     '哗',
     '哟',
     '哦',
     '哩',
     '哪',
     '哪个',
     '哪些',
     '哪儿',
     '哪天',
     '哪年',
     '哪怕',
     '哪样',
     '哪边',
     '哪里',
     '哼',
     '哼唷',
     '唉',
     '唯有',
     '啊',
     '啐',
     '啥',
     '啦',
     '啪达',
     '啷当',
     '喂',
     '喏',
     '喔唷',
     '喽',
     '嗡',
     '嗡嗡',
     '嗬',
     '嗯',
     '嗳',
     '嘎',
     '嘎登',
     '嘘',
     '嘛',
     '嘻',
     '嘿',
     '嘿嘿',
     '四',
     '因',
     '因为',
     '因了',
     '因此',
     '因着',
     '因而',
     '固然',
     '在',
     '在下',
     '在于',
     '地',
     '基于',
     '处在',
     '多',
     '多么',
     '多少',
     '大',
     '大家',
     '她',
     '她们',
     '好',
     '如',
     '如上',
     '如上所述',
     '如下',
     '如何',
     '如其',
     '如同',
     '如是',
     '如果',
     '如此',
     '如若',
     '始而',
     '孰料',
     '孰知',
     '宁',
     '宁可',
     '宁愿',
     '宁肯',
     '它',
     '它们',
     '对',
     '对于',
     '对待',
     '对方',
     '对比',
     '将',
     '小',
     '尔',
     '尔后',
     '尔尔',
     '尚且',
     '就',
     '就是',
     '就是了',
     '就是说',
     '就算',
     '就要',
     '尽',
     '尽管',
     '尽管如此',
     '岂但',
     '己',
     '已',
     '已矣',
     '巴',
     '巴巴',
     '年',
     '并',
     '并且',
     '庶乎',
     '庶几',
     '开外',
     '开始',
     '归',
     '归齐',
     '当',
     '当地',
     '当然',
     '当着',
     '彼',
     '彼时',
     '彼此',
     '往',
     '待',
     '很',
     '得',
     '得了',
     '怎',
     '怎么',
     '怎么办',
     '怎么样',
     '怎奈',
     '怎样',
     '总之',
     '总的来看',
     '总的来说',
     '总的说来',
     '总而言之',
     '恰恰相反',
     '您',
     '惟其',
     '想',
     '慢说',
     '我',
     '我们',
     '或',
     '或则',
     '或是',
     '或曰',
     '或者',
     '截至',
     '所',
     '所以',
     '所在',
     '所幸',
     '所有',
     '才',
     '才能',
     '打',
     '打从',
     '把',
     '抑或',
     '拿',
     '按',
     '按照',
     '换句话说',
     '换言之',
     '据',
     '据此',
     '接着',
     '故',
     '故此',
     '故而',
     '旁人',
     '无',
     '无宁',
     '无论',
     '既',
     '既往',
     '既是',
     '既然',
     '日',
     '时',
     '时候',
     '是',
     '是以',
     '是的',
     '更',
     '曾',
     '替',
     '替代',
     '最',
     '月',
     '有',
     '有些',
     '有关',
     '有及',
     '有时',
     '有的',
     '望',
     '朝',
     '朝着',
     '本',
     '本人',
     '本地',
     '本着',
     '本身',
     '来',
     '来着',
     '来自',
     '来说',
     '极了',
     '果然',
     '果真',
     '某',
     '某个',
     '某些',
     '某某',
     '根据',
     '欤',
     '正值',
     '正如',
     '正巧',
     '正是',
     '此',
     '此地',
     '此处',
     '此外',
     '此时',
     '此次',
     '此间',
     '毋宁',
     '每',
     '每当',
     '比',
     '比及',
     '比如',
     '比方',
     '没奈何',
     '沿',
     '沿着',
     '漫说',
     '点',
     '焉',
     '然则',
     '然后',
     '然而',
     '照',
     '照着',
     '犹且',
     '犹自',
     '甚且',
     '甚么',
     '甚或',
     '甚而',
     '甚至',
     '甚至于',
     '用',
     '用来',
     '由',
     '由于',
     '由是',
     '由此',
     '由此可见',
     '的',
     '的确',
     '的话',
     '直到',
     '相对而言',
     '省得',
     '看',
     '眨眼',
     '着',
     '着呢',
     '矣',
     '矣乎',
     '矣哉',
     '离',
     '秒',
     '称',
     '竟而',
     '第',
     '等',
     '等到',
     '等等',
     '简言之',
     '管',
     '类如',
     '紧接着',
     '纵',
     '纵令',
     '纵使',
     '纵然',
     '经',
     '经过',
     '结果',
     '给',
     '继之',
     '继后',
     '继而',
     '综上所述',
     '编曲',
     '罢了',
     '者',
     '而',
     '而且',
     '而况',
     '而后',
     '而外',
     '而已',
     '而是',
     '而言',
     '能',
     '能否',
     '腾',
     '自',
     '自个儿',
     '自从',
     '自各儿',
     '自后',
     '自家',
     '自己',
     '自打',
     '自身',
     '至',
     '至于',
     '至今',
     '至若',
     '致',
     '般的',
     '若',
     '若夫',
     '若是',
     '若果',
     '若非',
     '莫不然',
     '莫如',
     '莫若',
     '虽',
     '虽则',
     '虽然',
     '虽说',
     '被',
     '要',
     '要不',
     '要不是',
     '要不然',
     '要么',
     '要是',
     '譬喻',
     '譬如',
     '让',
     '许多',
     '论',
     '设使',
     '设或',
     '设若',
     '词曲',
     '诚如',
     '诚然',
     '该',
     '说',
     '说来',
     '请',
     '诸',
     '诸位',
     '诸如',
     '谁',
     '谁人',
     '谁料',
     '谁知',
     '贼死',
     '赖以',
     '赶',
     '起',
     '起见',
     '趁',
     '趁着',
     '越是',
     '距',
     '跟',
     '较',
     '较之',
     '边',
     '过',
     '还',
     '还是',
     '还有',
     '还要',
     '这',
     '这一来',
     '这个',
     '这么',
     '这么些',
     '这么样',
     '这么点儿',
     '这些',
     '这会儿',
     '这儿',
     '这就是说',
     '这时',
     '这样',
     '这次',
     '这般',
     '这边',
     '这里',
     '进而',
     '连',
     '连同',
     '逐步',
     '通过',
     '遵循',
     '遵照',
     '那',
     '那个',
     '那么',
     '那么些',
     '那么样',
     '那些',
     '那会儿',
     '那儿',
     '那时',
     '那样',
     '那般',
     '那边',
     '那里',
     '都',
     '鄙人',
     '里',
     '鉴于',
     '针对',
     '阿',
     '除',
     '除了',
     '除外',
     '除开',
     '除此之外',
     '除非',
     '随',
     '随后',
     '随时',
     '随着',
     '难道说',
     '零',
     '非',
     '非但',
     '非徒',
     '非特',
     '非独',
     '靠',
     '顺',
     '顺着',
     '首先',
     '︿',
     '！',
     '＃',
     '＄',
     '％',
     '＆',
     '（',
     '）',
     '＊',
     '＋',
     '，',
     '０',
     '１',
     '２',
     '３',
     '４',
     '５',
     '６',
     '７',
     '８',
     '９',
     '：',
     '；',
     '＜',
     '＞',
     '？',
     '＠',
     '［',
     '］',
     '｛',
     '｜',
     '｝',
     '～',
     '￥'}




```python
updated_stopwords
```




    {'、',
     '。',
     '〈',
     '〉',
     '《',
     '》',
     '一',
     '一个',
     '一些',
     '一何',
     '一切',
     '一则',
     '一方面',
     '一旦',
     '一来',
     '一样',
     '一种',
     '一般',
     '一转眼',
     '七',
     '万一',
     '三',
     '上',
     '上下',
     '下',
     '不',
     '不仅',
     '不但',
     '不光',
     '不单',
     '不只',
     '不外乎',
     '不如',
     '不妨',
     '不尽',
     '不尽然',
     '不得',
     '不怕',
     '不惟',
     '不成',
     '不拘',
     '不料',
     '不是',
     '不比',
     '不然',
     '不特',
     '不独',
     '不管',
     '不至于',
     '不若',
     '不论',
     '不过',
     '不问',
     '与',
     '与其',
     '与其说',
     '与否',
     '与此同时',
     '且',
     '且不说',
     '且说',
     '两者',
     '个',
     '个别',
     '中',
     '临',
     '为',
     '为了',
     '为什么',
     '为何',
     '为止',
     '为此',
     '为着',
     '乃',
     '乃至',
     '乃至于',
     '么',
     '之',
     '之一',
     '之所以',
     '之类',
     '乌乎',
     '乎',
     '乘',
     '九',
     '也',
     '也好',
     '也罢',
     '了',
     '二',
     '二来',
     '于',
     '于是',
     '于是乎',
     '云云',
     '云尔',
     '五',
     '些',
     '亦',
     '人',
     '人们',
     '人家',
     '什',
     '什么',
     '什么样',
     '今',
     '介于',
     '仍',
     '仍旧',
     '从',
     '从此',
     '从而',
     '他',
     '他人',
     '他们',
     '他们们',
     '以',
     '以上',
     '以为',
     '以便',
     '以免',
     '以及',
     '以故',
     '以期',
     '以来',
     '以至',
     '以至于',
     '以致',
     '们',
     '任',
     '任何',
     '任凭',
     '会',
     '似的',
     '但',
     '但凡',
     '但是',
     '何',
     '何以',
     '何况',
     '何处',
     '何时',
     '余外',
     '作为',
     '你',
     '你们',
     '使',
     '使得',
     '例如',
     '依',
     '依据',
     '依照',
     '便于',
     '俺',
     '俺们',
     '倘',
     '倘使',
     '倘或',
     '倘然',
     '倘若',
     '借',
     '借傥然',
     '假使',
     '假如',
     '假若',
     '做',
     '像',
     '儿',
     '先不先',
     '光',
     '光是',
     '全体',
     '全部',
     '八',
     '六',
     '兮',
     '共',
     '关于',
     '关于具体地说',
     '其',
     '其一',
     '其中',
     '其二',
     '其他',
     '其余',
     '其它',
     '其次',
     '具体地说',
     '具体说来',
     '兼之',
     '内',
     '再',
     '再其次',
     '再则',
     '再有',
     '再者',
     '再者说',
     '再说',
     '冒',
     '冲',
     '况且',
     '几',
     '几时',
     '凡',
     '凡是',
     '凭',
     '凭借',
     '出于',
     '出来',
     '分',
     '分别',
     '则',
     '则甚',
     '别',
     '别人',
     '别处',
     '别是',
     '别的',
     '别管',
     '别说',
     '到',
     '制作',
     '前后',
     '前此',
     '前者',
     '加之',
     '加以',
     '区',
     '即',
     '即令',
     '即使',
     '即便',
     '即如',
     '即或',
     '即若',
     '却',
     '去',
     '又',
     '又及',
     '及',
     '及其',
     '及至',
     '反之',
     '反而',
     '反过来',
     '反过来说',
     '受到',
     '另',
     '另一方面',
     '另外',
     '另悉',
     '只',
     '只当',
     '只怕',
     '只是',
     '只有',
     '只消',
     '只要',
     '只限',
     '叫',
     '叮咚',
     '可',
     '可以',
     '可是',
     '可见',
     '各',
     '各个',
     '各位',
     '各种',
     '各自',
     '同',
     '同时',
     '后',
     '后者',
     '向',
     '向使',
     '向着',
     '吓',
     '吗',
     '否则',
     '吧',
     '吧哒',
     '含',
     '吱',
     '呀',
     '呃',
     '呕',
     '呗',
     '呜',
     '呜呼',
     '呢',
     '呵',
     '呵呵',
     '呸',
     '呼哧',
     '咋',
     '和',
     '咚',
     '咦',
     '咧',
     '咱',
     '咱们',
     '咳',
     '哇',
     '哈',
     '哈哈',
     '哉',
     '哎',
     '哎呀',
     '哎哟',
     '哗',
     '哟',
     '哦',
     '哩',
     '哪',
     '哪个',
     '哪些',
     '哪儿',
     '哪天',
     '哪年',
     '哪怕',
     '哪样',
     '哪边',
     '哪里',
     '哼',
     '哼唷',
     '唉',
     '唯有',
     '啊',
     '啐',
     '啥',
     '啦',
     '啪达',
     '啷当',
     '喂',
     '喏',
     '喔唷',
     '喽',
     '嗡',
     '嗡嗡',
     '嗬',
     '嗯',
     '嗳',
     '嘎',
     '嘎登',
     '嘘',
     '嘛',
     '嘻',
     '嘿',
     '嘿嘿',
     '四',
     '因',
     '因为',
     '因了',
     '因此',
     '因着',
     '因而',
     '固然',
     '在',
     '在下',
     '在于',
     '地',
     '基于',
     '处在',
     '多',
     '多么',
     '多少',
     '大',
     '大家',
     '她',
     '她们',
     '好',
     '如',
     '如上',
     '如上所述',
     '如下',
     '如何',
     '如其',
     '如同',
     '如是',
     '如果',
     '如此',
     '如若',
     '始而',
     '孰料',
     '孰知',
     '宁',
     '宁可',
     '宁愿',
     '宁肯',
     '它',
     '它们',
     '对',
     '对于',
     '对待',
     '对方',
     '对比',
     '将',
     '小',
     '尔',
     '尔后',
     '尔尔',
     '尚且',
     '就',
     '就是',
     '就是了',
     '就是说',
     '就算',
     '就要',
     '尽',
     '尽管',
     '尽管如此',
     '岂但',
     '己',
     '已',
     '已矣',
     '巴',
     '巴巴',
     '年',
     '并',
     '并且',
     '庶乎',
     '庶几',
     '开外',
     '开始',
     '归',
     '归齐',
     '当',
     '当地',
     '当然',
     '当着',
     '彼',
     '彼时',
     '彼此',
     '往',
     '待',
     '很',
     '得',
     '得了',
     '怎',
     '怎么',
     '怎么办',
     '怎么样',
     '怎奈',
     '怎样',
     '总之',
     '总的来看',
     '总的来说',
     '总的说来',
     '总而言之',
     '恰恰相反',
     '您',
     '惟其',
     '想',
     '慢说',
     '我',
     '我们',
     '或',
     '或则',
     '或是',
     '或曰',
     '或者',
     '截至',
     '所',
     '所以',
     '所在',
     '所幸',
     '所有',
     '才',
     '才能',
     '打',
     '打从',
     '把',
     '抑或',
     '拿',
     '按',
     '按照',
     '换句话说',
     '换言之',
     '据',
     '据此',
     '接着',
     '故',
     '故此',
     '故而',
     '旁人',
     '无',
     '无宁',
     '无论',
     '既',
     '既往',
     '既是',
     '既然',
     '日',
     '时',
     '时候',
     '是',
     '是以',
     '是的',
     '更',
     '曾',
     '替',
     '替代',
     '最',
     '月',
     '有',
     '有些',
     '有关',
     '有及',
     '有时',
     '有的',
     '望',
     '朝',
     '朝着',
     '本',
     '本人',
     '本地',
     '本着',
     '本身',
     '来',
     '来着',
     '来自',
     '来说',
     '极了',
     '果然',
     '果真',
     '某',
     '某个',
     '某些',
     '某某',
     '根据',
     '欤',
     '正值',
     '正如',
     '正巧',
     '正是',
     '此',
     '此地',
     '此处',
     '此外',
     '此时',
     '此次',
     '此间',
     '毋宁',
     '每',
     '每当',
     '比',
     '比及',
     '比如',
     '比方',
     '没奈何',
     '沿',
     '沿着',
     '漫说',
     '点',
     '焉',
     '然则',
     '然后',
     '然而',
     '照',
     '照着',
     '犹且',
     '犹自',
     '甚且',
     '甚么',
     '甚或',
     '甚而',
     '甚至',
     '甚至于',
     '用',
     '用来',
     '由',
     '由于',
     '由是',
     '由此',
     '由此可见',
     '的',
     '的确',
     '的话',
     '直到',
     '相对而言',
     '省得',
     '看',
     '眨眼',
     '着',
     '着呢',
     '矣',
     '矣乎',
     '矣哉',
     '离',
     '秒',
     '称',
     '竟而',
     '第',
     '等',
     '等到',
     '等等',
     '简言之',
     '管',
     '类如',
     '紧接着',
     '纵',
     '纵令',
     '纵使',
     '纵然',
     '经',
     '经过',
     '结果',
     '给',
     '继之',
     '继后',
     '继而',
     '综上所述',
     '编曲',
     '罢了',
     '者',
     '而',
     '而且',
     '而况',
     '而后',
     '而外',
     '而已',
     '而是',
     '而言',
     '能',
     '能否',
     '腾',
     '自',
     '自个儿',
     '自从',
     '自各儿',
     '自后',
     '自家',
     '自己',
     '自打',
     '自身',
     '至',
     '至于',
     '至今',
     '至若',
     '致',
     '般的',
     '若',
     '若夫',
     '若是',
     '若果',
     '若非',
     '莫不然',
     '莫如',
     '莫若',
     '虽',
     '虽则',
     '虽然',
     '虽说',
     '被',
     '要',
     '要不',
     '要不是',
     '要不然',
     '要么',
     '要是',
     '譬喻',
     '譬如',
     '让',
     '许多',
     '论',
     '设使',
     '设或',
     '设若',
     '词曲',
     '诚如',
     '诚然',
     '该',
     '说',
     '说来',
     '请',
     '诸',
     '诸位',
     '诸如',
     '谁',
     '谁人',
     '谁料',
     '谁知',
     '贼死',
     '赖以',
     '赶',
     '起',
     '起见',
     '趁',
     '趁着',
     '越是',
     '距',
     '跟',
     '较',
     '较之',
     '边',
     '过',
     '还',
     '还是',
     '还有',
     '还要',
     '这',
     '这一来',
     '这个',
     '这么',
     '这么些',
     '这么样',
     '这么点儿',
     '这些',
     '这会儿',
     '这儿',
     '这就是说',
     '这时',
     '这样',
     '这次',
     '这般',
     '这边',
     '这里',
     '进而',
     '连',
     '连同',
     '逐步',
     '通过',
     '遵循',
     '遵照',
     '那',
     '那个',
     '那么',
     '那么些',
     '那么样',
     '那些',
     '那会儿',
     '那儿',
     '那时',
     '那样',
     '那般',
     '那边',
     '那里',
     '都',
     '鄙人',
     '里',
     '鉴于',
     '针对',
     '阿',
     '除',
     '除了',
     '除外',
     '除开',
     '除此之外',
     '除非',
     '随',
     '随后',
     '随时',
     '随着',
     '难道说',
     '零',
     '非',
     '非但',
     '非徒',
     '非特',
     '非独',
     '靠',
     '顺',
     '顺着',
     '首先',
     '︿',
     '！',
     '＃',
     '＄',
     '％',
     '＆',
     '（',
     '）',
     '＊',
     '＋',
     '，',
     '０',
     '１',
     '２',
     '３',
     '４',
     '５',
     '６',
     '７',
     '８',
     '９',
     '：',
     '；',
     '＜',
     '＞',
     '？',
     '＠',
     '［',
     '］',
     '｛',
     '｜',
     '｝',
     '～',
     '￥'}




```python

```


```python
# check this has worked: 
cvec = CountVectorizer(tokenizer=man_token_jie,
                       max_features=10000,
                       stop_words=updated_stopwords)

X_all = cvec.fit_transform(lyrics)
X_all_df = pd.DataFrame(X_all.toarray(), columns =cvec.get_feature_names() )
sns.set_style("darkgrid",{"font.sans-serif":['simhei', 'Arial']})
pd.options.display.max_rows = None

sns.set(rc={'figure.figsize':(14,20)})
sns.set
# set Chinese as font for plot
plt.rcParams['font.family'] = ['Heiti TC']

plot_data = X_all_df.sum().sort_values(ascending=False)[0:40]
plot = sns.barplot(x = plot_data.index, y = plot_data);
plot.tick_params(axis='x', rotation=0)
```


    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_134_0.png)
    



```python
X_all_df = pd.DataFrame(X_all.toarray(), columns =cvec.get_feature_names() )

X_all_df_filtered1 = pd.DataFrame(X_all_df.sum().sort_values(ascending=False)[:100])
additional_stops1 = X_all_df_filtered1.index

X_all_df_filtered2 = pd.DataFrame(X_all_df.sum().sort_values(ascending=False)[:300])
additional_stops2 = X_all_df_filtered2.index
```


```python
X_all_df_filtered.index
```




    Index(['爱', '没有', '世界', '心', '走', '不要', '知道', '没', '爱情', '不会', '太', '永远', '一起',
           '快乐', '梦', '不能', '时间', '寂寞', '一天', '听', '笑', '感觉', '不再', '幸福', '已经',
           '真的', '不想', '现在', '离开', '心里', '也许', '美丽', '需要', '最后', '心中', '回忆', '相信',
           '今天', '想要', '喜欢', '一次', '陪', '一生', '懂', '过去', '曾经', '是否', '未来', '温柔',
           '眼泪', '我会', '问', '混音', '身边', '唱', '带', '其实', '找', '生活', '生命', '忘记',
           '一点', '拥抱', '总是', '一直', '怕', '失去', '明白', '希望', '等待', '忘', '记得', '天空',
           '朋友', '哭', '我要', '原来', '无法', '人生', '讲', '明天', '手', '不到', '告诉', '孤单',
           '变成', '慢慢', '自由', '能够', '以后', '每个', '路', '拥有', '故事', '未', '女', '眼睛',
           '想起', '男', '继续'],
          dtype='object')




```python
# Also create stopword list that excludes top 100
updated_stopwords_2 = list(updated_stopwords)
updated_stopwords_2.extend(additional_stops1)
updated_stopwords_2 = set(updated_stopwords_2)
updated_stopwords_2

# Also create stopword list that excludes top 300
updated_stopwords_3 = list(updated_stopwords)
updated_stopwords_3.extend(additional_stops2)
updated_stopwords_3 = set(updated_stopwords_3)
updated_stopwords_3
```




    {'矣乎',
     '巴',
     '沿',
     '么',
     '呀',
     '青春',
     '改变',
     '待',
     '还要',
     '一则',
     '这么',
     '及',
     '矣哉',
     '不会',
     '从来',
     '如是',
     '孰料',
     '呵呵',
     '为了',
     '它',
     '处在',
     '甚且',
     '各个',
     '不如',
     '年',
     '是的',
     '习惯',
     '离开',
     '与其说',
     '乃至于',
     '往',
     '有时',
     '心情',
     '就',
     '为什么',
     '因而',
     '论',
     '一种',
     '不得',
     '庶乎',
     '一片',
     '远',
     '你们',
     '就是',
     '无法',
     '反而',
     '心',
     '云尔',
     '一旦',
     '设使',
     '沉默',
     '是不是',
     '起来',
     '何以',
     '留下',
     '果真',
     '真的',
     '清楚',
     '但',
     '嘘',
     '像',
     '哪儿',
     '先不先',
     '同',
     '已经',
     '陪',
     '太阳',
     '点',
     '简单',
     '眼神',
     '分手',
     '归齐',
     '或则',
     '同时',
     '被',
     '鄙人',
     '应该',
     '４',
     '其余',
     '只当',
     '吉他',
     '咋',
     '前此',
     '〉',
     '嘎',
     '不尽',
     '非但',
     '不特',
     '不至于',
     '再则',
     '只要',
     '加以',
     '毋宁',
     '首先',
     '余外',
     '６',
     '出现',
     '而',
     '看见',
     '难',
     '旁人',
     '然而',
     '其中',
     '可是',
     '％',
     '痛苦',
     '仍',
     '某些',
     '可能',
     '类如',
     '回来',
     '其他',
     '有及',
     '称',
     '曾',
     '一切',
     '越',
     '伤',
     '以上',
     '腾',
     '冲',
     '那儿',
     '似',
     '关于',
     '１',
     '果然',
     '没奈何',
     '归',
     '不若',
     '分',
     '许多',
     '＜',
     '再见',
     '唯一',
     '？',
     '不能',
     '设或',
     '人',
     '随时',
     '正值',
     '倘或',
     '不',
     '用来',
     '继之',
     '只能',
     '不拘',
     '固然',
     '那会儿',
     '那般',
     '或是',
     '开',
     '全部',
     '哦',
     '与其',
     '于',
     '尽管',
     '紧接着',
     '基于',
     '若',
     '连',
     '没有',
     '眼睛',
     '光是',
     '何处',
     '简言之',
     '或',
     '期待',
     '呸',
     '做',
     '难过',
     '既',
     '飞',
     '哉',
     '时候',
     '比方',
     '来说',
     '孤独',
     '没法',
     '从而',
     '大',
     '如何',
     '咱们',
     '将',
     '随着',
     '）',
     '不用',
     '不停',
     '见',
     '临',
     '那些',
     '上',
     '其一',
     '尔尔',
     '感动',
     '根据',
     '忘',
     '以至于',
     '叮咚',
     '哪样',
     '后悔',
     '哎',
     '替代',
     '记得',
     '真心',
     '为着',
     '继后',
     '再者',
     '消失',
     '呕',
     '而况',
     '几',
     '恨',
     '故',
     '心中',
     '忘记',
     '｝',
     '当着',
     '直到',
     '人生',
     '回忆',
     '所有',
     '令',
     '具体地说',
     '［',
     '而已',
     '就是说',
     '不光',
     '当',
     '譬喻',
     '出于',
     '懂',
     '温暖',
     '呼吸',
     '这会儿',
     '如其',
     '温柔',
     '承诺',
     '生活',
     '已矣',
     '去',
     '欤',
     '由',
     '美丽',
     '此次',
     '因了',
     '明天',
     '从前',
     '勇敢',
     '走过',
     '前后',
     '呗',
     '需要',
     '告诉',
     '情',
     '一点',
     '吧',
     '自后',
     '乌乎',
     '吗',
     '己',
     '灵魂',
     '有人',
     '即令',
     '由于',
     '以便',
     '本人',
     '什',
     '对于',
     '编曲',
     '矣',
     '二',
     '任',
     '可',
     '谁人',
     '老',
     '爱',
     '无',
     '若是',
     '难道说',
     '你',
     '这里',
     '＆',
     '请',
     '音乐',
     '觉得',
     '！',
     '俺们',
     '吱',
     '月',
     '抱',
     '设若',
     '才',
     '喜欢',
     '无奈',
     '后者',
     '里',
     '为何',
     '太',
     '身边',
     '今天',
     '总是',
     '愿意',
     '别',
     '分别',
     '天空',
     '以故',
     '人家',
     '故而',
     '至',
     '并且',
     '说话',
     '关于具体地说',
     '他们',
     '其它',
     '存在',
     '怀念',
     '中',
     '甚么',
     '致',
     '事',
     '含',
     '只',
     '对',
     '生命',
     '似的',
     '快',
     '呵',
     '昨天',
     '呢',
     '七',
     '看',
     '听',
     '两者',
     '诸位',
     '谁料',
     '；',
     '真',
     '以',
     '嗯',
     '而是',
     '这些',
     '所幸',
     '》',
     '向使',
     '即或',
     '彼此',
     '诸如',
     '这时',
     '另一方面',
     '不管',
     '结果',
     '最',
     '既往',
     '乘',
     '说来',
     '八',
     '等待',
     '声音',
     '过去',
     '则甚',
     '非独',
     '的话',
     '莫不然',
     '歌',
     '较之',
     '感受',
     '后',
     '般的',
     '到底',
     '或许',
     '那边',
     '他人',
     '我们',
     '变',
     '某',
     '据此',
     '那里',
     '此外',
     '靠',
     '哭',
     '阮',
     '就让',
     '遗憾',
     '使',
     '况且',
     '与',
     '如上所述',
     '纵',
     '于是',
     '想念',
     '抑或',
     '有没有',
     '什么',
     '犹自',
     '〈',
     '既是',
     '兼之',
     '完美',
     '＄',
     '假若',
     '多少',
     '还有',
     '～',
     '相爱',
     '以免',
     '诸',
     '纵使',
     '他',
     '＠',
     '未来',
     '知',
     '没',
     '尚且',
     '凭借',
     '继续',
     '各自',
     '这么样',
     '日子',
     '甜蜜',
     '使得',
     '倘使',
     '儿',
     '另',
     '喏',
     '来自',
     '孤单',
     '夜',
     '哈哈',
     '如同',
     '渴望',
     '谁知',
     '自从',
     '地',
     '换言之',
     '虽',
     '记忆',
     '或曰',
     '得了',
     '着呢',
     '顺着',
     '可以',
     '依照',
     '好好',
     '罢了',
     '难道',
     '第',
     '起见',
     '接着',
     '能够',
     '我会',
     '其次',
     '至今',
     '以致',
     '照着',
     '一次',
     '、',
     '来着',
     '尽',
     '感情',
     '看到',
     '甚至',
     '笑',
     '不要',
     '当地',
     '不怕',
     '截至',
     '双手',
     '啷当',
     '我',
     '此地',
     '知道',
     '找',
     '在于',
     '综上所述',
     '嘻',
     '打',
     '这个',
     '别处',
     '还',
     '朝着',
     '孰知',
     '倘然',
     '总',
     '时',
     '焉',
     '虽说',
     '不敢',
     '不了',
     '才能',
     '用',
     '命运',
     '比如',
     '较',
     '只有',
     '即使',
     '不单',
     '倘若',
     '原来',
     '这',
     '别是',
     '里面',
     '远方',
     '一个',
     '开始',
     '错',
     '得到',
     '独自',
     '按',
     '一般',
     '她们',
     '小',
     '诚如',
     '和',
     '除外',
     '他们们',
     '｜',
     '如',
     '等等',
     '时间',
     '感觉',
     '在',
     '浪漫',
     '除开',
     '好',
     '不但',
     '再其次',
     '词曲',
     '回头',
     '情人',
     '哗',
     '仍旧',
     '吧哒',
     '每个',
     '无论',
     '付出',
     '另外',
     '可见',
     '对待',
     '不问',
     '本着',
     '却',
     '以期',
     '何况',
     '微笑',
     '很',
     '脸',
     '黑暗',
     '个别',
     '既然',
     '梦',
     '趁着',
     '比及',
     '的',
     '吹',
     '为此',
     '以来',
     '边',
     '最后',
     '呃',
     '永远',
     '每',
     '男',
     '一',
     '不是',
     '这儿',
     '那么',
     '爱情',
     '不尽然',
     '哪里',
     '赖以',
     '为止',
     '监制',
     '纵令',
     '早已',
     '三',
     '具体说来',
     '不惟',
     '共',
     '别人',
     '＊',
     '眼泪',
     '加之',
     '并',
     '如果',
     '替',
     '此时',
     '也许',
     '即便',
     '发现',
     '希望',
     '这般',
     '自家',
     '喔',
     '极了',
     '望',
     '身旁',
     '一方面',
     '故事',
     '甚而',
     '莫若',
     '不必',
     '但凡',
     '一定',
     '之间',
     '相对而言',
     '完',
     '只是',
     '唱',
     '则',
     '再',
     '要不',
     '啐',
     '面对',
     '恰恰相反',
     '梦想',
     '庶几',
     '美好',
     '原谅',
     '之类',
     '等',
     '本身',
     '二来',
     '哩',
     '哪',
     '鉴于',
     '贼死',
     '尔',
     '而言',
     '但是',
     '正是',
     '嘎登',
     '害怕',
     '一场',
     '录音室',
     '然后',
     '顺',
     '想起',
     '者',
     '以至',
     '￥',
     '变成',
     '当初',
     '否则',
     '无宁',
     '睡',
     '除',
     '有些',
     '有的',
     '若果',
     '如今',
     '几时',
     '省得',
     '阿',
     '两个',
     '伤心',
     '泪',
     '要是',
     '这样',
     '随',
     '怎么办',
     '有',
     '进而',
     '＞',
     '不愿',
     '至若',
     '上下',
     '假如',
     '风',
     '听见',
     '自己',
     '那么些',
     '７',
     '乎',
     '区',
     '朝',
     '出',
     '由此',
     '最好',
     '只限',
     '与此同时',
     '。',
     '来',
     '譬如',
     '云云',
     '自身',
     '巴巴',
     '个',
     '２',
     '再说',
     '嘛',
     '想',
     '不知',
     '宁可',
     '万一',
     '花',
     '如若',
     '那样',
     '零',
     '有关',
     '哪天',
     '要',
     '不独',
     '借傥然',
     '路',
     '痛',
     '慢慢',
     '也好',
     '依据',
     '由是',
     '说',
     '此处',
     '自',
     '乃',
     '介于',
     '起',
     '一些',
     '城市',
     '便',
     '新',
     '总而言之',
     '下',
     '通过',
     '得',
     '随后',
     '其二',
     '或者',
     '从此',
     '也',
     '所以',
     '以为',
     '另悉',
     '（',
     '趁',
     '啥',
     '打从',
     '选择',
     '会',
     '相信',
     '其实',
     '不想',
     '六',
     '凭',
     '遵循',
     '从',
     '本',
     '女人',
     '到',
     '岁月',
     '且说',
     '就要',
     '要不然',
     '慢说',
     '而后',
     '咱',
     '大家',
     '５',
     '也罢',
     '即若',
     '某某',
     '正如',
     '然则',
     '及其',
     '非特',
     '依然',
     '仿佛',
     '乃至',
     '亲爱',
     '已',
     '在下',
     '＋',
     '拥抱',
     '五',
     '《',
     '犹且',
     '：',
     '时光',
     '能否',
     '想要',
     '好像',
     '的确',
     '虽则',
     '跳',
     '哪边',
     '别的',
     '喽',
     '因为',
     '我要',
     '笑容',
     '今',
     '别管',
     '它们',
     '据',
     '呼哧',
     '不只',
     '变得',
     '一颗',
     '唉',
     '这么些',
     '怎样',
     '故此',
     '现在',
     '手',
     '经过',
     '假使',
     '一天',
     '受到',
     '把',
     '一刻',
     '混音',
     '嗡嗡',
     '任凭',
     '多么',
     '自打',
     '总的来看',
     '宁愿',
     '对比',
     '岂但',
     '虽然',
     '３',
     '录音',
     '即',
     '怎么样',
     '以及',
     '再者说',
     '每天',
     '离',
     '如下',
     '幸福',
     '尔后',
     '哎呀',
     '继而',
     '这就是说',
     '不料',
     '全体',
     '带',
     '我心',
     '因',
     '明白',
     '哪个',
     '哇',
     '嗬',
     '何时',
     '放弃',
     '依',
     '让',
     '兮',
     '针对',
     '当然',
     '呜呼',
     '拿',
     '开心',
     '只消',
     '问',
     '能',
     '不妨',
     '啊',
     '啪达',
     '您',
     '除非',
     '写',
     '这边',
     '吓',
     '女',
     '一句',
     '别说',
     '悲伤',
     '不然',
     '为',
     '于是乎',
     '该',
     '世界',
     '尽管如此',
     '各种',
     '一直',
     '借',
     '不到',
     '何',
     '这次',
     '等到',
     '失去',
     '雨',
     '哼唷',
     '漫说',
     '竟而',
     '爱过',
     '话',
     '遗忘',
     '便于',
     '始而',
     '站',
     '咧',
     '不外乎',
     '自由',
     '弦乐',
     '且不说',
     '又及',
     '多',
     '黑夜',
     '所在',
     '是以',
     '与否',
     '惟其',
     '吻',
     '之',
     '什么样',
     '是',
     '反过来',
     '总的来说',
     '其',
     '恋爱',
     '诚然',
     '终于',
     '管',
     '人们',
     '走',
     '一生',
     '反过来说',
     '沿着',
     '而外',
     '要么',
     '由此可见',
     '还是',
     '喂',
     '合',
     '因此',
     '怎',
     '若非',
     '制作',
     '嗡',
     '不论',
     '前者',
     '了',
     '非徒',
     '声',
     '咚',
     '即如',
     '着',
     '回到',
     '唯有',
     '一来',
     '自个儿',
     '且',
     '她',
     '心跳',
     '一何',
     '︿',
     '努力',
     '追',
     '哈',
     '所',
     '轻轻',
     '如上',
     '，',
     '此间',
     '８',
     '天',
     '哎哟',
     '凡',
     '们',
     '自各儿',
     '决定',
     '经',
     '我爱你',
     '某个',
     '都',
     '突然',
     '这么点儿',
     '些',
     '跟',
     '开外',
     '看着',
     '这一来',
     '又',
     '此',
     '拥有',
     '快乐',
     '九',
     '凡是',
     '光',
     '仍然',
     '＃',
     '不再',
     '例如',
     '纵然',
     '哪怕',
     '咦',
     '吃',
     '不仅',
     '哪年',
     '曾经',
     '不比',
     '不过',
     '未',
     '越是',
     '若夫',
     '怎么',
     '本地',
     '那个',
     '再有',
     '夜里',
     ...}




```python
# check this has worked: 
cvec = CountVectorizer(tokenizer=man_token_jie,
                       max_features=10000,
                       stop_words=updated_stopwords_2)

X_all = cvec.fit_transform(lyrics)
X_all_df = pd.DataFrame(X_all.toarray(), columns =cvec.get_feature_names() )
sns.set_style("darkgrid",{"font.sans-serif":['simhei', 'Arial']})
pd.options.display.max_rows = None

sns.set(rc={'figure.figsize':(14,20)})
sns.set
# set Chinese as font for plot
plt.rcParams['font.family'] = ['Heiti TC']

plot_data = X_all_df.sum().sort_values(ascending=False)[0:40]
plot = sns.barplot(x = plot_data.index, y = plot_data);
plot.tick_params(axis='x', rotation=0)
```


    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_138_0.png)
    


## 8. Create English Song Data File


```python
english_tracks = grab_song_from_playlist(
    'spotify:playlist:5IgEf6pGG8q565BqhApd7x', 'english_tracks')
```


```python
# For each artist scrape similar artists and save the artist details in a new dataframe
df_eg_playlist_similar_artists = pd.DataFrame()
for ind, i in enumerate(english_tracks['artist_uri']):
    try:
        df_eg_playlist_similar_artists = df_eg_playlist_similar_artists.append(
            scrape_similar_artists(i))
    except:
        pass

df_similar_english_lang_artists = df_eg_playlist_similar_artists.drop_duplicates(
    subset='artist_id', keep="last")
```


```python
df_similar_english_lang_artists_expand = scrape_similar_artists_expansion(
    df_similar_english_lang_artists)
```


```python
# Loop through all of the artists scraped in 3.0 and get a list of albums
# Here we run through only the first 2 artists as an example
empty_list_albums = []

for artist_uri in tqdm(df_similar_english_lang_artists['artist_uri']):
    album_uri_temp = sp.artist_albums(artist_uri)
    for ind, i in enumerate(album_uri_temp['items']):
        empty_list_albums.append(album_uri_temp['items'][ind]['uri'])
```


```python
# loop through all the albums and get track ids for each (top 2 only here)
empty_list_tracks3 = []
for album in tqdm(empty_list_albums):
    temp = sp.album_tracks(album)['items']
    for ind, track in enumerate(temp):
        try:
            empty_list_tracks3.append(
                sp.album_tracks(album)['items'][ind]['uri'])
        except:
            pass
```


```python
# Loop through list of track IDs and get song information
tracks = []
for i in tqdm(empty_list_tracks3):
    try:
        track = getTrackFeatures(i)
        tracks.append(track)
    except:
        pass
```


```python
df_english_tracks_master = pd.read_csv(
    '/Users/stuart/Desktop/Spoty-Linguist-Project/df_english_tracks_master.csv',
    sep=',',
    index_col=0)
df_english_tracks_master.shape
```




    (8742, 29)




```python
artists = pd.DataFrame(
    df_english_tracks_master.artist.drop_duplicates().dropna())
```


```python
artists['genre'] = artists
```


```python
artists.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>artist</th>
      <th>genre</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>Tom Grennan</td>
      <td>Tom Grennan</td>
    </tr>
    <tr>
      <th>75</th>
      <td>Calvin Harris</td>
      <td>Calvin Harris</td>
    </tr>
    <tr>
      <th>88</th>
      <td>Ella Henderson</td>
      <td>Ella Henderson</td>
    </tr>
    <tr>
      <th>94</th>
      <td>The Hunna</td>
      <td>The Hunna</td>
    </tr>
    <tr>
      <th>311</th>
      <td>Inhaler</td>
      <td>Inhaler</td>
    </tr>
  </tbody>
</table>
</div>




```python
def get_artist_genre(x):
    try:
        artist_uri = sp.search(
            x)['tracks']['items'][0]['album']['artists'][0]['uri']
        artist_genre = sp.artist(artist_uri)['genres'][0]
        return artist_genre
    except:
        return np.nan
```


```python
artists['genre'] = artists.genre.apply(get_artist_genre)
```


```python
df_english_tracks_master_genre = df_english_tracks_master.merge(artists,
                                                                on='artist',
                                                                how='inner')
```


```python
df_english_tracks_master_genre.to_csv('df_english_tracks_master_genre.csv')
```

## 9. Some Modelling


```python
# import data file
df_tracks_master_clean3_lyrics_genre_hsk_v2 = pd.read_csv(
    '/Users/stuart/Desktop/Spoty-Linguist-Project/df_tracks_master_clean3_lyrics_genre_hsk_v2.csv',
    sep=',',
    index_col=0)
```

### 9.1. Logistic Regression to identify songs in Cantonese
#### Summary
Artis genre was available for ~85% of all songs. Of those with genre available roughly 23% are classified as "Cantopop". Taking random samples from this subset showed that many of the songs in this category are sung in Cantonese rather than Mandarin (although they are written with the same characters) and these may therefore 
need to be excluded. 

It would be helpful if I could identify songs that are "Cantopop" where genre is not available so these could also be excluded...

#### Question: Is it possible to train a model to identify  songs in the genre "Cantopop" using the subset of songs where genre is available and to then apply this model to the rest of the data where no genre is available?

The challenge with this approach is that "Cantopop" does not strictly mean the song is sang in Cantonese. In addition genre information is available on a artist basis rather than per song.


Model 5 proved to be the most effective model in classifying songs into the genre "Cantopop" with a final recall rate of 87%. Precision rates were disapointing (e.g. 60% for minority class in the test data) albeit not surprising. As I highlighted before there are fundamental limitations to this model given the genre "Cantopop" does not strictly mean that the song will be sung in Cantonese. I applied model 5 to the songs where no genre is available as shown below. However, I decided to not exclude Cantopop from the dataset given the unreliability of the model and that it would result in an unnecessarily large loss of in scope songs. 

#### 9.1.1. Preprocessing
Inspecting the data and creating X and Y variables. Classification will be between Cantopop (27%) and other genres. 


```python
df_tracks_master_clean3_lyrics_genre_hsk_v2.Genre2.value_counts(normalize=True)
```




    cantopop             0.274605
    classic mandopop     0.200139
    taiwan pop           0.133954
    mandopop             0.107493
    chinese indie        0.035930
                           ...   
    kayokyoku            0.000033
    anime rock           0.000033
    blues rock           0.000033
    taiwan electronic    0.000033
    xinyao               0.000033
    Name: Genre2, Length: 64, dtype: float64



Looking at the subset of songs where genre is available create a binary logistic regresssion to identify which songs are Cantonese. 


```python
# Create a new column which identifies if a song is Cantopop, other genre, or no genre available
df_c = df_tracks_master_clean3_lyrics_genre_hsk_v2.copy()
df_c['is_canto'] = np.nan
df_c['is_canto'][(df_tracks_master_clean3_lyrics_genre_hsk_v2.Genre2.notnull()) |
    (df_tracks_master_clean3_lyrics_genre_hsk_v2.Genre3.notnull())] = 0
df_c['is_canto'][(df_c.Genre2 == 'cantopop') |
                 (df_c.Genre3 == 'classic cantopop')] = 1
```


```python
display(df_c['is_canto'].value_counts(dropna=False))
display(df_c['is_canto'].value_counts(normalize=True, dropna=False))
```


    0.0    21906
    1.0     8403
    NaN     5612
    Name: is_canto, dtype: int64



    0.0    0.609838
    1.0    0.233930
    NaN    0.156232
    Name: is_canto, dtype: float64



```python
# Crate a new data
df_c_clean = df_c[df_c.is_canto.notnull()]
```


```python
display(df_c_clean['is_canto'].value_counts(dropna=False))
display(df_c_clean['is_canto'].value_counts(normalize=True, dropna=False))
```


    0.0    21906
    1.0     8403
    Name: is_canto, dtype: int64



    0.0    0.722756
    1.0    0.277244
    Name: is_canto, dtype: float64


Approximately 27% of the songs are classified as Cantopop


```python
y = df_c_clean['is_canto']
X = df_c_clean['lyrics']
```


```python
X.shape[0], y.shape[0]
```




    (30309, 30309)



#### 9.1.2. Function for plots
A function to show ROC, confusion matrix etc. for models.


```python
# Function to return some plots for Logistic Regression
def logist_plots(lr, X_train, y_train, X_test, y_test):

    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, plot_confusion_matrix
    from sklearn.metrics import plot_confusion_matrix, plot_roc_curve, plot_precision_recall_curve, roc_auc_score, average_precision_score
    from matplotlib.colors import ListedColormap
    import scikitplot as skplt

    # classification report
    predictions_train = lr.predict(X_train)
    predictions_test = lr.predict(X_test)
    print('y_train classification report:')
    print(classification_report(y_train, predictions_train))
    print('y_test classification report:')
    print(classification_report(y_test, predictions_test))

    # plot the confusion matrix
    fig, ax = plt.subplots(ncols=2, figsize=(14, 6), sharey=True)
    plot_confusion_matrix(lr,
                          X_train,
                          y_train,
                          cmap='Blues',
                          ax=ax[0],xticks_rotation='vertical' )
    plot_confusion_matrix(lr,
                          X_test,
                          y_test,
                          cmap='Blues',
                          ax=ax[1],xticks_rotation='vertical')
    
    for a in ax:
        texts = a.texts
        for text in texts:
            text.set_size(20)
    plt.show()

    # ROC  curves:
    print('ROC Curves:')
    fig_rec, ax_rec = plt.subplots(ncols=2, figsize=(12, 6), sharey=True)
    cmap = ListedColormap(sns.color_palette("husl", len(lr.classes_)))
    probabilities_train = lr.predict_proba(X_train)
    probabilities_test = lr.predict_proba(X_test)
    skplt.metrics.plot_roc(y_train,
                           probabilities_train,
                           cmap=cmap,
                           ax=ax_rec[0])
    skplt.metrics.plot_roc(y_test, probabilities_test, cmap=cmap, ax=ax_rec[1])
    ax_rec[0].set_title('Train')
    ax_rec[1].set_title('Test')
    plt.show()

    # Precision Recall:
    print('Precision Recall Curve:')
    fig_prec, ax_prec = plt.subplots(ncols=2, figsize=(12, 6), sharey=True)
    cmap = ListedColormap(sns.color_palette("husl", len(lr.classes_)))
    probabilities_train = lr.predict_proba(X_train)
    probabilities_test = lr.predict_proba(X_test)
    skplt.metrics.plot_precision_recall(y_train,
                                        probabilities_train,
                                        cmap=cmap,
                                        ax=ax_prec[0])
    skplt.metrics.plot_precision_recall(y_test,
                                        probabilities_test,
                                        cmap=cmap,
                                        ax=ax_prec[1])
    ax_prec[0].set_title('Train')
    ax_prec[1].set_title('Test')
    plt.show()

#     # Plot False Positive Rate and Accuracy Change Threshold
#     #  false positive rate as threshold changed:
#     print('Threholds (test data) impact on Accuracy / False Positive Rate:')
#     false_positives = []
#     for thresh in np.arange(1, 100) / 100.:
#         labeled_1 = np.array(
#             [1 if x >= thresh else 0 for x in lr.predict_proba(X_test)[:, 1]])
#         fp = np.mean((y_test == 0) & (labeled_1 == 1))
#         false_positives.append((thresh, fp))
#     #     print('Threshold:', thresh, 'false positives:', fp)
#     false_positives = np.array(false_positives)

#     #  accuracy change with threshold
#     accuracies = []
#     for thresh in np.arange(1, 100) / 100.:
#         labeled_1 = np.array(
#             [1 if x >= thresh else 0 for x in lr.predict_proba(X_test)[:, 1]])
#         acc = np.mean(y_test == labeled_1)
#         accuracies.append((thresh, acc))
#     #     print('Threshold:', thresh, 'Correct predictions:', acc)
#     accuracies = np.array(accuracies)

#     fig_acc, ax_acc = plt.subplots(ncols=2, figsize=(12, 6), sharey=True)
#     ax_acc[0].plot(false_positives[:, 0], false_positives[:, 1], lw=2)
#     ax_acc[0].set_xlabel('threshold')
#     ax_acc[0].set_ylabel('false positive fraction')

#     ax_acc[1].plot(*accuracies.T, lw=2)
#     ax_acc[1].set_xlabel('threshold')
#     ax_acc[1].set_ylabel('accuracy')
#     plt.show()
```

#### 9.1.3. Tokenizing Lyrics
I initially ran the logistic regression model using SnowNLP (as used in previous EDA stages) to tokenize the lyrics. However, the model performed poorly and was slow to run. I switched to an alernative tokenizer "Jieba", which increased computation time and improved results and I have used this in the models below. 


```python
# Import Jieba to tokenize lyrics
import jieba.posseg as pseg
import jieba
```


```python
# Turn off warnings
warnings.filterwarnings("ignore")
warnings.filterwarnings("ignore", category=DeprecationWarning)

# Demonstration of Jieba, note it recognises that 朋 and 友 together form the word 朋友 -> friend
sentence = '你是我的好朋友'
jieba.lcut(sentence)
```




    ['你', '是', '我', '的', '好', '朋友']




```python
# Create a customer tokenizer using JieBa to account for Mandarin
def man_token_jie(x):
    return jieba.lcut(x)
```

#### 9.1.4. Logistic Regression Model (1)
The mean cross validated score suggests that we can expect to make correct classifications in 85% of cases which compares with the base line of 72% if we were to apply the majority class. From the classification report it can be seen that the model tends to favour the majority class which resulted in a training recall score of 65% for the minority class which in turn impacted the precision scores of both classes.

Could the recall of the minority class (Cantopop) be improved by assigning weights?



```python
# Get Train test split from sample data
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    stratify=y,
                                                    test_size=0.2,
                                                    random_state=1)
```


```python
# Tokenize lyrics
cvec = CountVectorizer(tokenizer=man_token_jie, max_features=1000)
cvec.fit(X_train)
cvec.get_feature_names()[:5]
```




    ['一', '一下', '一世', '一个', '一些']




```python
# Transform training data
cvec_mat = cvec.transform(X_train)
X_train = cvec_mat
X_test = cvec.transform(X_test)
```


```python
# Import and fit a logistic regression model and test
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold

Bold = '\033[1m'
Normal = '\033[0m'

skf = StratifiedKFold(n_splits=5)
skf.get_n_splits(X_train, y_train)
lr = LogisticRegression(solver='lbfgs', multi_class='ovr')
lr.fit(X_train, y_train)
lr_scores = cross_val_score(lr, X_train, y_train, cv=skf, n_jobs=6)

print(Bold + 'Base line score:' + Normal,
      np.round(y_train.value_counts(normalize=True)[0], 3))
print(Bold + 'Training Score:' + Normal, np.round(lr.score(X_train, y_train),
                                                  3))
print(Bold + 'Test Score:' + Normal, np.round(lr.score(X_test, y_test), 3))
print(Bold + "Cross-validated training scores:" + Normal,
      np.round(lr_scores, 4))
print(Bold + "Mean cross-validated training score:" + Normal,
      np.round(lr_scores.mean(), 3), '\n')
```

    [1mBase line score:[0m 0.723
    [1mTraining Score:[0m 0.866
    [1mTest Score:[0m 0.856
    [1mCross-validated training scores:[0m [0.8524 0.8507 0.8497 0.8492 0.8391]
    [1mMean cross-validated training score:[0m 0.848 
    


Scores reflects the mean accuracy on the given test data and labels:
![image.png](attachment:image.png)

The mean cross validated score suggests that we can expect to make correct classifications in 85% of cases which compares with the base line of 72% if we were to apply the majority class. From the classification report it can be seen that the model tends to favour the majority class which resulted in a training recall score of 65% for the minority class which in turn impacted the precision scores of both classes.

Could the recall of the minority class (is Cantonese) be improved by assigning weights?


```python
logist_plots(lr,
             X_train=X_train,
             y_train=y_train,
             X_test=X_test,
             y_test=y_test)
```

    y_train classification report:
                  precision    recall  f1-score   support
    
             0.0       0.88      0.95      0.91     17525
             1.0       0.83      0.65      0.73      6722
    
        accuracy                           0.87     24247
       macro avg       0.85      0.80      0.82     24247
    weighted avg       0.86      0.87      0.86     24247
    
    y_test classification report:
                  precision    recall  f1-score   support
    
             0.0       0.87      0.94      0.90      4381
             1.0       0.81      0.63      0.71      1681
    
        accuracy                           0.86      6062
       macro avg       0.84      0.79      0.81      6062
    weighted avg       0.85      0.86      0.85      6062
    



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_179_1.png)
    


    ROC Curves:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_179_3.png)
    


    Precision Recall Curve:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_179_5.png)
    


    Threholds (test data) impact on Accuracy / False Positive Rate:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_179_7.png)
    


#### 9.1.5. Logistic Regression Model (2):  weights assigned to each class
Assigning a weight of 2 to the minority class improved the test recall rate from 63% previously to 81%. This was at the expense of majority class recall which declined from 94% to 76%. The mean cross validation score was 77% only slighlty ahead of the base line. 


```python
# Import and fit a logistic regression model and test
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold

Bold = '\033[1m'
Normal = '\033[0m'

skf = StratifiedKFold(n_splits=5)
skf.get_n_splits(X_train, y_train)

# assign weights to the classes
w = {0: 1, 1: 4}
lr = LogisticRegression(solver='lbfgs', multi_class='ovr', class_weight=w)
lr.fit(X_train, y_train)
lr_scores = cross_val_score(lr, X_train, y_train, cv=skf, n_jobs=6)

print(Bold + 'Base line score:' + Normal,
      np.round(y_train.value_counts(normalize=True)[0], 3))
print(Bold + 'Training Score:' + Normal, np.round(lr.score(X_train, y_train),
                                                  3))
print(Bold + 'Test Score:' + Normal, np.round(lr.score(X_test, y_test), 3))
print(Bold + "Cross-validated training scores:" + Normal,
      np.round(lr_scores, 4))
print(Bold + "Mean cross-validated training score:" + Normal,
      np.round(lr_scores.mean(), 3), '\n')
logist_plots(lr,
             X_train=X_train,
             y_train=y_train,
             X_test=X_test,
             y_test=y_test)
```

    [1mBase line score:[0m 0.723
    [1mTraining Score:[0m 0.798
    [1mTest Score:[0m 0.773
    [1mCross-validated training scores:[0m [0.7833 0.7736 0.7639 0.7771 0.7684]
    [1mMean cross-validated training score:[0m 0.773 
    
    y_train classification report:
                  precision    recall  f1-score   support
    
             0.0       0.93      0.78      0.85     17525
             1.0       0.59      0.86      0.70      6722
    
        accuracy                           0.80     24247
       macro avg       0.76      0.82      0.77     24247
    weighted avg       0.84      0.80      0.81     24247
    
    y_test classification report:
                  precision    recall  f1-score   support
    
             0.0       0.91      0.76      0.83      4381
             1.0       0.56      0.81      0.67      1681
    
        accuracy                           0.77      6062
       macro avg       0.74      0.79      0.75      6062
    weighted avg       0.82      0.77      0.78      6062
    



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_181_1.png)
    


    ROC Curves:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_181_3.png)
    


    Precision Recall Curve:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_181_5.png)
    


    Threholds (test data) impact on Accuracy / False Positive Rate:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_181_7.png)
    


#### 9.1.6. Logistic Regression Model (3):  increasing the number of features to include
I increased the number of max_features from 1000 to 5000. This increased the computation time but resulted in improved scores. The comparatively stronger training score of 90% compared with the mean cross val score of 78% suggests the model could be overfit. The recall of the minority class of 75% in the test data remains low. This could be improved by further increasing the weight applied to the minority class.



```python
# Get Tran test split from sample data
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    stratify=y,
                                                    test_size=0.4,
                                                    random_state=1)

cvec = CountVectorizer(tokenizer=man_token_jie, max_features=5000)
cvec.fit(X_train)
cvec_mat = cvec.transform(X_train)
X_train = cvec_mat
X_test = cvec.transform(X_test)

# Import and fit a logistic regression model and test
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold

Bold = '\033[1m'
Normal = '\033[0m'

skf = StratifiedKFold(n_splits=5)
skf.get_n_splits(X_train, y_train)
w = {0: 1, 1: 4}
lr = LogisticRegression(solver='lbfgs', multi_class='ovr', class_weight=w)
lr.fit(X_train, y_train)
lr_scores = cross_val_score(lr, X_train, y_train, cv=skf, n_jobs=6)

print(Bold + 'Base line score:' + Normal,
      np.round(y_train.value_counts(normalize=True)[0], 3))
print(Bold + 'Training Score:' + Normal, np.round(lr.score(X_train, y_train),
                                                  3))
print(Bold + 'Test Score:' + Normal, np.round(lr.score(X_test, y_test), 3))
print(Bold + "Cross-validated training scores:" + Normal,
      np.round(lr_scores, 4))
print(Bold + "Mean cross-validated training score:" + Normal,
      np.round(lr_scores.mean(), 3), '\n')
logist_plots(lr=lr,
             X_train=X_train,
             y_train=y_train,
             X_test=X_test,
             y_test=y_test)
```

    [1mBase line score:[0m 0.723
    [1mTraining Score:[0m 0.903
    [1mTest Score:[0m 0.794
    [1mCross-validated training scores:[0m [0.7825 0.7723 0.7784 0.791  0.7847]
    [1mMean cross-validated training score:[0m 0.782 
    
    y_train classification report:
                  precision    recall  f1-score   support
    
             0.0       0.98      0.88      0.93     13143
             1.0       0.76      0.96      0.84      5042
    
        accuracy                           0.90     18185
       macro avg       0.87      0.92      0.89     18185
    weighted avg       0.92      0.90      0.91     18185
    
    y_test classification report:
                  precision    recall  f1-score   support
    
             0.0       0.90      0.81      0.85      8763
             1.0       0.60      0.75      0.67      3361
    
        accuracy                           0.79     12124
       macro avg       0.75      0.78      0.76     12124
    weighted avg       0.81      0.79      0.80     12124
    



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_183_1.png)
    


    ROC Curves:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_183_3.png)
    


    Precision Recall Curve:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_183_5.png)
    


    Threholds (test data) impact on Accuracy / False Positive Rate:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_183_7.png)
    


#### 9.1.7. Logistic Regression Model (4):  with cross validation
I experimented with using Logistic Regression with cross validation to improve regularization. I used StratifiedKFold so that the split of classes was kept balanced during cross validation. To reduce computation time I tried only three alpha values and three L1 ratios and set the max_iterations to 100. Finally I cranked up the weight on the minority class to six (previously set at four) to try and further increase recall of the minority class.

Given that I increased the weighting on the minority class the training score is not directly comparable with the previous model and based on this metric alone would make this model look inferior. However, this is due to the lower  precision for the majority class which came at the expense of improving recall for the minority class which increased to 84% in the test data from 73% previously. The training, test and mean cross validation scores are more closely alligned (75-87%) suggesting that the model is less overfit. Overall this model is an improvement on previous ones however the model performance remains poor. 


```python
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import plot_confusion_matrix, plot_roc_curve, plot_precision_recall_curve, roc_auc_score, average_precision_score

Bold = '\033[1m'
Normal = '\033[0m'

# Get Tran test split from sample data
X_train, X_test, y_train, y_test = train_test_split(X,y,stratify=y,test_size=0.4,random_state=1)
skf = StratifiedKFold(n_splits=5)
skf.get_n_splits(X_train, y_train)

cvec = CountVectorizer(tokenizer=man_token_jie, max_features=5000)
cvec.fit(X_train)
cvec_mat = cvec.transform(X_train)
X_train = cvec_mat
X_test = cvec.transform(X_test)

# Cs represent lambda, therefore penalty severity
# 10 ^-4 as alpha, then 15 intervals up to 4^10
# for multi class add --> multi_class='ovr'    and change solver to --->     'solver': ['liblinear']
w = {0: 1, 1: 6}
LRCV = LogisticRegressionCV(Cs=np.logspace(-4, 4, 3),
                            penalty='elasticnet',
                            max_iter=100,
                            cv=skf,
                            solver='saga',
                            l1_ratios=[0.001, 0.1, 0.5],
                            n_jobs=6,
                            class_weight=w)  # cv=LRCV_kfolds
# fit the model
LRCV.fit(X_train, y_train)

lr_scores = cross_val_score(LRCV, X_train, y_train, cv=skf, n_jobs=6)

# get the best alpha
print(Bold + 'LRCV Model:' + Normal)
print('LRCV Best alpha:', LRCV.C_)
# get the best l1-ratio
print('Best l1-ratio:', LRCV.l1_ratio_)
# evaluate on the training set
print('LRCV CV Training score:', LRCV.score(X_train, y_train))
# evaluate on the test set
print("LRCV Test Score:", LRCV.score(X_test, y_test), '\n')
# ROC Train and test score
print('ROC Train and Test Score:',
      roc_auc_score(y_train,
                    LRCV.predict_proba(X_train)[:, 1]),
      roc_auc_score(y_test,
                    LRCV.predict_proba(X_test)[:, 1]))

print(Bold + "Cross-validated training scores:" + Normal,
      np.round(lr_scores, 4))
print(Bold + "Mean cross-validated training score:" + Normal,
      np.round(lr_scores.mean(), 3), '\n')
logist_plots(lr=LRCV,
             X_train=X_train,
             y_train=y_train,
             X_test=X_test,
             y_test=y_test)
```

    Building prefix dict from the default dictionary ...
    Loading model from cache /var/folders/87/4nvcbwyx25s9fz86v2nq_xmw0000gq/T/jieba.cache
    Loading model cost 0.527 seconds.
    Prefix dict has been built successfully.


    [1mLRCV Model:[0m
    LRCV Best alpha: [10000.]
    Best l1-ratio: [0.001]
    LRCV CV Training score: 0.8566950783612868
    LRCV Test Score: 0.7732596502804355 
    
    ROC Train and Test Score: 0.961037442977279 0.8676157526219471
    [1mCross-validated training scores:[0m [0.7778 0.7696 0.7586 0.7806 0.7677]
    [1mMean cross-validated training score:[0m 0.771 
    
    y_train classification report:
                  precision    recall  f1-score   support
    
             0.0       0.98      0.81      0.89     13143
             1.0       0.67      0.97      0.79      5042
    
        accuracy                           0.86     18185
       macro avg       0.83      0.89      0.84     18185
    weighted avg       0.90      0.86      0.86     18185
    
    y_test classification report:
                  precision    recall  f1-score   support
    
             0.0       0.92      0.75      0.83      8763
             1.0       0.56      0.83      0.67      3361
    
        accuracy                           0.77     12124
       macro avg       0.74      0.79      0.75     12124
    weighted avg       0.82      0.77      0.78     12124
    



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_185_2.png)
    


    ROC Curves:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_185_4.png)
    


    Precision Recall Curve:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_185_6.png)
    


#### 9.1.8. Logistic Regression Model (5):  with cross validation and stop words

Finally I tried adding stop words to remove the most commonly used words. My hypothesis was that this would reduce the effiacy of the model, as I suspected that it would be the most commonly used words which best distinguish a dialect. This proved to be the case with the minority class test recall declining to 81% (previously 87%). 


```python
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import plot_confusion_matrix, plot_roc_curve, plot_precision_recall_curve, roc_auc_score, average_precision_score

Bold = '\033[1m'
Normal = '\033[0m'

# Get Tran test split from sample data
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    stratify=y,
                                                    test_size=0.4,
                                                    random_state=1)

skf = StratifiedKFold(n_splits=5)
skf.get_n_splits(X_train, y_train)

cvec = CountVectorizer(tokenizer=man_token_jie,
                       max_features=5000,
                       stop_words=stopwords(["zh"]))
cvec.fit(X_train)
cvec_mat = cvec.transform(X_train)
X_train = cvec_mat
X_test = cvec.transform(X_test)

# Cs represent lambda, therefore penalty severity
# 10 ^-4 as alpha, then 15 intervals up to 4^10
# for multi class add --> multi_class='ovr'    and change solver to --->     'solver': ['liblinear']

w = {0: 1, 1: 6}
LRCV = LogisticRegressionCV(Cs=np.logspace(-4, 4, 3),
                            penalty='elasticnet',
                            max_iter=100,
                            cv=skf,
                            solver='saga',
                            l1_ratios=[0.001, 0.1, 0.5],
                            n_jobs=6,
                            class_weight=w)  # cv=LRCV_kfolds
# fit the model
LRCV.fit(X_train, y_train)

lr_scores = cross_val_score(LRCV, X_train, y_train, cv=skf, n_jobs=6)

### Some outputs

# get the best alpha
print(Bold + 'LRCV Model:' + Normal)
print('LRCV Best alpha:', LRCV.C_)
# get the best l1-ratio
print('Best l1-ratio:', LRCV.l1_ratio_)
# evaluate on the training set
print('LRCV CV Training score:', LRCV.score(X_train, y_train))
# evaluate on the test set
print("LRCV Test Score:", LRCV.score(X_test, y_test), '\n')
# ROC Train and test score
print('ROC Train and Test Score:',
      roc_auc_score(y_train,
                    LRCV.predict_proba(X_train)[:, 1]),
      roc_auc_score(y_test,
                    LRCV.predict_proba(X_test)[:, 1]))

print(Bold + "Cross-validated training scores:" + Normal,
      np.round(lr_scores, 4))
print(Bold + "Mean cross-validated training score:" + Normal,
      np.round(lr_scores.mean(), 3), '\n')
logist_plots(lr=LRCV,
             X_train=X_train,
             y_train=y_train,
             X_test=X_test,
             y_test=y_test)
```

    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(


    [1mLRCV Model:[0m
    LRCV Best alpha: [10000.]
    Best l1-ratio: [0.5]
    LRCV CV Training score: 0.8645587022271103
    LRCV Test Score: 0.7664137248432861 
    
    ROC Train and Test Score: 0.965502560957711 0.8543414887518839
    [1mCross-validated training scores:[0m [0.7602 0.755  0.7569 0.7734 0.766 ]
    [1mMean cross-validated training score:[0m 0.762 
    
    y_train classification report:
                  precision    recall  f1-score   support
    
             0.0       0.99      0.82      0.90     13143
             1.0       0.68      0.97      0.80      5042
    
        accuracy                           0.86     18185
       macro avg       0.83      0.90      0.85     18185
    weighted avg       0.90      0.86      0.87     18185
    
    y_test classification report:
                  precision    recall  f1-score   support
    
             0.0       0.91      0.75      0.82      8763
             1.0       0.55      0.80      0.66      3361
    
        accuracy                           0.77     12124
       macro avg       0.73      0.78      0.74     12124
    weighted avg       0.81      0.77      0.78     12124
    



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_188_2.png)
    


    ROC Curves:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_188_4.png)
    


    Precision Recall Curve:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_188_6.png)
    


    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(


#### 9.2.0 Appying Model to Data with no Genre


```python
# Apply model 5 to the songs where no genre is available to label those that are suspected of being "Cantopop"
df_tracks_no_genre = df_tracks_master_clean2_lyrics_genre_hsk_v2[
    (df_tracks_master_clean2_lyrics_genre_hsk_v2.genre_2.isnull()) |
    (df_tracks_master_clean2_lyrics_genre_hsk_v2.genre_2.isnull())]
df_tracks_no_genre_lyrics = df_tracks_no_genre['lyrics']
```


```python
cvec_mat_no_genre = cvec.transform(df_tracks_no_genre_lyrics)
```


```python
df_tracks_no_genre['predict_is_canton'] = LRCV.predict(cvec_mat_no_genre)
```


```python
df_tracks_no_genre.predict_is_canton.value_counts()
```




    0.0    11549
    1.0    10512
    Name: predict_is_canton, dtype: int64



    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge
      warnings.warn(


### 9.2. Spotify Meta Data to HSK Relationship
#### Summary 
Finding songs that are accessible at the right level is computationally intensive and time consuming given that the method used so far requires scraping lyrics for each song in the database and then processing these. Is it possible to speed up this process by identifying songs that are more likely to be at the right level based on Spotify data?

In model 1 I tried to identify songs that are HSK 1-4 vs HSK 5-6 from Spotify song meta data. The mean cross validation score was 54% compared with the base line of 47%. This score suggests that there is very limited information contained in song meta data for grading HSK level. 

In model 2 I added artist and genre. The mean cross validation score improved to 60%. The coefficient with the greatest weight was Chinese R&B. When looking at this genre it can be seen that 86% of songs have lyrics within HSK1-4 vs. the base line of 52%. Singoaporean Mandopop was the second coefficient and has 68% in scope for HSK 1-4. 

Whilst the model was poor overall at identifying HSK level it was able to identify some genres which may be more accessible to language learners. 

#### 9.2.1. Modelling


```python
df_h4 = df_tracks_master_clean3_lyrics_genre_hsk_v2.copy()
df_h4['is_h4'] = 0
df_h4['is_h4'][df_tracks_master_clean3_lyrics_genre_hsk_v2.hsk4 > 0.77] = 1
```


```python
df_h4['is_h4'].value_counts()
```




    1    18931
    0    16990
    Name: is_h4, dtype: int64




```python
X = df_h4[[
    'song_duration_ms', 'is_explicit', 'length', 'popularity', 'acousticness',
    'danceability', 'energy', 'instrumentalness', 'liveness', 'loudness',
    'speechiness', 'tempo', 'time_signature', 'is_h4'
]]

y = X.pop('is_h4')
```


```python
# Get Tran test split from sample data
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    stratify=y,
                                                    test_size=0.3,
                                                    random_state=1)

# Import and fit a logistic regression model and test
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold

Bold = '\033[1m'
Normal = '\033[0m'

scaler = StandardScaler()
X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X.columns)
X_test = pd.DataFrame(scaler.transform(X_test), columns=X.columns)

skf = StratifiedKFold(n_splits=5)
skf.get_n_splits(X_train, y_train)
w = {0: 1, 1: 1}
lr = LogisticRegression(solver='lbfgs', multi_class='ovr', class_weight=w)
lr.fit(X_train, y_train)
lr_scores = cross_val_score(lr, X_train, y_train, cv=skf, n_jobs=6)

print(Bold + 'Base line score:' + Normal,
      np.round(y_train.value_counts(normalize=True)[0], 3))
print(Bold + 'Training Score:' + Normal, np.round(lr.score(X_train, y_train),
                                                  3))
print(Bold + 'Test Score:' + Normal, np.round(lr.score(X_test, y_test), 3))
print(Bold + "Cross-validated training scores:" + Normal,
      np.round(lr_scores, 4))
print(Bold + "Mean cross-validated training score:" + Normal,
      np.round(lr_scores.mean(), 3), '\n')
logist_plots(lr=lr,
             X_train=X_train,
             y_train=y_train,
             X_test=X_test,
             y_test=y_test)
```

    [1mBase line score:[0m 0.473
    [1mTraining Score:[0m 0.547
    [1mTest Score:[0m 0.556
    [1mCross-validated training scores:[0m [0.5534 0.545  0.5454 0.5357 0.5493]
    [1mMean cross-validated training score:[0m 0.546 
    
    y_train classification report:
                  precision    recall  f1-score   support
    
               0       0.54      0.29      0.38     11893
               1       0.55      0.78      0.64     13251
    
        accuracy                           0.55     25144
       macro avg       0.54      0.53      0.51     25144
    weighted avg       0.54      0.55      0.52     25144
    
    y_test classification report:
                  precision    recall  f1-score   support
    
               0       0.56      0.29      0.39      5097
               1       0.56      0.79      0.65      5680
    
        accuracy                           0.56     10777
       macro avg       0.56      0.54      0.52     10777
    weighted avg       0.56      0.56      0.53     10777
    



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_199_1.png)
    


    ROC Curves:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_199_3.png)
    


    Precision Recall Curve:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_199_5.png)
    


    Threholds (test data) impact on Accuracy / False Positive Rate:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_199_7.png)
    


#### 9.2.2. Spotify Meta Data Plus Genre


```python
X = df_h4[[
    'artist', 'Genre1', 'Genre2', 'Genre3', 'song_duration_ms',
    'is_explicit', 'length', 'popularity', 'acousticness', 'danceability',
    'energy', 'instrumentalness', 'liveness', 'loudness', 'speechiness',
    'tempo', 'time_signature', 'is_h4']]
X = pd.get_dummies(X,columns=['artist', 'Genre1', 'Genre2', 'Genre3'],drop_first=True)
y = X.pop('is_h4')
```

There is some improvement in the scores overall by adding artist and genre. The mean cross validation score of 60% was ahead of the baseline score of 47%. 


```python
# Get Tran test split from sample data
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    stratify=y,
                                                    test_size=0.3,
                                                    random_state=1)

# Import and fit a logistic regression model and test
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold

Bold = '\033[1m'
Normal = '\033[0m'

scaler = StandardScaler()
X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X.columns)
X_test = pd.DataFrame(scaler.transform(X_test), columns=X.columns)

skf = StratifiedKFold(n_splits=5)
skf.get_n_splits(X_train, y_train)
w = {0: 1, 1: 1}
lr = LogisticRegression(solver='lbfgs', multi_class='ovr', class_weight=w)
lr.fit(X_train, y_train)
lr_scores = cross_val_score(lr, X_train, y_train, cv=skf, n_jobs=6)

print(Bold + 'Base line score:' + Normal,
      np.round(y_train.value_counts(normalize=True)[0], 3))
print(Bold + 'Training Score:' + Normal, np.round(lr.score(X_train, y_train),
                                                  3))
print(Bold + 'Test Score:' + Normal, np.round(lr.score(X_test, y_test), 3))
print(Bold + "Cross-validated training scores:" + Normal,
      np.round(lr_scores, 4))
print(Bold + "Mean cross-validated training score:" + Normal,
      np.round(lr_scores.mean(), 3), '\n')
logist_plots(lr=lr,
             X_train=X_train,
             y_train=y_train,
             X_test=X_test,
             y_test=y_test)
```

    [1mBase line score:[0m 0.473
    [1mTraining Score:[0m 0.642
    [1mTest Score:[0m 0.618
    [1mCross-validated training scores:[0m [0.6017 0.6071 0.5971 0.6059 0.613 ]
    [1mMean cross-validated training score:[0m 0.605 
    
    y_train classification report:
                  precision    recall  f1-score   support
    
               0       0.64      0.56      0.60     11893
               1       0.64      0.71      0.68     13251
    
        accuracy                           0.64     25144
       macro avg       0.64      0.64      0.64     25144
    weighted avg       0.64      0.64      0.64     25144
    
    y_test classification report:
                  precision    recall  f1-score   support
    
               0       0.61      0.53      0.57      5097
               1       0.62      0.70      0.66      5680
    
        accuracy                           0.62     10777
       macro avg       0.62      0.61      0.61     10777
    weighted avg       0.62      0.62      0.61     10777
    



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_203_1.png)
    


    ROC Curves:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_203_3.png)
    


    Precision Recall Curve:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_203_5.png)
    


    Threholds (test data) impact on Accuracy / False Positive Rate:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_203_7.png)
    



```python
results = pd.DataFrame(list(zip(
    X_train.columns,
    lr.coef_[0],
)),
                       columns=['coef',
                                'coef_mag']).sort_values(by='coef_mag',
                                                         ascending=False)
```


```python
results.head(15)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>coef</th>
      <th>coef_mag</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1000</th>
      <td>Genre2_chinese r&amp;b</td>
      <td>0.492115</td>
    </tr>
    <tr>
      <th>1023</th>
      <td>Genre2_singaporean mandopop</td>
      <td>0.438669</td>
    </tr>
    <tr>
      <th>900</th>
      <td>Genre1_c-pop</td>
      <td>0.415215</td>
    </tr>
    <tr>
      <th>997</th>
      <td>Genre2_chinese post-punk</td>
      <td>0.409854</td>
    </tr>
    <tr>
      <th>1056</th>
      <td>Genre3_chinese singer-songwriter</td>
      <td>0.357753</td>
    </tr>
    <tr>
      <th>906</th>
      <td>Genre1_chinese electropop</td>
      <td>0.348468</td>
    </tr>
    <tr>
      <th>950</th>
      <td>Genre1_mandopop</td>
      <td>0.317644</td>
    </tr>
    <tr>
      <th>920</th>
      <td>Genre1_chinese post-punk</td>
      <td>0.302995</td>
    </tr>
    <tr>
      <th>373</th>
      <td>artist_Stefanie Sun</td>
      <td>0.296506</td>
    </tr>
    <tr>
      <th>391</th>
      <td>artist_Tanya Chua</td>
      <td>0.261405</td>
    </tr>
    <tr>
      <th>245</th>
      <td>artist_Leo王</td>
      <td>0.250308</td>
    </tr>
    <tr>
      <th>911</th>
      <td>Genre1_chinese indie</td>
      <td>0.249276</td>
    </tr>
    <tr>
      <th>1016</th>
      <td>Genre2_lo star</td>
      <td>0.245622</td>
    </tr>
    <tr>
      <th>1019</th>
      <td>Genre2_mandopop</td>
      <td>0.222118</td>
    </tr>
    <tr>
      <th>397</th>
      <td>artist_The Life Journey</td>
      <td>0.216112</td>
    </tr>
  </tbody>
</table>
</div>



Does this mean that Chinese R&B is the most accessible genre for Chinese language learners??


```python
df_h4['is_h4'].value_counts(normalize=True)
```




    1    0.527018
    0    0.472982
    Name: is_h4, dtype: float64




```python
df_h4['is_h4'][df_h4['Genre2'] == 'chinese r&b'].value_counts(normalize=True)
```




    1    0.864286
    0    0.135714
    Name: is_h4, dtype: float64



Yes! Chinese R&B songs fall into category HSK 1 to HSK 4 in 86% of cases vs. the overall genre average of 52%. 


```python
df_h4['is_h4'][df_h4['Genre2'] == 'singaporean mandopop'].value_counts(normalize=True)
```




    1    0.680328
    0    0.319672
    Name: is_h4, dtype: float64



Similarly for Singaporean Mandopop

### 9.3 Binary Classification of HSK Based on Lyrics

#### 9.3.1. Binary Classification is HSK 1 - 5: Logistic Regression, TfidfVectorizer,  weights (50:50)
- Mean Cross validation score of 61% vs. base line of 53%. 
- Recall (train) of HSK5 is 100% with precision of 82%. 
- Recall (train) of 75% for not HSK5 with precision of 100%.


```python
# import data file
df_tracks_master_clean3_lyrics_genre_hsk_v2 = pd.read_csv(
    '/Users/stuart/Desktop/Spoty-Linguist-Project/df_tracks_master_clean3_lyrics_genre_hsk_v2.csv',
    sep=',',
    index_col=0)
```


```python
# Create HSK5 binary variable
df_h5 = df_tracks_master_clean3_lyrics_genre_hsk_v2.copy()
df_h5['is_h5'] = 'NotHSK'
df_h5['is_h5'][df_tracks_master_clean3_lyrics_genre_hsk_v2.hsk5 > 0.9] = 'HSK5'

X = df_h5[['lyrics','is_h5'
]]

y = X.pop('is_h5')
X=X['lyrics']

df_h5['is_h5'].value_counts()
```




    HSK5      18893
    NotHSK    17028
    Name: is_h5, dtype: int64




```python
# Logistis Regression
from time import time
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import RidgeClassifier
from sklearn.pipeline import Pipeline
from sklearn.svm import LinearSVC
from sklearn.linear_model import SGDClassifier
from sklearn.linear_model import Perceptron
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.naive_bayes import BernoulliNB, MultinomialNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors import NearestCentroid
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils.extmath import density
from sklearn.feature_selection import SelectFromModel
from sklearn import metrics


# Get Train test split from sample data
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    stratify=y,
                                                    test_size=0.2,
                                                    random_state=1)

w = {'HSK5': 1, 'NotHSK': 1}

model = make_pipeline(TfidfVectorizer(stop_words=updated_stopwords,
                                      # sublinear_tf=True,
                                      max_df=2,
                                      min_df = 1,
                                      max_features=50000,
                                      tokenizer=man_token_jie,
                                      norm='l2'),
                                      LogisticRegression(solver='lbfgs', 
                                        multi_class='ovr', 
                                        class_weight=w),
                      )

model.fit(X_train, y_train)

Bold = '\033[1m'
Normal = '\033[0m'

skf = StratifiedKFold(n_splits=5)
skf.get_n_splits(X_train, y_train)

lr_scores = cross_val_score(model, X_train, y_train, cv=skf, n_jobs=6)

print(Bold + 'Base line score:' + Normal,
      np.round(y_train.value_counts(normalize=True)[0], 3))
print(Bold + 'Training Score:' + Normal, np.round(model.score(X_train, y_train),
                                                  3))
print(Bold + 'Test Score:' + Normal, np.round(model.score(X_test, y_test), 3))
print(Bold + "Cross-validated training scores:" + Normal,
      np.round(lr_scores, 4))
print(Bold + "Mean cross-validated training score:" + Normal,
      np.round(lr_scores.mean(), 3), '\n')
```

    Building prefix dict from the default dictionary ...
    Loading model from cache /var/folders/87/4nvcbwyx25s9fz86v2nq_xmw0000gq/T/jieba.cache
    Building prefix dict from the default dictionary ...
    Loading model from cache /var/folders/87/4nvcbwyx25s9fz86v2nq_xmw0000gq/T/jieba.cache
    Building prefix dict from the default dictionary ...
    Loading model from cache /var/folders/87/4nvcbwyx25s9fz86v2nq_xmw0000gq/T/jieba.cache
    Building prefix dict from the default dictionary ...
    Loading model from cache /var/folders/87/4nvcbwyx25s9fz86v2nq_xmw0000gq/T/jieba.cache
    Building prefix dict from the default dictionary ...
    Loading model from cache /var/folders/87/4nvcbwyx25s9fz86v2nq_xmw0000gq/T/jieba.cache
    Loading model cost 0.665 seconds.
    Prefix dict has been built successfully.
    Loading model cost 0.674 seconds.
    Prefix dict has been built successfully.
    Loading model cost 0.659 seconds.
    Prefix dict has been built successfully.
    Loading model cost 0.646 seconds.
    Prefix dict has been built successfully.
    Loading model cost 0.644 seconds.
    Prefix dict has been built successfully.


    [1mBase line score:[0m 0.526
    [1mTraining Score:[0m 0.883
    [1mTest Score:[0m 0.606
    [1mCross-validated training scores:[0m [0.6079 0.6174 0.6038 0.6111 0.6109]
    [1mMean cross-validated training score:[0m 0.61 
    



```python
logist_plots(model,
             X_train=X_train,
             y_train=y_train,
             X_test=X_test,
             y_test=y_test)
```

    y_train classification report:
                  precision    recall  f1-score   support
    
            HSK5       0.82      1.00      0.90     15114
          NotHSK       1.00      0.75      0.86     13622
    
        accuracy                           0.88     28736
       macro avg       0.91      0.88      0.88     28736
    weighted avg       0.90      0.88      0.88     28736
    
    y_test classification report:
                  precision    recall  f1-score   support
    
            HSK5       0.59      0.82      0.69      3779
          NotHSK       0.65      0.36      0.47      3406
    
        accuracy                           0.61      7185
       macro avg       0.62      0.59      0.58      7185
    weighted avg       0.62      0.61      0.58      7185
    



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_217_1.png)
    


    ROC Curves:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_217_3.png)
    


    Precision Recall Curve:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_217_5.png)
    


    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['借傥', '兼', '前', '唷', '啪', '啷', '喔', '天', '始', '漫', '然', '特', '竟', '莫', '见', '设', '达'] not in stop_words.
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['借傥', '兼', '前', '唷', '啪', '啷', '喔', '天', '始', '漫', '然', '特', '竟', '莫', '见', '设', '达'] not in stop_words.
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['借傥', '兼', '前', '唷', '啪', '啷', '喔', '天', '始', '漫', '然', '特', '竟', '莫', '见', '设', '达'] not in stop_words.
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['借傥', '兼', '前', '唷', '啪', '啷', '喔', '天', '始', '漫', '然', '特', '竟', '莫', '见', '设', '达'] not in stop_words.
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['借傥', '兼', '前', '唷', '啪', '啷', '喔', '天', '始', '漫', '然', '特', '竟', '莫', '见', '设', '达'] not in stop_words.
      warnings.warn(


#### 9.3.2.Binary Classification is HSK 1 - 5: Logistic Regression, TfidfVectorizer,  weights (1.05 on minority)

There is a tipping point for weights above and below 1.2:

Applying weights of 1.1-1.19:
- Mean cross validation score reduced to 53% vs. base line of 53% and 63% previously. 
- Recall (train) of HSK5 98% precision of 83%.
- Recall (train) of NotHSK5 78% and precision of 98%

Applying weights of 1.2:
- Mean cross validation score reduced to 53%
- Recall (train) of HSK5 61% precision of 100%.
- Recall (train) of NotHSK5 100% and precision of 70%


```python
## wieght updated to w = {'HSK5': 1, 'NotHSK': 1.199}
w = {'HSK5': 1, 'NotHSK': 1.199}

model = make_pipeline(TfidfVectorizer(stop_words=updated_stopwords,
                                      # sublinear_tf=True,
                                      max_df=2,
                                      min_df = 1,
                                      max_features=50000,
                                      tokenizer=man_token_jie,
                                      norm='l2'),
                                      LogisticRegression(solver='lbfgs', 
                                        multi_class='ovr', 
                                        class_weight=w),
                      )

model.fit(X_train, y_train)

Bold = '\033[1m'
Normal = '\033[0m'

skf = StratifiedKFold(n_splits=5)
skf.get_n_splits(X_train, y_train)

lr_scores = cross_val_score(model, X_train, y_train, cv=skf, n_jobs=6)

print(Bold + 'Base line score:' + Normal,
      np.round(y_train.value_counts(normalize=True)[0], 3))
print(Bold + 'Training Score:' + Normal, np.round(model.score(X_train, y_train),
                                                  3))
print(Bold + 'Test Score:' + Normal, np.round(model.score(X_test, y_test), 3))
print(Bold + "Cross-validated training scores:" + Normal,
      np.round(lr_scores, 4))
print(Bold + "Mean cross-validated training score:" + Normal,
      np.round(lr_scores.mean(), 3), '\n')
```

    Building prefix dict from the default dictionary ...
    Loading model from cache /var/folders/87/4nvcbwyx25s9fz86v2nq_xmw0000gq/T/jieba.cache
    Building prefix dict from the default dictionary ...
    Loading model from cache /var/folders/87/4nvcbwyx25s9fz86v2nq_xmw0000gq/T/jieba.cache
    Building prefix dict from the default dictionary ...
    Loading model from cache /var/folders/87/4nvcbwyx25s9fz86v2nq_xmw0000gq/T/jieba.cache
    Building prefix dict from the default dictionary ...
    Loading model from cache /var/folders/87/4nvcbwyx25s9fz86v2nq_xmw0000gq/T/jieba.cache
    Building prefix dict from the default dictionary ...
    Loading model from cache /var/folders/87/4nvcbwyx25s9fz86v2nq_xmw0000gq/T/jieba.cache
    Loading model cost 0.691 seconds.
    Prefix dict has been built successfully.
    Loading model cost 0.669 seconds.
    Prefix dict has been built successfully.
    Loading model cost 0.651 seconds.
    Prefix dict has been built successfully.
    Loading model cost 0.632 seconds.
    Prefix dict has been built successfully.
    Loading model cost 0.657 seconds.
    Prefix dict has been built successfully.


    [1mBase line score:[0m 0.526
    [1mTraining Score:[0m 0.886
    [1mTest Score:[0m 0.62
    [1mCross-validated training scores:[0m [0.5186 0.5269 0.5318 0.5384 0.5281]
    [1mMean cross-validated training score:[0m 0.529 
    



```python
logist_plots(model,
             X_train=X_train,
             y_train=y_train,
             X_test=X_test,
             y_test=y_test)
```

    y_train classification report:
                  precision    recall  f1-score   support
    
            HSK5       0.83      0.98      0.90     15114
          NotHSK       0.98      0.78      0.87     13622
    
        accuracy                           0.89     28736
       macro avg       0.90      0.88      0.88     28736
    weighted avg       0.90      0.89      0.88     28736
    
    y_test classification report:
                  precision    recall  f1-score   support
    
            HSK5       0.61      0.74      0.67      3779
          NotHSK       0.63      0.48      0.55      3406
    
        accuracy                           0.62      7185
       macro avg       0.62      0.61      0.61      7185
    weighted avg       0.62      0.62      0.61      7185
    



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_220_1.png)
    


    ROC Curves:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_220_3.png)
    


    Precision Recall Curve:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_220_5.png)
    



```python
# weights updated to w = {'HSK5': 1, 'NotHSK': 1.2}
w = {'HSK5': 1, 'NotHSK': 1.2}

model = make_pipeline(TfidfVectorizer(stop_words=updated_stopwords,
                                      # sublinear_tf=True,
                                      max_df=2,
                                      min_df = 1,
                                      max_features=50000,
                                      tokenizer=man_token_jie,
                                      norm='l2'),
                                      LogisticRegression(solver='lbfgs', 
                                        multi_class='ovr', 
                                        class_weight=w),
                      )

model.fit(X_train, y_train)

Bold = '\033[1m'
Normal = '\033[0m'

skf = StratifiedKFold(n_splits=5)
skf.get_n_splits(X_train, y_train)

lr_scores = cross_val_score(model, X_train, y_train, cv=skf, n_jobs=6)

print(Bold + 'Base line score:' + Normal,
      np.round(y_train.value_counts(normalize=True)[0], 3))
print(Bold + 'Training Score:' + Normal, np.round(model.score(X_train, y_train),
                                                  3))
print(Bold + 'Test Score:' + Normal, np.round(model.score(X_test, y_test), 3))
print(Bold + "Cross-validated training scores:" + Normal,
      np.round(lr_scores, 4))
print(Bold + "Mean cross-validated training score:" + Normal,
      np.round(lr_scores.mean(), 3), '\n')

logist_plots(model,
             X_train=X_train,
             y_train=y_train,
             X_test=X_test,
             y_test=y_test)
```

    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['借傥', '兼', '前', '唷', '啪', '啷', '喔', '天', '始', '漫', '然', '特', '竟', '莫', '见', '设', '达'] not in stop_words.
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['借傥', '兼', '前', '唷', '啪', '啷', '喔', '天', '始', '漫', '然', '特', '竟', '莫', '见', '设', '达'] not in stop_words.
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['借傥', '兼', '前', '唷', '啪', '啷', '喔', '天', '始', '漫', '然', '特', '竟', '莫', '见', '设', '达'] not in stop_words.
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['借傥', '兼', '前', '唷', '啪', '啷', '喔', '天', '始', '漫', '然', '特', '竟', '莫', '见', '设', '达'] not in stop_words.
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['借傥', '兼', '前', '唷', '啪', '啷', '喔', '天', '始', '漫', '然', '特', '竟', '莫', '见', '设', '达'] not in stop_words.
      warnings.warn(
    Building prefix dict from the default dictionary ...
    Loading model from cache /var/folders/87/4nvcbwyx25s9fz86v2nq_xmw0000gq/T/jieba.cache
    Building prefix dict from the default dictionary ...
    Loading model from cache /var/folders/87/4nvcbwyx25s9fz86v2nq_xmw0000gq/T/jieba.cache
    Building prefix dict from the default dictionary ...
    Loading model from cache /var/folders/87/4nvcbwyx25s9fz86v2nq_xmw0000gq/T/jieba.cache
    Building prefix dict from the default dictionary ...
    Loading model from cache /var/folders/87/4nvcbwyx25s9fz86v2nq_xmw0000gq/T/jieba.cache
    Building prefix dict from the default dictionary ...
    Loading model from cache /var/folders/87/4nvcbwyx25s9fz86v2nq_xmw0000gq/T/jieba.cache
    Loading model cost 0.649 seconds.
    Prefix dict has been built successfully.
    Loading model cost 0.640 seconds.
    Prefix dict has been built successfully.
    Loading model cost 0.626 seconds.
    Prefix dict has been built successfully.
    Loading model cost 0.637 seconds.
    Prefix dict has been built successfully.
    Loading model cost 0.640 seconds.
    Prefix dict has been built successfully.


    [1mBase line score:[0m 0.526
    [1mTraining Score:[0m 0.795
    [1mTest Score:[0m 0.522
    [1mCross-validated training scores:[0m [0.5188 0.5272 0.5319 0.5384 0.5286]
    [1mMean cross-validated training score:[0m 0.529 
    
    y_train classification report:
                  precision    recall  f1-score   support
    
            HSK5       1.00      0.61      0.76     15114
          NotHSK       0.70      1.00      0.82     13622
    
        accuracy                           0.79     28736
       macro avg       0.85      0.80      0.79     28736
    weighted avg       0.86      0.79      0.79     28736
    
    y_test classification report:
                  precision    recall  f1-score   support
    
            HSK5       0.60      0.28      0.38      3779
          NotHSK       0.50      0.79      0.61      3406
    
        accuracy                           0.52      7185
       macro avg       0.55      0.54      0.50      7185
    weighted avg       0.55      0.52      0.49      7185
    



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_221_2.png)
    


    ROC Curves:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_221_4.png)
    


    Precision Recall Curve:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_221_6.png)
    


    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['借傥', '兼', '前', '唷', '啪', '啷', '喔', '天', '始', '漫', '然', '特', '竟', '莫', '见', '设', '达'] not in stop_words.
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['借傥', '兼', '前', '唷', '啪', '啷', '喔', '天', '始', '漫', '然', '特', '竟', '莫', '见', '设', '达'] not in stop_words.
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['借傥', '兼', '前', '唷', '啪', '啷', '喔', '天', '始', '漫', '然', '特', '竟', '莫', '见', '设', '达'] not in stop_words.
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['借傥', '兼', '前', '唷', '啪', '啷', '喔', '天', '始', '漫', '然', '特', '竟', '莫', '见', '设', '达'] not in stop_words.
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['借傥', '兼', '前', '唷', '啪', '啷', '喔', '天', '始', '漫', '然', '特', '竟', '莫', '见', '设', '达'] not in stop_words.
      warnings.warn(


#### 9.3.2.Binary Classification is HSK 1 - 5: Logistic Regression, CountVectorizer,  weights (1.2 on minority)
Given that HSK levels are losely based on frequency of word occurence it makes sense to try and CountVectorizer model.

Weights were kept even at 1:1.

- Training score of 99.7% (vs. baseline of 53%).
- Mean cross validation score of 77% (ahead of previous best of 63%).
- Recall (train) score of HSK5 100% precision of 100%
- Recall (train) score of NotHSK5 100% precision of 100%
- Recall and Precision of Test data 76-80% accross both classes

Model has significantly improved using CVEC. Strong training score vs. mean cross validation score implies some model overfitting. 


```python
# weights kept at 1:1
w = {'HSK5': 1, 'NotHSK': 1}

model = make_pipeline(CountVectorizer(stop_words=updated_stopwords,
                                      # sublinear_tf=True,
                                      max_features=50000,
                                      tokenizer=man_token_jie),
                                      LogisticRegression(solver='lbfgs', 
                                        multi_class='ovr', 
                                        class_weight=w),
                      )

model.fit(X_train, y_train)

Bold = '\033[1m'
Normal = '\033[0m'

skf = StratifiedKFold(n_splits=5)
skf.get_n_splits(X_train, y_train)

lr_scores = cross_val_score(model, X_train, y_train, cv=skf, n_jobs=6)

print(Bold + 'Base line score:' + Normal,
      np.round(y_train.value_counts(normalize=True)[0], 3))
print(Bold + 'Training Score:' + Normal, np.round(model.score(X_train, y_train),
                                                  3))
print(Bold + 'Test Score:' + Normal, np.round(model.score(X_test, y_test), 3))
print(Bold + "Cross-validated training scores:" + Normal,
      np.round(lr_scores, 4))
print(Bold + "Mean cross-validated training score:" + Normal,
      np.round(lr_scores.mean(), 3), '\n')

logist_plots(model,
             X_train=X_train,
             y_train=y_train,
             X_test=X_test,
             y_test=y_test)
```

    Building prefix dict from the default dictionary ...
    Loading model from cache /var/folders/87/4nvcbwyx25s9fz86v2nq_xmw0000gq/T/jieba.cache
    Building prefix dict from the default dictionary ...
    Loading model from cache /var/folders/87/4nvcbwyx25s9fz86v2nq_xmw0000gq/T/jieba.cache
    Building prefix dict from the default dictionary ...
    Loading model from cache /var/folders/87/4nvcbwyx25s9fz86v2nq_xmw0000gq/T/jieba.cache
    Building prefix dict from the default dictionary ...
    Loading model from cache /var/folders/87/4nvcbwyx25s9fz86v2nq_xmw0000gq/T/jieba.cache
    Building prefix dict from the default dictionary ...
    Loading model from cache /var/folders/87/4nvcbwyx25s9fz86v2nq_xmw0000gq/T/jieba.cache
    Loading model cost 0.706 seconds.
    Prefix dict has been built successfully.
    Loading model cost 0.684 seconds.
    Prefix dict has been built successfully.
    Loading model cost 0.688 seconds.
    Prefix dict has been built successfully.
    Loading model cost 0.678 seconds.
    Prefix dict has been built successfully.
    Loading model cost 0.680 seconds.
    Prefix dict has been built successfully.


    [1mBase line score:[0m 0.526
    [1mTraining Score:[0m 0.997
    [1mTest Score:[0m 0.782
    [1mCross-validated training scores:[0m [0.7747 0.7651 0.7783 0.7724 0.7654]
    [1mMean cross-validated training score:[0m 0.771 
    
    y_train classification report:
                  precision    recall  f1-score   support
    
            HSK5       1.00      1.00      1.00     15114
          NotHSK       1.00      1.00      1.00     13622
    
        accuracy                           1.00     28736
       macro avg       1.00      1.00      1.00     28736
    weighted avg       1.00      1.00      1.00     28736
    
    y_test classification report:
                  precision    recall  f1-score   support
    
            HSK5       0.79      0.80      0.79      3779
          NotHSK       0.78      0.76      0.77      3406
    
        accuracy                           0.78      7185
       macro avg       0.78      0.78      0.78      7185
    weighted avg       0.78      0.78      0.78      7185
    



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_223_2.png)
    


    ROC Curves:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_223_4.png)
    


    Precision Recall Curve:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_223_6.png)
    


    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['借傥', '兼', '前', '唷', '啪', '啷', '喔', '天', '始', '漫', '然', '特', '竟', '莫', '见', '设', '达'] not in stop_words.
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
    
    Increase the number of iterations (max_iter) or scale the data as shown in:
        https://scikit-learn.org/stable/modules/preprocessing.html
    Please also refer to the documentation for alternative solver options:
        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
      n_iter_i = _check_optimize_result(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['借傥', '兼', '前', '唷', '啪', '啷', '喔', '天', '始', '漫', '然', '特', '竟', '莫', '见', '设', '达'] not in stop_words.
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
    
    Increase the number of iterations (max_iter) or scale the data as shown in:
        https://scikit-learn.org/stable/modules/preprocessing.html
    Please also refer to the documentation for alternative solver options:
        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
      n_iter_i = _check_optimize_result(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['借傥', '兼', '前', '唷', '啪', '啷', '喔', '天', '始', '漫', '然', '特', '竟', '莫', '见', '设', '达'] not in stop_words.
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
    
    Increase the number of iterations (max_iter) or scale the data as shown in:
        https://scikit-learn.org/stable/modules/preprocessing.html
    Please also refer to the documentation for alternative solver options:
        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
      n_iter_i = _check_optimize_result(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['借傥', '兼', '前', '唷', '啪', '啷', '喔', '天', '始', '漫', '然', '特', '竟', '莫', '见', '设', '达'] not in stop_words.
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
    
    Increase the number of iterations (max_iter) or scale the data as shown in:
        https://scikit-learn.org/stable/modules/preprocessing.html
    Please also refer to the documentation for alternative solver options:
        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
      n_iter_i = _check_optimize_result(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['借傥', '兼', '前', '唷', '啪', '啷', '喔', '天', '始', '漫', '然', '特', '竟', '莫', '见', '设', '达'] not in stop_words.
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
    
    Increase the number of iterations (max_iter) or scale the data as shown in:
        https://scikit-learn.org/stable/modules/preprocessing.html
    Please also refer to the documentation for alternative solver options:
        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
      n_iter_i = _check_optimize_result(


#### 9.3.3. Binary Regression: All Classifiers


```python
# Create HSK5 binary variable
df_h5 = df_tracks_master_clean3_lyrics_genre_hsk_v2.copy()
df_h5['is_h5'] = 'NotHSK'
df_h5['is_h5'][df_tracks_master_clean3_lyrics_genre_hsk_v2.hsk5 > 0.9] = 'HSK5'
X = df_h5[['lyrics','is_h5'
]]
y = X.pop('is_h5')
X = X['lyrics']
df_h5['is_h5'].value_counts()
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    stratify=y,
                                                    test_size=0.2,
                                                    random_state=1)

```


```python
from time import time
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import RidgeClassifier
from sklearn.pipeline import Pipeline
from sklearn.svm import LinearSVC
from sklearn.linear_model import SGDClassifier
from sklearn.linear_model import Perceptron
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.naive_bayes import BernoulliNB, MultinomialNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors import NearestCentroid
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils.extmath import density
from sklearn.feature_selection import SelectFromModel
from sklearn import metrics




w = {'HSK5': 1, 'NotHSK': 1}

cvec = CountVectorizer(stop_words=updated_stopwords,
                                      # sublinear_tf=True,
                                      max_features=50000,
                                      tokenizer=man_token_jie)




cvec.fit(X_train)
X_train = cvec.transform(X_train)
X_test = cvec.transform(X_test)


feature_names = np.array(cvec.get_feature_names())


def trim(s):
    """Trim string to fit on terminal (assuming 80-column display)"""
    return s if len(s) <= 80 else s[:77] + "..."

###############################################################################
# Benchmark classifiers


def benchmark(clf):
    print(('_' * 80))
    print("Training: ")
    print(clf)
    t0 = time()
    clf.fit(X_train, y_train)
    train_time = time() - t0
    print(("train time: %0.3fs" % train_time))

    t0 = time()
    pred = clf.predict(X_test)
    test_time = time() - t0
    print(("test time:  %0.3fs" % test_time))

    score = metrics.accuracy_score(y_test, pred)
    print(("accuracy:   %0.3f" % score))

    if hasattr(clf, 'coef_'):
        print(("dimensionality: %d" % clf.coef_.shape[1]))
        print(("density: %f" % density(clf.coef_)))
        
        try: 
            if feature_names is not None:
                print("top 10 keywords per class:")
                for i, category in enumerate(set(y_train.values)):
                    top10 = np.argsort(clf.coef_[i])[-10:]
                    print((trim("%s: %s"
                                % (category, " ".join(feature_names[top10])))))
            print()
        except: print('Top 10 features not available')
            
    print("classification report:")
    print((metrics.classification_report(y_test, pred,
                                         target_names=set(y_train.values))))

    print("confusion matrix:")
    print((metrics.confusion_matrix(y_test, pred)))

    print()
    clf_descr = str(clf).split('(')[0]
    return clf_descr, score, train_time, test_time


results = []
for clf, name in ((Perceptron(max_iter=1000, tol=1e-3, class_weight=w, n_jobs=-1), "Perceptron"),(PassiveAggressiveClassifier(max_iter=1000, tol=1e-3, class_weight=w,n_jobs=-1), "Passive-Aggressive"),(KNeighborsClassifier(n_neighbors=10, n_jobs=-1), "kNN"),(RandomForestClassifier(n_estimators=100, class_weight=w, n_jobs=-1), "Random forest")):
    print(('=' * 80))
    print(name)
    try: 
        results.append(benchmark(clf))
    except:
        results.append(np.nan)

for penalty in ["l2", "l1"]:
    print(('=' * 80))
    print(("%s penalty" % penalty.upper()))
    # Train Liblinear model
    try: 
        results.append(benchmark(LinearSVC(loss='squared_hinge', penalty=penalty,
                                                dual=False)))
    except:  results.append(np.nan)

    # Train SGD model
    try: 
        results.append(benchmark(SGDClassifier(alpha=.0001,
                                               penalty=penalty,
                                               max_iter=1000,
                                               tol=1e-3)))
    except: results.append(np.nan)

# Train SGD with Elastic Net penalty
print(('=' * 80))
print("Elastic-Net penalty")
try: 
    results.append(benchmark(SGDClassifier(alpha=.0001,
                                           penalty="elasticnet",
                                           max_iter=1000,
                                           tol=1e-3)))
except: results.append(np.nan)
    
# Train NearestCentroid without threshold
print(('=' * 80))
print("NearestCentroid (aka Rocchio classifier)")
try:
    results.append(benchmark(NearestCentroid()))
except:
    retults.append(np.nan)
# Train sparse Naive Bayes classifiers
print(('=' * 80))
print("Naive Bayes")
try: 
    results.append(benchmark(MultinomialNB(alpha=.01)))
    results.append(benchmark(BernoulliNB(alpha=.01)))
except:
        results.append(np.nan)
        results.append(np.nan)

print(('=' * 80))
print("LinearSVC with L1-based feature selection")
# The smaller C, the stronger the regularization.
# The more regularization, the more sparsity.
try:
    results.append(benchmark(Pipeline([
        ('feature_selection', SelectFromModel(LinearSVC(penalty="l1", dual=False))),
        ('classification', LinearSVC(penalty="l2"))])))
except:
    results.append(np.nan)
    results.append(np.nan
                  )
# make some plots

indices = np.arange(len(results))

results = [[x[i] for x in results] for i in range(4)]

clf_names, score, training_time, test_time = results
training_time = np.array(training_time) / np.max(training_time)
test_time = np.array(test_time) / np.max(test_time)

plt.figure(figsize=(12, 8))
plt.title("Score")
plt.barh(indices, score, .2, label="score", color='r')
plt.barh(indices + .3, training_time, .2, label="training time", color='g')
plt.barh(indices + .6, test_time, .2, label="test time", color='b')
plt.yticks((), fontsize=14)
plt.xticks(fontsize=14)
plt.legend(loc='best')
plt.subplots_adjust(left=.25)
plt.subplots_adjust(top=.95)
plt.subplots_adjust(bottom=.05)

for i, c in zip(indices, clf_names):
    plt.text(-.3, i, c, fontsize=14)

plt.show()
```

    ================================================================================
    Perceptron
    ________________________________________________________________________________
    Training: 
    Perceptron(class_weight={'HSK5': 1, 'NotHSK': 1}, n_jobs=-1)
    train time: 0.210s
    test time:  0.002s
    accuracy:   0.760
    dimensionality: 50000
    density: 0.899960
    top 10 keywords per class:
    HSK5: 荒谬 尽快 虔诚 神情 恍惚 盘旋 创作 又哭又笑 晨光 奢侈
    Top 10 features not available
    classification report:
                  precision    recall  f1-score   support
    
            HSK5       0.76      0.80      0.78      3779
          NotHSK       0.76      0.72      0.74      3406
    
        accuracy                           0.76      7185
       macro avg       0.76      0.76      0.76      7185
    weighted avg       0.76      0.76      0.76      7185
    
    confusion matrix:
    [[3021  758]
     [ 969 2437]]
    
    ================================================================================
    Passive-Aggressive
    ________________________________________________________________________________
    Training: 
    PassiveAggressiveClassifier(class_weight={'HSK5': 1, 'NotHSK': 1}, n_jobs=-1)
    train time: 0.462s
    test time:  0.001s
    accuracy:   0.761
    dimensionality: 50000
    density: 0.961740
    top 10 keywords per class:
    HSK5: 号 进行曲 編曲 无忧无虑 器乐 搞砸 版本 未经许可 混音 血肉
    Top 10 features not available
    classification report:
                  precision    recall  f1-score   support
    
            HSK5       0.77      0.79      0.78      3779
          NotHSK       0.76      0.73      0.74      3406
    
        accuracy                           0.76      7185
       macro avg       0.76      0.76      0.76      7185
    weighted avg       0.76      0.76      0.76      7185
    
    confusion matrix:
    [[2974  805]
     [ 913 2493]]
    
    ================================================================================
    kNN
    ________________________________________________________________________________
    Training: 
    KNeighborsClassifier(n_jobs=-1, n_neighbors=10)
    train time: 0.023s
    test time:  6.634s
    accuracy:   0.608
    classification report:
                  precision    recall  f1-score   support
    
            HSK5       0.59      0.80      0.68      3779
          NotHSK       0.64      0.39      0.49      3406
    
        accuracy                           0.61      7185
       macro avg       0.62      0.60      0.58      7185
    weighted avg       0.62      0.61      0.59      7185
    
    confusion matrix:
    [[3030  749]
     [2068 1338]]
    
    ================================================================================
    Random forest
    ________________________________________________________________________________
    Training: 
    RandomForestClassifier(class_weight={'HSK5': 1, 'NotHSK': 1}, n_jobs=-1)
    train time: 7.704s
    test time:  0.109s
    accuracy:   0.722
    classification report:
                  precision    recall  f1-score   support
    
            HSK5       0.68      0.90      0.77      3779
          NotHSK       0.82      0.53      0.64      3406
    
        accuracy                           0.72      7185
       macro avg       0.75      0.71      0.71      7185
    weighted avg       0.75      0.72      0.71      7185
    
    confusion matrix:
    [[3396  383]
     [1615 1791]]
    
    ================================================================================
    L2 penalty
    ________________________________________________________________________________
    Training: 
    LinearSVC(dual=False)
    train time: 16.814s
    test time:  0.002s
    accuracy:   0.762
    dimensionality: 50000
    density: 1.000000
    top 10 keywords per class:
    HSK5: 混音 夺 绑 邓丽君 吟唱 奢侈 冒充 編曲 晨光 又哭又笑
    Top 10 features not available
    classification report:
                  precision    recall  f1-score   support
    
            HSK5       0.77      0.79      0.78      3779
          NotHSK       0.76      0.74      0.75      3406
    
        accuracy                           0.76      7185
       macro avg       0.76      0.76      0.76      7185
    weighted avg       0.76      0.76      0.76      7185
    
    confusion matrix:
    [[2970  809]
     [ 901 2505]]
    
    ________________________________________________________________________________
    Training: 
    SGDClassifier()
    train time: 0.396s
    test time:  0.001s
    accuracy:   0.767
    dimensionality: 50000
    density: 0.937260
    top 10 keywords per class:
    HSK5: 虔诚 创作 夺 熬 盘旋 吟唱 恍惚 又哭又笑 晨光 奢侈
    Top 10 features not available
    classification report:
                  precision    recall  f1-score   support
    
            HSK5       0.78      0.78      0.78      3779
          NotHSK       0.75      0.75      0.75      3406
    
        accuracy                           0.77      7185
       macro avg       0.77      0.77      0.77      7185
    weighted avg       0.77      0.77      0.77      7185
    
    confusion matrix:
    [[2943  836]
     [ 837 2569]]
    
    ================================================================================
    L1 penalty
    ________________________________________________________________________________
    Training: 
    LinearSVC(dual=False, penalty='l1')
    train time: 15.816s
    test time:  0.003s
    accuracy:   0.766
    dimensionality: 50000
    density: 0.238220
    top 10 keywords per class:
    HSK5: 旧情绵绵 晾干 丁点 冉冉升起 邓丽君 浑浑噩噩 拈来 蟑螂 冒充 吟唱
    Top 10 features not available
    classification report:
                  precision    recall  f1-score   support
    
            HSK5       0.77      0.79      0.78      3779
          NotHSK       0.76      0.74      0.75      3406
    
        accuracy                           0.77      7185
       macro avg       0.77      0.76      0.76      7185
    weighted avg       0.77      0.77      0.77      7185
    
    confusion matrix:
    [[2970  809]
     [ 874 2532]]
    
    ________________________________________________________________________________
    Training: 
    SGDClassifier(penalty='l1')
    train time: 10.077s
    test time:  0.001s
    accuracy:   0.742
    dimensionality: 50000
    density: 0.209240
    top 10 keywords per class:
    HSK5: 冬枣 喺 狂想 拜年 摩 牙齿 阿弥陀佛 钻 拄 痛痛
    Top 10 features not available
    classification report:
                  precision    recall  f1-score   support
    
            HSK5       0.76      0.75      0.75      3779
          NotHSK       0.73      0.73      0.73      3406
    
        accuracy                           0.74      7185
       macro avg       0.74      0.74      0.74      7185
    weighted avg       0.74      0.74      0.74      7185
    
    confusion matrix:
    [[2845  934]
     [ 920 2486]]
    
    ================================================================================
    Elastic-Net penalty
    ________________________________________________________________________________
    Training: 
    SGDClassifier(penalty='elasticnet')
    train time: 0.752s
    test time:  0.001s
    accuracy:   0.776
    dimensionality: 50000
    density: 0.405080
    top 10 keywords per class:
    HSK5: 创作 若即若离 蟑螂 盘旋 观世音 恍惚 吟唱 又哭又笑 奢侈 晨光
    Top 10 features not available
    classification report:
                  precision    recall  f1-score   support
    
            HSK5       0.77      0.82      0.79      3779
          NotHSK       0.78      0.73      0.76      3406
    
        accuracy                           0.78      7185
       macro avg       0.78      0.77      0.77      7185
    weighted avg       0.78      0.78      0.78      7185
    
    confusion matrix:
    [[3080  699]
     [ 912 2494]]
    
    ================================================================================
    NearestCentroid (aka Rocchio classifier)
    ________________________________________________________________________________
    Training: 
    NearestCentroid()
    train time: 0.043s
    test time:  0.004s
    accuracy:   0.601
    classification report:
                  precision    recall  f1-score   support
    
            HSK5       0.68      0.46      0.55      3779
          NotHSK       0.56      0.76      0.64      3406
    
        accuracy                           0.60      7185
       macro avg       0.62      0.61      0.60      7185
    weighted avg       0.62      0.60      0.59      7185
    
    confusion matrix:
    [[1745 2034]
     [ 833 2573]]
    
    ================================================================================
    Naive Bayes
    ________________________________________________________________________________
    Training: 
    MultinomialNB(alpha=0.01)
    train time: 0.066s
    test time:  0.002s
    accuracy:   0.738
    dimensionality: 50000
    density: 1.000000
    top 10 keywords per class:
    HSK5: 梦 爱情 太 不要 没 走 心 世界 没有 爱
    Top 10 features not available
    classification report:
                  precision    recall  f1-score   support
    
            HSK5       0.74      0.76      0.75      3779
          NotHSK       0.73      0.71      0.72      3406
    
        accuracy                           0.74      7185
       macro avg       0.74      0.74      0.74      7185
    weighted avg       0.74      0.74      0.74      7185
    
    confusion matrix:
    [[2886  893]
     [ 992 2414]]
    
    ________________________________________________________________________________
    Training: 
    BernoulliNB(alpha=0.01)
    train time: 0.073s
    test time:  0.006s
    accuracy:   0.746
    dimensionality: 50000
    density: 1.000000
    top 10 keywords per class:
    HSK5: 吉他 梦 太 走 没 心 混音 世界 没有 爱
    Top 10 features not available
    classification report:
                  precision    recall  f1-score   support
    
            HSK5       0.74      0.80      0.77      3779
          NotHSK       0.75      0.69      0.72      3406
    
        accuracy                           0.75      7185
       macro avg       0.75      0.74      0.74      7185
    weighted avg       0.75      0.75      0.75      7185
    
    confusion matrix:
    [[3006  773]
     [1052 2354]]
    
    ================================================================================
    LinearSVC with L1-based feature selection
    ________________________________________________________________________________
    Training: 
    Pipeline(steps=[('feature_selection',
                     SelectFromModel(estimator=LinearSVC(dual=False,
                                                         penalty='l1'))),
                    ('classification', LinearSVC())])
    train time: 14.113s
    test time:  0.008s
    accuracy:   0.766
    classification report:
                  precision    recall  f1-score   support
    
            HSK5       0.77      0.78      0.78      3779
          NotHSK       0.76      0.75      0.75      3406
    
        accuracy                           0.77      7185
       macro avg       0.77      0.76      0.76      7185
    weighted avg       0.77      0.77      0.77      7185
    
    confusion matrix:
    [[2958  821]
     [ 862 2544]]
    



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_226_1.png)
    


#### 9.3.4. Neural Network modes: Multilayer Perceptron Feed-Forward network - Setup Run


```python
# Create HSK5 binary variable
df_h5 = df_tracks_master_clean3_lyrics_genre_hsk_v2.copy()
df_h5['is_h5'] = 'NotHSK'
df_h5['is_h5'][df_tracks_master_clean3_lyrics_genre_hsk_v2.hsk5 > 0.9] = 'HSK5'
X = df_h5[['lyrics','is_h5'
]]
y = X.pop('is_h5')
X = X['lyrics']
df_h5['is_h5'].value_counts()
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    stratify=y,
                                                    test_size=0.2,
                                                    random_state=1)
```


```python
from sklearn.neural_network import MLPClassifier

# Tokenize lyrics
cvec = CountVectorizer(tokenizer=man_token_jie, max_features=50000)
cvec.fit(X_train)

# Transform training data
cvec_mat = cvec.transform(X_train)
X_train = cvec_mat
X_test = cvec.transform(X_test)

model = MLPClassifier(solver='lbfgs',
                    alpha=10**(-10),
                    hidden_layer_sizes=1,
                    activation='identity',
                    random_state=1,
                    batch_size='auto')


model.fit(X_train, y_train)



Bold = '\033[1m'
Normal = '\033[0m'

skf = StratifiedKFold(n_splits=5)
skf.get_n_splits(X_train, y_train)

lr_scores = cross_val_score(model, X_train, y_train, cv=skf, n_jobs=6)

print(Bold + 'Base line score:' + Normal,
      np.round(y_train.value_counts(normalize=True)[0], 3))
print(Bold + 'Training Score:' + Normal, np.round(model.score(X_train, y_train),
                                                  3))
print(Bold + 'Test Score:' + Normal, np.round(model.score(X_test, y_test), 3))
print(Bold + "Cross-validated training scores:" + Normal,
      np.round(lr_scores, 4))
print(Bold + "Mean cross-validated training score:" + Normal,
      np.round(lr_scores.mean(), 3), '\n')

logist_plots(model,
             X_train=X_train,
             y_train=y_train,
             X_test=X_test,
             y_test=y_test)
```

    [1mBase line score:[0m 0.526
    [1mTraining Score:[0m 0.993
    [1mTest Score:[0m 0.78
    [1mCross-validated training scores:[0m [0.7773 0.7611 0.7734 0.7741 0.7637]
    [1mMean cross-validated training score:[0m 0.77 
    
    y_train classification report:
                  precision    recall  f1-score   support
    
            HSK5       0.99      0.99      0.99     15114
          NotHSK       0.99      0.99      0.99     13622
    
        accuracy                           0.99     28736
       macro avg       0.99      0.99      0.99     28736
    weighted avg       0.99      0.99      0.99     28736
    
    y_test classification report:
                  precision    recall  f1-score   support
    
            HSK5       0.79      0.79      0.79      3779
          NotHSK       0.77      0.77      0.77      3406
    
        accuracy                           0.78      7185
       macro avg       0.78      0.78      0.78      7185
    weighted avg       0.78      0.78      0.78      7185
    



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_229_1.png)
    


    ROC Curves:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_229_3.png)
    


    Precision Recall Curve:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_229_5.png)
    


#### 9.3.5. Neural Network modes: Multilayer Perceptron Feed-Forward network
Params:
- max_feat: 1000
- activation: 'relu', 
- hidden_layer: (20)
- max iterations: default (200)

Results:
- run time:(2mins)
- MSE: 69%


```python
# Create HSK5 binary variable
df_h5 = df_tracks_master_clean3_lyrics_genre_hsk_v2.copy()
df_h5['is_h5'] = 'NotHSK'
df_h5['is_h5'][df_tracks_master_clean3_lyrics_genre_hsk_v2.hsk5 > 0.9] = 'HSK5'
X = df_h5[['lyrics','is_h5'
]]
y = X.pop('is_h5')
X = X['lyrics']
df_h5['is_h5'].value_counts()
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    stratify=y,
                                                    test_size=0.2,
                                                    random_state=1)
```


```python
from sklearn.neural_network import MLPClassifier

# Tokenize lyrics
cvec = CountVectorizer(tokenizer=man_token_jie, max_features=1000)
cvec.fit(X_train)

# Transform training data
cvec_mat = cvec.transform(X_train)
X_train = cvec_mat
X_test = cvec.transform(X_test)

model = MLPClassifier(solver='adam',
                    alpha=10**(0),
                    hidden_layer_sizes=(20),
                    activation='relu',
                    random_state=1,
                    batch_size='auto',
                    warm_start=True, max_iter=200, verbose=True)


model.fit(X_train, y_train)

Bold = '\033[1m'
Normal = '\033[0m'

skf = StratifiedKFold(n_splits=5)
skf.get_n_splits(X_train, y_train)

lr_scores = cross_val_score(model, X_train, y_train, cv=skf, n_jobs=6)

print(Bold + 'Base line score:' + Normal,
      np.round(y_train.value_counts(normalize=True)[0], 3))
print(Bold + 'Training Score:' + Normal, np.round(model.score(X_train, y_train),
                                                  3))
print(Bold + 'Test Score:' + Normal, np.round(model.score(X_test, y_test), 3))
print(Bold + "Cross-validated training scores:" + Normal,
      np.round(lr_scores, 4))
print(Bold + "Mean cross-validated training score:" + Normal,
      np.round(lr_scores.mean(), 3), '\n')

logist_plots(model,
             X_train=X_train,
             y_train=y_train,
             X_test=X_test,
             y_test=y_test)
```

    Iteration 1, loss = 0.69617860
    Iteration 2, loss = 0.61250176
    Iteration 3, loss = 0.58969204
    Iteration 4, loss = 0.57750362
    Iteration 5, loss = 0.56917023
    Iteration 6, loss = 0.56223128
    Iteration 7, loss = 0.55697544
    Iteration 8, loss = 0.55101358
    Iteration 9, loss = 0.54668859
    Iteration 10, loss = 0.54150511
    Iteration 11, loss = 0.53778703
    Iteration 12, loss = 0.53414516
    Iteration 13, loss = 0.53110520
    Iteration 14, loss = 0.52694947
    Iteration 15, loss = 0.52392560
    Iteration 16, loss = 0.52222829
    Iteration 17, loss = 0.51775146
    Iteration 18, loss = 0.51417511
    Iteration 19, loss = 0.51364728
    Iteration 20, loss = 0.51057123
    Iteration 21, loss = 0.50816712
    Iteration 22, loss = 0.50586420
    Iteration 23, loss = 0.50457372
    Iteration 24, loss = 0.50422499
    Iteration 25, loss = 0.50382882
    Iteration 26, loss = 0.49920513
    Iteration 27, loss = 0.49772956
    Iteration 28, loss = 0.49676852
    Iteration 29, loss = 0.49543514
    Iteration 30, loss = 0.49255365
    Iteration 31, loss = 0.49105571
    Iteration 32, loss = 0.49120348
    Iteration 33, loss = 0.49034728
    Iteration 34, loss = 0.48884207
    Iteration 35, loss = 0.48773211
    Iteration 36, loss = 0.48685383
    Iteration 37, loss = 0.48741913
    Iteration 38, loss = 0.48653601
    Iteration 39, loss = 0.48344604
    Iteration 40, loss = 0.48414716
    Iteration 41, loss = 0.48336764
    Iteration 42, loss = 0.48229013
    Iteration 43, loss = 0.48283549
    Iteration 44, loss = 0.48146154
    Iteration 45, loss = 0.48122993
    Iteration 46, loss = 0.48108623
    Iteration 47, loss = 0.47949900
    Iteration 48, loss = 0.47951782
    Iteration 49, loss = 0.47854133
    Iteration 50, loss = 0.47856109
    Iteration 51, loss = 0.47719690
    Iteration 52, loss = 0.47825746
    Iteration 53, loss = 0.47683128
    Iteration 54, loss = 0.47713017
    Iteration 55, loss = 0.47577381
    Iteration 56, loss = 0.47560158
    Iteration 57, loss = 0.47544537
    Iteration 58, loss = 0.47591400
    Iteration 59, loss = 0.47547239
    Iteration 60, loss = 0.47361315
    Iteration 61, loss = 0.47239822
    Iteration 62, loss = 0.47223159
    Iteration 63, loss = 0.47419766
    Iteration 64, loss = 0.47206210
    Iteration 65, loss = 0.47329501
    Iteration 66, loss = 0.47246465
    Iteration 67, loss = 0.47237366
    Iteration 68, loss = 0.47201336
    Iteration 69, loss = 0.47024905
    Iteration 70, loss = 0.47107649
    Iteration 71, loss = 0.47086642
    Iteration 72, loss = 0.47059617
    Iteration 73, loss = 0.47142526
    Iteration 74, loss = 0.47107839
    Iteration 75, loss = 0.46981200
    Iteration 76, loss = 0.47016424
    Iteration 77, loss = 0.46890008
    Iteration 78, loss = 0.46895093
    Iteration 79, loss = 0.47077787
    Iteration 80, loss = 0.46793714
    Iteration 81, loss = 0.46774803
    Iteration 82, loss = 0.47069220
    Iteration 83, loss = 0.46842018
    Iteration 84, loss = 0.46788519
    Iteration 85, loss = 0.46791205
    Iteration 86, loss = 0.46733760
    Iteration 87, loss = 0.46718788
    Iteration 88, loss = 0.46818468
    Iteration 89, loss = 0.46732245
    Iteration 90, loss = 0.46786175
    Iteration 91, loss = 0.46890069
    Iteration 92, loss = 0.46801146
    Iteration 93, loss = 0.46867458
    Iteration 94, loss = 0.46680307
    Iteration 95, loss = 0.46611315
    Iteration 96, loss = 0.46668780
    Iteration 97, loss = 0.46529620
    Iteration 98, loss = 0.46595073
    Iteration 99, loss = 0.46680598
    Iteration 100, loss = 0.46713865
    Iteration 101, loss = 0.46714497
    Iteration 102, loss = 0.46586878
    Iteration 103, loss = 0.46460061
    Iteration 104, loss = 0.46610259
    Iteration 105, loss = 0.46441690
    Iteration 106, loss = 0.46429256
    Iteration 107, loss = 0.46461811
    Iteration 108, loss = 0.46467793
    Iteration 109, loss = 0.46655323
    Iteration 110, loss = 0.46429392
    Iteration 111, loss = 0.46398523
    Iteration 112, loss = 0.46443737
    Iteration 113, loss = 0.46520948
    Iteration 114, loss = 0.46454719
    Iteration 115, loss = 0.46364536
    Iteration 116, loss = 0.46386621
    Iteration 117, loss = 0.46629039
    Iteration 118, loss = 0.46403523
    Iteration 119, loss = 0.46442672
    Iteration 120, loss = 0.46445487
    Iteration 121, loss = 0.46417754
    Iteration 122, loss = 0.46394413
    Iteration 123, loss = 0.46313252
    Iteration 124, loss = 0.46333857
    Iteration 125, loss = 0.46326521
    Iteration 126, loss = 0.46313767
    Iteration 127, loss = 0.46311902
    Iteration 128, loss = 0.46216847
    Iteration 129, loss = 0.46243083
    Iteration 130, loss = 0.46243197
    Iteration 131, loss = 0.46209239
    Iteration 132, loss = 0.46375392
    Iteration 133, loss = 0.46274740
    Iteration 134, loss = 0.46216050
    Iteration 135, loss = 0.46165276
    Iteration 136, loss = 0.46237597
    Iteration 137, loss = 0.46245966
    Iteration 138, loss = 0.46398895
    Iteration 139, loss = 0.46281095
    Iteration 140, loss = 0.46110443
    Iteration 141, loss = 0.46427053
    Iteration 142, loss = 0.46178829
    Iteration 143, loss = 0.46299768
    Iteration 144, loss = 0.46220516
    Iteration 145, loss = 0.46247498
    Iteration 146, loss = 0.46430390
    Iteration 147, loss = 0.46122764
    Iteration 148, loss = 0.46173252
    Iteration 149, loss = 0.46267760
    Iteration 150, loss = 0.46300096
    Iteration 151, loss = 0.46174536
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [1mBase line score:[0m 0.526
    [1mTraining Score:[0m 0.9
    [1mTest Score:[0m 0.702
    [1mCross-validated training scores:[0m [0.7008 0.6802 0.6934 0.6852 0.6835]
    [1mMean cross-validated training score:[0m 0.689 
    
    y_train classification report:
                  precision    recall  f1-score   support
    
            HSK5       0.90      0.92      0.91     15114
          NotHSK       0.90      0.88      0.89     13622
    
        accuracy                           0.90     28736
       macro avg       0.90      0.90      0.90     28736
    weighted avg       0.90      0.90      0.90     28736
    
    y_test classification report:
                  precision    recall  f1-score   support
    
            HSK5       0.71      0.74      0.72      3779
          NotHSK       0.70      0.66      0.68      3406
    
        accuracy                           0.70      7185
       macro avg       0.70      0.70      0.70      7185
    weighted avg       0.70      0.70      0.70      7185
    



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_232_1.png)
    


    ROC Curves:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_232_3.png)
    


    Precision Recall Curve:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_232_5.png)
    


#### 9.3.6. Neural Network modes: Multilayer Perceptron Feed-Forward network - activation: 'relu', (500, 10)

Params:
- max_feat: 2000
- activation: 'relu', 
- hidden_layer: (100,100)
- max iterations: default (200)

Results:
- run time:(10mins)
- MSE: 72%


```python
# Create HSK5 binary variable
df_h5 = df_tracks_master_clean3_lyrics_genre_hsk_v2.copy()
df_h5['is_h5'] = 'NotHSK'
df_h5['is_h5'][df_tracks_master_clean3_lyrics_genre_hsk_v2.hsk5 > 0.9] = 'HSK5'
X = df_h5[['lyrics','is_h5'
]]
y = X.pop('is_h5')
X = X['lyrics']
df_h5['is_h5'].value_counts()
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    stratify=y,
                                                    test_size=0.2,
                                                    random_state=1)
```


```python
from sklearn.neural_network import MLPClassifier

# Tokenize lyrics
cvec = CountVectorizer(tokenizer=man_token_jie, max_features=2000)
cvec.fit(X_train)

# Transform training data
cvec_mat = cvec.transform(X_train)
X_train = cvec_mat
X_test = cvec.transform(X_test)

model = MLPClassifier(solver='adam',
                    alpha=10**(1),
                    hidden_layer_sizes=(100, 100),
                    activation='relu',
                    random_state=1,
                    batch_size='auto',verbose=True, warm_start=True)


model.fit(X_train, y_train)

Bold = '\033[1m'
Normal = '\033[0m'

skf = StratifiedKFold(n_splits=5)
skf.get_n_splits(X_train, y_train)

lr_scores = cross_val_score(model, X_train, y_train, cv=skf, n_jobs=6)

print(Bold + 'Base line score:' + Normal,
      np.round(y_train.value_counts(normalize=True)[0], 3))
print(Bold + 'Training Score:' + Normal, np.round(model.score(X_train, y_train),
                                                  3))
print(Bold + 'Test Score:' + Normal, np.round(model.score(X_test, y_test), 3))
print(Bold + "Cross-validated training scores:" + Normal,
      np.round(lr_scores, 4))
print(Bold + "Mean cross-validated training score:" + Normal,
      np.round(lr_scores.mean(), 3), '\n')

logist_plots(model,
             X_train=X_train,
             y_train=y_train,
             X_test=X_test,
             y_test=y_test)
```

    Iteration 1, loss = 2.15894925
    Iteration 2, loss = 0.72569286
    Iteration 3, loss = 0.67439789
    Iteration 4, loss = 0.67025631
    Iteration 5, loss = 0.66765934
    Iteration 6, loss = 0.66628304
    Iteration 7, loss = 0.66526631
    Iteration 8, loss = 0.66306601
    Iteration 9, loss = 0.66301215
    Iteration 10, loss = 0.66215384
    Iteration 11, loss = 0.66117766
    Iteration 12, loss = 0.66081834
    Iteration 13, loss = 0.65955271
    Iteration 14, loss = 0.65927768
    Iteration 15, loss = 0.65890505
    Iteration 16, loss = 0.65868304
    Iteration 17, loss = 0.65739426
    Iteration 18, loss = 0.65758669
    Iteration 19, loss = 0.65715959
    Iteration 20, loss = 0.65696541
    Iteration 21, loss = 0.65758157
    Iteration 22, loss = 0.65548609
    Iteration 23, loss = 0.65700433
    Iteration 24, loss = 0.65543409
    Iteration 25, loss = 0.65613741
    Iteration 26, loss = 0.65512922
    Iteration 27, loss = 0.65534525
    Iteration 28, loss = 0.65516405
    Iteration 29, loss = 0.65525887
    Iteration 30, loss = 0.65460455
    Iteration 31, loss = 0.65460151
    Iteration 32, loss = 0.65492692
    Iteration 33, loss = 0.65418087
    Iteration 34, loss = 0.65484947
    Iteration 35, loss = 0.65439765
    Iteration 36, loss = 0.65537167
    Iteration 37, loss = 0.65420369
    Iteration 38, loss = 0.65484232
    Iteration 39, loss = 0.65425649
    Iteration 40, loss = 0.65520648
    Iteration 41, loss = 0.65369370
    Iteration 42, loss = 0.65391368
    Iteration 43, loss = 0.65423260
    Iteration 44, loss = 0.65290641
    Iteration 45, loss = 0.65459258
    Iteration 46, loss = 0.65409459
    Iteration 47, loss = 0.65413286
    Iteration 48, loss = 0.65399727
    Iteration 49, loss = 0.65408800
    Iteration 50, loss = 0.65370602
    Iteration 51, loss = 0.65342206
    Iteration 52, loss = 0.65338633
    Iteration 53, loss = 0.65399037
    Iteration 54, loss = 0.65411577
    Iteration 55, loss = 0.65249805
    Iteration 56, loss = 0.65286060
    Iteration 57, loss = 0.65269708
    Iteration 58, loss = 0.65377345
    Iteration 59, loss = 0.65322258
    Iteration 60, loss = 0.65372307
    Iteration 61, loss = 0.65324849
    Iteration 62, loss = 0.65357249
    Iteration 63, loss = 0.65323320
    Iteration 64, loss = 0.65247091
    Iteration 65, loss = 0.65304508
    Iteration 66, loss = 0.65317550
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [1mBase line score:[0m 0.526
    [1mTraining Score:[0m 0.723
    [1mTest Score:[0m 0.704
    [1mCross-validated training scores:[0m [0.7121 0.6988 0.7052 0.7113 0.6896]
    [1mMean cross-validated training score:[0m 0.703 
    
    y_train classification report:
                  precision    recall  f1-score   support
    
            HSK5       0.72      0.77      0.75     15114
          NotHSK       0.73      0.67      0.70     13622
    
        accuracy                           0.72     28736
       macro avg       0.72      0.72      0.72     28736
    weighted avg       0.72      0.72      0.72     28736
    
    y_test classification report:
                  precision    recall  f1-score   support
    
            HSK5       0.70      0.75      0.73      3779
          NotHSK       0.70      0.65      0.68      3406
    
        accuracy                           0.70      7185
       macro avg       0.70      0.70      0.70      7185
    weighted avg       0.70      0.70      0.70      7185
    



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_235_1.png)
    


    ROC Curves:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_235_3.png)
    


    Precision Recall Curve:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_235_5.png)
    


#### 9.3.6. Neural Network modes: Multilayer Perceptron Feed-Forward network - activation: 'relu', Grid Search
Params:
- max_feat: 5000
- activation: 'relu', 
- hidden_layer: [(20, 20, 20, 20, 20), (10,120,10)]
- alpha: [10^(0), 10^(0.1)],
- max iterations: default (400)

Results:
- run time: 4 mins
- MSE: 72%


```python
# Create HSK5 binary variable
df_h5 = df_tracks_master_clean3_lyrics_genre_hsk_v2.copy()
df_h5['is_h5'] = 'NotHSK'
df_h5['is_h5'][df_tracks_master_clean3_lyrics_genre_hsk_v2.hsk5 > 0.9] = 'HSK5'
X = df_h5[['lyrics','is_h5'
]]
y = X.pop('is_h5')
X = X['lyrics']
df_h5['is_h5'].value_counts()
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    stratify=y,
                                                    test_size=0.2,
                                                    random_state=1)
```


```python
from sklearn.model_selection import GridSearchCV

from sklearn.neural_network import MLPClassifier

# Tokenize lyrics
cvec = CountVectorizer(tokenizer=man_token_jie, max_features=5000)
cvec.fit(X_train)

# Transform training data
cvec_mat = cvec.transform(X_train)
X_train = cvec_mat
X_test = cvec.transform(X_test)
```


```python
params = {'solver':['adam'],
                'alpha':[10**(0), 10**(0.1)],
                'hidden_layer_sizes':[(20, 20, 20, 20, 20), (10,120,10)],
                'activation':['relu'],
                'random_state':[42],
                'batch_size':['auto'],
                'max_iter':[400], 'verbose':[True]}


skf = StratifiedKFold(n_splits=3)
skf.get_n_splits(X_train, y_train)



gs = GridSearchCV(MLPClassifier(), param_grid=params, cv=skf, n_jobs=-1, verbose=2)
gs.fit(X_train, y_train)

model = gs.best_estimator_
print(gs.best_estimator_)
print('R2 training: ', gs.score(X_train, y_train))
print('R2 CV training: ', gs.best_score_)
print('R2 test: ', gs.score(X_test, y_test))
```

    Fitting 3 folds for each of 4 candidates, totalling 12 fits
    Iteration 1, loss = 0.81766635
    Iteration 2, loss = 0.61479181
    Iteration 3, loss = 0.55495943
    Iteration 4, loss = 0.52425254
    Iteration 5, loss = 0.50114541
    Iteration 6, loss = 0.47969645
    Iteration 7, loss = 0.45589368
    Iteration 8, loss = 0.43812543
    Iteration 9, loss = 0.41655007
    Iteration 10, loss = 0.39912382
    Iteration 11, loss = 0.38161417
    Iteration 12, loss = 0.37046972
    Iteration 13, loss = 0.35446057
    Iteration 14, loss = 0.36063909
    Iteration 15, loss = 0.34765092
    Iteration 16, loss = 0.32916001
    Iteration 17, loss = 0.32551016
    Iteration 18, loss = 0.33178346
    Iteration 19, loss = 0.31650513
    Iteration 20, loss = 0.31376270
    Iteration 21, loss = 0.31281944
    Iteration 22, loss = 0.30750374
    Iteration 23, loss = 0.31543963
    Iteration 24, loss = 0.30186886
    Iteration 25, loss = 0.29964405
    Iteration 26, loss = 0.30514396
    Iteration 27, loss = 0.30066262
    Iteration 28, loss = 0.30179357
    Iteration 29, loss = 0.30319338
    Iteration 30, loss = 0.29548641
    Iteration 31, loss = 0.29091095
    Iteration 32, loss = 0.29726855
    Iteration 33, loss = 0.28314987
    Iteration 34, loss = 0.29013971
    Iteration 35, loss = 0.30097491
    Iteration 36, loss = 0.29551127
    Iteration 37, loss = 0.29122257
    Iteration 38, loss = 0.28999482
    Iteration 39, loss = 0.28377569
    Iteration 40, loss = 0.28060142
    Iteration 41, loss = 0.28880160
    Iteration 42, loss = 0.28567366
    Iteration 43, loss = 0.28385627
    Iteration 44, loss = 0.29110451
    Iteration 45, loss = 0.27921728
    Iteration 46, loss = 0.28253165
    Iteration 47, loss = 0.29123817
    Iteration 48, loss = 0.28654443
    Iteration 49, loss = 0.27792291
    Iteration 50, loss = 0.28285085
    Iteration 51, loss = 0.27544891
    Iteration 52, loss = 0.28326053
    Iteration 53, loss = 0.28529523
    Iteration 54, loss = 0.28743644
    Iteration 55, loss = 0.28242960
    Iteration 56, loss = 0.27710606
    Iteration 57, loss = 0.27419998
    Iteration 58, loss = 0.28177904
    Iteration 59, loss = 0.28403403
    Iteration 60, loss = 0.28186669
    Iteration 61, loss = 0.28019514
    Iteration 62, loss = 0.27011391
    Iteration 63, loss = 0.27526719
    Iteration 64, loss = 0.27685961
    Iteration 65, loss = 0.27670996
    Iteration 66, loss = 0.28250778
    Iteration 67, loss = 0.28026423
    Iteration 68, loss = 0.26782584
    Iteration 69, loss = 0.27521166
    Iteration 70, loss = 0.28152851
    Iteration 71, loss = 0.27011184
    Iteration 72, loss = 0.27864494
    Iteration 73, loss = 0.26818840
    Iteration 74, loss = 0.26620183
    Iteration 75, loss = 0.27317987
    Iteration 76, loss = 0.28725282
    Iteration 77, loss = 0.28339606
    Iteration 78, loss = 0.27504576
    Iteration 79, loss = 0.27779072
    Iteration 80, loss = 0.27578261
    Iteration 81, loss = 0.26152054
    Iteration 82, loss = 0.26282559
    Iteration 83, loss = 0.28449469
    Iteration 84, loss = 0.27905185
    Iteration 85, loss = 0.27124332
    Iteration 86, loss = 0.27594186
    Iteration 87, loss = 0.26475477
    Iteration 88, loss = 0.26722334
    Iteration 89, loss = 0.27335305
    Iteration 90, loss = 0.26578801
    Iteration 91, loss = 0.26091147
    Iteration 92, loss = 0.26568700
    Iteration 93, loss = 0.27766936
    Iteration 94, loss = 0.27741597
    Iteration 95, loss = 0.26348882
    Iteration 96, loss = 0.26429329
    Iteration 97, loss = 0.27139402
    Iteration 98, loss = 0.27429586
    Iteration 99, loss = 0.26448117
    Iteration 100, loss = 0.26499574
    Iteration 101, loss = 0.26811270
    Iteration 102, loss = 0.26181852
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    MLPClassifier(alpha=1.2589254117941673, hidden_layer_sizes=(20, 20, 20, 20, 20),
                  max_iter=400, random_state=42, verbose=True)
    R2 training:  0.9879245545657016
    R2 CV training:  0.7210466100669319
    R2 test:  0.7333333333333333


#### 9.3.6. Neural Network modes: Multilayer Perceptron Feed-Forward network - activation: 'relu', Grid Search
- max_feat: 5000
- activation: 'relu', 
- hidden_layer: [(20, 20, 20, 20), (40,40,40,40)]
- alpha: [10^(1), 10^(2)],
- max iterations: default (400)

Results:
- run time: 4 mins
- MSE: 74.8%


```python
params = {'solver':['adam'],
                'alpha':[10, 10**(1),10**(2)],
                'hidden_layer_sizes':[(20, 20, 20, 20), (40,40,40,40)],
                'activation':['relu'],
                'random_state':[42],
                'batch_size':['auto'],
                'max_iter':[400], 'verbose':[True],'warm_start':[False]}


skf = StratifiedKFold(n_splits=3)
skf.get_n_splits(X_train, y_train)



gs = GridSearchCV(MLPClassifier(), param_grid=params, cv=skf, n_jobs=-1, verbose=2)
gs.fit(X_train, y_train)

model = gs.best_estimator_
print(gs.best_estimator_)
print('R2 training: ', gs.score(X_train, y_train))
print('R2 CV training: ', gs.best_score_)
print('R2 test: ', gs.score(X_test, y_test))
```

    Fitting 3 folds for each of 6 candidates, totalling 18 fits
    Iteration 1, loss = 1.69985123
    Iteration 2, loss = 0.98329225
    Iteration 3, loss = 0.79008771
    Iteration 4, loss = 0.73795442
    Iteration 5, loss = 0.72236998
    Iteration 6, loss = 0.71760245
    Iteration 7, loss = 0.71492348
    Iteration 8, loss = 0.71382409
    Iteration 9, loss = 0.71161659
    Iteration 10, loss = 0.71085495
    Iteration 11, loss = 0.71029281
    Iteration 12, loss = 0.70855166
    Iteration 13, loss = 0.70918418
    Iteration 14, loss = 0.70868875
    Iteration 15, loss = 0.70854968
    Iteration 16, loss = 0.70759618
    Iteration 17, loss = 0.70707795
    Iteration 18, loss = 0.70752291
    Iteration 19, loss = 0.70739690
    Iteration 20, loss = 0.70670531
    Iteration 21, loss = 0.70599467
    Iteration 22, loss = 0.70653734
    Iteration 23, loss = 0.70610944
    Iteration 24, loss = 0.70608667
    Iteration 25, loss = 0.70616153
    Iteration 26, loss = 0.70644156
    Iteration 27, loss = 0.70485813
    Iteration 28, loss = 0.70522708
    Iteration 29, loss = 0.70470553
    Iteration 30, loss = 0.70474695
    Iteration 31, loss = 0.70510941
    Iteration 32, loss = 0.70457465
    Iteration 33, loss = 0.70401859
    Iteration 34, loss = 0.70364843
    Iteration 35, loss = 0.70377522
    Iteration 36, loss = 0.70414468
    Iteration 37, loss = 0.70396794
    Iteration 38, loss = 0.70277868
    Iteration 39, loss = 0.70321831
    Iteration 40, loss = 0.70282491
    Iteration 41, loss = 0.70344673
    Iteration 42, loss = 0.70266360
    Iteration 43, loss = 0.70333606
    Iteration 44, loss = 0.70216746
    Iteration 45, loss = 0.70216052
    Iteration 46, loss = 0.70203723
    Iteration 47, loss = 0.70187208
    Iteration 48, loss = 0.70196799
    Iteration 49, loss = 0.70074838
    Iteration 50, loss = 0.70105717
    Iteration 51, loss = 0.70095847
    Iteration 52, loss = 0.70008153
    Iteration 53, loss = 0.70075770
    Iteration 54, loss = 0.70071151
    Iteration 55, loss = 0.70052114
    Iteration 56, loss = 0.70007405
    Iteration 57, loss = 0.69909532
    Iteration 58, loss = 0.69928361
    Iteration 59, loss = 0.69955050
    Iteration 60, loss = 0.69966746
    Iteration 61, loss = 0.69947596
    Iteration 62, loss = 0.69962815
    Iteration 63, loss = 0.69784232
    Iteration 64, loss = 0.69916683
    Iteration 65, loss = 0.69912373
    Iteration 66, loss = 0.69840423
    Iteration 67, loss = 0.69969542
    Iteration 68, loss = 0.69750870
    Iteration 69, loss = 0.69940932
    Iteration 70, loss = 0.69854693
    Iteration 71, loss = 0.69798534
    Iteration 72, loss = 0.69774516
    Iteration 73, loss = 0.69993075
    Iteration 74, loss = 0.69739086
    Iteration 75, loss = 0.69758378
    Iteration 76, loss = 0.69783528
    Iteration 77, loss = 0.69745317
    Iteration 78, loss = 0.69825381
    Iteration 79, loss = 0.69723627
    Iteration 80, loss = 0.69726306
    Iteration 81, loss = 0.69773822
    Iteration 82, loss = 0.69861629
    Iteration 83, loss = 0.69747403
    Iteration 84, loss = 0.69771240
    Iteration 85, loss = 0.69685664
    Iteration 86, loss = 0.69802434
    Iteration 87, loss = 0.69695103
    Iteration 88, loss = 0.69637646
    Iteration 89, loss = 0.69692599
    Iteration 90, loss = 0.69674559
    Iteration 91, loss = 0.69636453
    Iteration 92, loss = 0.69701952
    Iteration 93, loss = 0.69712654
    Iteration 94, loss = 0.69625251
    Iteration 95, loss = 0.69655975
    Iteration 96, loss = 0.69634591
    Iteration 97, loss = 0.69618461
    Iteration 98, loss = 0.69565186
    Iteration 99, loss = 0.69593249
    Iteration 100, loss = 0.69583045
    Iteration 101, loss = 0.69525039
    Iteration 102, loss = 0.69698461
    Iteration 103, loss = 0.69515718
    Iteration 104, loss = 0.69533180
    Iteration 105, loss = 0.69574911
    Iteration 106, loss = 0.69494078
    Iteration 107, loss = 0.69565556
    Iteration 108, loss = 0.69501925
    Iteration 109, loss = 0.69494129
    Iteration 110, loss = 0.69505672
    Iteration 111, loss = 0.69529977
    Iteration 112, loss = 0.69587686
    Iteration 113, loss = 0.69457395
    Iteration 114, loss = 0.69415116
    Iteration 115, loss = 0.69499870
    Iteration 116, loss = 0.69386673
    Iteration 117, loss = 0.69524402
    Iteration 118, loss = 0.69510438
    Iteration 119, loss = 0.69395416
    Iteration 120, loss = 0.69458735
    Iteration 121, loss = 0.69361478
    Iteration 122, loss = 0.69360560
    Iteration 123, loss = 0.69435152
    Iteration 124, loss = 0.69434216
    Iteration 125, loss = 0.69370324
    Iteration 126, loss = 0.69356723
    Iteration 127, loss = 0.69427925
    Iteration 128, loss = 0.69306479
    Iteration 129, loss = 0.69476255
    Iteration 130, loss = 0.69303359
    Iteration 131, loss = 0.69338150
    Iteration 132, loss = 0.69207360
    Iteration 133, loss = 0.69330133
    Iteration 134, loss = 0.69367252
    Iteration 135, loss = 0.69387676
    Iteration 136, loss = 0.69305109
    Iteration 137, loss = 0.69237339
    Iteration 138, loss = 0.69255344
    Iteration 139, loss = 0.69327489
    Iteration 140, loss = 0.69283094
    Iteration 141, loss = 0.69254301
    Iteration 142, loss = 0.69262622
    Iteration 143, loss = 0.69187231
    Iteration 144, loss = 0.69298653
    Iteration 145, loss = 0.69227693
    Iteration 146, loss = 0.69167850
    Iteration 147, loss = 0.69204897
    Iteration 148, loss = 0.69197632
    Iteration 149, loss = 0.69189388
    Iteration 150, loss = 0.69232591
    Iteration 151, loss = 0.69184117
    Iteration 152, loss = 0.69265486
    Iteration 153, loss = 0.69277523
    Iteration 154, loss = 0.69137772
    Iteration 155, loss = 0.69324615
    Iteration 156, loss = 0.69115673
    Iteration 157, loss = 0.69143776
    Iteration 158, loss = 0.69204652
    Iteration 159, loss = 0.69123977
    Iteration 160, loss = 0.69162886
    Iteration 161, loss = 0.69186152
    Iteration 162, loss = 0.69047883
    Iteration 163, loss = 0.69051532
    Iteration 164, loss = 0.69098397
    Iteration 165, loss = 0.69132311
    Iteration 166, loss = 0.69109915
    Iteration 167, loss = 0.69096251
    Iteration 168, loss = 0.69023885
    Iteration 169, loss = 0.69130694
    Iteration 170, loss = 0.69110444
    Iteration 171, loss = 0.69150644
    Iteration 172, loss = 0.69049336
    Iteration 173, loss = 0.69014699
    Iteration 174, loss = 0.69102743
    Iteration 175, loss = 0.69098132
    Iteration 176, loss = 0.69047498
    Iteration 177, loss = 0.69110922
    Iteration 178, loss = 0.69079139
    Iteration 179, loss = 0.69026366
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    MLPClassifier(alpha=10, hidden_layer_sizes=(20, 20, 20, 20), max_iter=400,
                  random_state=42, verbose=True)
    R2 training:  0.7865047327394209
    R2 CV training:  0.7482248757466975
    R2 test:  0.7430758524704245
    Iteration 1, loss = 0.79153517
    Iteration 2, loss = 0.60306742
    Iteration 3, loss = 0.51678580
    Iteration 4, loss = 0.46232460
    Iteration 5, loss = 0.42327147
    Iteration 6, loss = 0.38374648
    Iteration 7, loss = 0.34841063
    Iteration 8, loss = 0.32406050
    Iteration 9, loss = 0.29790926
    Iteration 10, loss = 0.29037273
    Iteration 11, loss = 0.27328321
    Iteration 12, loss = 0.26688973
    Iteration 13, loss = 0.25487655
    Iteration 14, loss = 0.25060987
    Iteration 15, loss = 0.24453818
    Iteration 16, loss = 0.24577087
    Iteration 17, loss = 0.24295813
    Iteration 18, loss = 0.23546207
    Iteration 19, loss = 0.23438653
    Iteration 20, loss = 0.22722336
    Iteration 21, loss = 0.22528850
    Iteration 22, loss = 0.22361359
    Iteration 23, loss = 0.21679218
    Iteration 24, loss = 0.21699120
    Iteration 25, loss = 0.22542419
    Iteration 26, loss = 0.22842155
    Iteration 27, loss = 0.22810901
    Iteration 28, loss = 0.21219777
    Iteration 29, loss = 0.19897869
    Iteration 30, loss = 0.20291833
    Iteration 31, loss = 0.21069158
    Iteration 32, loss = 0.22164509
    Iteration 33, loss = 0.22082501
    Iteration 34, loss = 0.21176922
    Iteration 35, loss = 0.21296431
    Iteration 36, loss = 0.20305982
    Iteration 37, loss = 0.19731844
    Iteration 38, loss = 0.19219659
    Iteration 39, loss = 0.20411100
    Iteration 40, loss = 0.21717420
    Iteration 41, loss = 0.21742678
    Iteration 42, loss = 0.21574564
    Iteration 43, loss = 0.20326854
    Iteration 44, loss = 0.20174343
    Iteration 45, loss = 0.19230218
    Iteration 46, loss = 0.18225829
    Iteration 47, loss = 0.17921829
    Iteration 48, loss = 0.18853708
    Iteration 49, loss = 0.22110826
    Iteration 50, loss = 0.23162561
    Iteration 51, loss = 0.21229630
    Iteration 52, loss = 0.19148107
    Iteration 53, loss = 0.18336608
    Iteration 54, loss = 0.17816919
    Iteration 55, loss = 0.17620238
    Iteration 56, loss = 0.18379364
    Iteration 57, loss = 0.20774181
    Iteration 58, loss = 0.23211914
    Iteration 59, loss = 0.22261111
    Iteration 60, loss = 0.20142908
    Iteration 61, loss = 0.18732270
    Iteration 62, loss = 0.17588635
    Iteration 63, loss = 0.16598150
    Iteration 64, loss = 0.15748797
    Iteration 65, loss = 0.15648405
    Iteration 66, loss = 0.18947025
    Iteration 67, loss = 0.24598652
    Iteration 68, loss = 0.23414886
    Iteration 69, loss = 0.20941053
    Iteration 70, loss = 0.19564674
    Iteration 71, loss = 0.18626574
    Iteration 72, loss = 0.17695893
    Iteration 73, loss = 0.17918500
    Iteration 74, loss = 0.18942100
    Iteration 75, loss = 0.20617441
    Iteration 76, loss = 0.21155304
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=1, batch_size=auto, hidden_layer_sizes=(20, 20, 20, 20), max_iter=400, random_state=42, solver=adam, verbose=True, warm_start=False; total time= 2.2min
    Iteration 1, loss = 12.66025032
    Iteration 2, loss = 5.64639954
    Iteration 3, loss = 2.80135747
    Iteration 4, loss = 1.52321096
    Iteration 5, loss = 0.99320248
    Iteration 6, loss = 0.79248376
    Iteration 7, loss = 0.72315974
    Iteration 8, loss = 0.70110688
    Iteration 9, loss = 0.69452188
    Iteration 10, loss = 0.69260962
    Iteration 11, loss = 0.69206373
    Iteration 12, loss = 0.69190083
    Iteration 13, loss = 0.69184247
    Iteration 14, loss = 0.69182544
    Iteration 15, loss = 0.69182691
    Iteration 16, loss = 0.69182190
    Iteration 17, loss = 0.69184139
    Iteration 18, loss = 0.69183033
    Iteration 19, loss = 0.69182046
    Iteration 20, loss = 0.69181581
    Iteration 21, loss = 0.69182416
    Iteration 22, loss = 0.69182318
    Iteration 23, loss = 0.69184064
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=100, batch_size=auto, hidden_layer_sizes=(20, 20, 20, 20), max_iter=400, random_state=42, solver=adam, verbose=True, warm_start=False; total time=  41.2s
    Iteration 1, loss = 0.89438967
    Iteration 2, loss = 0.64671518
    Iteration 3, loss = 0.54534548
    Iteration 4, loss = 0.47127805
    Iteration 5, loss = 0.40594966
    Iteration 6, loss = 0.35944871
    Iteration 7, loss = 0.32130265
    Iteration 8, loss = 0.29885495
    Iteration 9, loss = 0.29876019
    Iteration 10, loss = 0.28726688
    Iteration 11, loss = 0.27451230
    Iteration 12, loss = 0.26148554
    Iteration 13, loss = 0.25146080
    Iteration 14, loss = 0.24864995
    Iteration 15, loss = 0.24023416
    Iteration 16, loss = 0.23736126
    Iteration 17, loss = 0.23972638
    Iteration 18, loss = 0.24655187
    Iteration 19, loss = 0.24357090
    Iteration 20, loss = 0.24147235
    Iteration 21, loss = 0.23554039
    Iteration 22, loss = 0.21986789
    Iteration 23, loss = 0.21344630
    Iteration 24, loss = 0.20893454
    Iteration 25, loss = 0.21426368
    Iteration 26, loss = 0.22231766
    Iteration 27, loss = 0.22377042
    Iteration 28, loss = 0.22023398
    Iteration 29, loss = 0.21948621
    Iteration 30, loss = 0.22452396
    Iteration 31, loss = 0.21999034
    Iteration 32, loss = 0.21261147
    Iteration 33, loss = 0.20580825
    Iteration 34, loss = 0.19551995
    Iteration 35, loss = 0.19360989
    Iteration 36, loss = 0.18887756
    Iteration 37, loss = 0.20217040
    Iteration 38, loss = 0.23836075
    Iteration 39, loss = 0.24047088
    Iteration 40, loss = 0.22114505
    Iteration 41, loss = 0.19904663
    Iteration 42, loss = 0.18388292
    Iteration 43, loss = 0.17301541
    Iteration 44, loss = 0.17205589
    Iteration 45, loss = 0.20499722
    Iteration 46, loss = 0.24243600
    Iteration 47, loss = 0.23791152
    Iteration 48, loss = 0.21749619
    Iteration 49, loss = 0.19845680
    Iteration 50, loss = 0.18793169
    Iteration 51, loss = 0.17945951
    Iteration 52, loss = 0.17659086
    Iteration 53, loss = 0.17758597
    Iteration 54, loss = 0.20484901
    Iteration 55, loss = 0.22742974
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=1, batch_size=auto, hidden_layer_sizes=(40, 40, 40, 40), max_iter=400, random_state=42, solver=adam, verbose=True, warm_start=False; total time= 3.0min
    Iteration 1, loss = 0.89266253
    Iteration 2, loss = 0.64481049
    Iteration 3, loss = 0.55024412
    Iteration 4, loss = 0.48178527
    Iteration 5, loss = 0.42033316
    Iteration 6, loss = 0.37542410
    Iteration 7, loss = 0.33528807
    Iteration 8, loss = 0.30399806
    Iteration 9, loss = 0.30115291
    Iteration 10, loss = 0.28745701
    Iteration 11, loss = 0.26948911
    Iteration 12, loss = 0.26639345
    Iteration 13, loss = 0.26242963
    Iteration 14, loss = 0.25903243
    Iteration 15, loss = 0.25085852
    Iteration 16, loss = 0.24704070
    Iteration 17, loss = 0.24261219
    Iteration 18, loss = 0.23943984
    Iteration 19, loss = 0.22928603
    Iteration 20, loss = 0.22507837
    Iteration 21, loss = 0.23259382
    Iteration 22, loss = 0.23563610
    Iteration 23, loss = 0.22553223
    Iteration 24, loss = 0.21637650
    Iteration 25, loss = 0.22379679
    Iteration 26, loss = 0.21763763
    Iteration 27, loss = 0.21366752
    Iteration 28, loss = 0.20954957
    Iteration 29, loss = 0.20686871
    Iteration 30, loss = 0.21162118
    Iteration 31, loss = 0.22924665
    Iteration 32, loss = 0.22818198
    Iteration 33, loss = 0.21760354
    Iteration 34, loss = 0.20681157
    Iteration 35, loss = 0.20202683
    Iteration 36, loss = 0.19516574
    Iteration 37, loss = 0.19735503
    Iteration 38, loss = 0.21055507
    Iteration 39, loss = 0.21401994
    Iteration 40, loss = 0.20898519
    Iteration 41, loss = 0.20318645
    Iteration 42, loss = 0.20243130
    Iteration 43, loss = 0.21305721
    Iteration 44, loss = 0.21218858
    Iteration 45, loss = 0.20431788
    Iteration 46, loss = 0.19780987
    Iteration 47, loss = 0.19335744
    Iteration 48, loss = 0.20587825
    Iteration 49, loss = 0.20387951
    Iteration 50, loss = 0.20112967
    Iteration 51, loss = 0.20325498
    Iteration 52, loss = 0.20546505
    Iteration 53, loss = 0.20348579
    Iteration 54, loss = 0.19641233
    Iteration 55, loss = 0.19718640
    Iteration 56, loss = 0.19671241
    Iteration 57, loss = 0.19543996
    Iteration 58, loss = 0.20242045
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=1, batch_size=auto, hidden_layer_sizes=(40, 40, 40, 40), max_iter=400, random_state=42, solver=adam, verbose=True, warm_start=False; total time= 3.1min
    Iteration 1, loss = 0.78571806
    Iteration 2, loss = 0.59357042
    Iteration 3, loss = 0.50691701
    Iteration 4, loss = 0.46062754
    Iteration 5, loss = 0.41897033
    Iteration 6, loss = 0.38105408
    Iteration 7, loss = 0.35283508
    Iteration 8, loss = 0.32241627
    Iteration 9, loss = 0.29883561
    Iteration 10, loss = 0.28417690
    Iteration 11, loss = 0.27788546
    Iteration 12, loss = 0.26589567
    Iteration 13, loss = 0.26236473
    Iteration 14, loss = 0.24415089
    Iteration 15, loss = 0.23470661
    Iteration 16, loss = 0.23405314
    Iteration 17, loss = 0.22852970
    Iteration 18, loss = 0.23896461
    Iteration 19, loss = 0.23346072
    Iteration 20, loss = 0.23069185
    Iteration 21, loss = 0.21546897
    Iteration 22, loss = 0.21164630
    Iteration 23, loss = 0.20864434
    Iteration 24, loss = 0.21719902
    Iteration 25, loss = 0.23386596
    Iteration 26, loss = 0.21969519
    Iteration 27, loss = 0.21307418
    Iteration 28, loss = 0.20442693
    Iteration 29, loss = 0.19690711
    Iteration 30, loss = 0.19477141
    Iteration 31, loss = 0.20597405
    Iteration 32, loss = 0.22076173
    Iteration 33, loss = 0.22490772
    Iteration 34, loss = 0.21264662
    Iteration 35, loss = 0.19840277
    Iteration 36, loss = 0.19275941
    Iteration 37, loss = 0.19846070
    Iteration 38, loss = 0.20657181
    Iteration 39, loss = 0.21957625
    Iteration 40, loss = 0.21113893
    Iteration 41, loss = 0.20369765
    Iteration 42, loss = 0.19615818
    Iteration 43, loss = 0.18947420
    Iteration 44, loss = 0.18257901
    Iteration 45, loss = 0.18368724
    Iteration 46, loss = 0.18837165
    Iteration 47, loss = 0.21901389
    Iteration 48, loss = 0.24417295
    Iteration 49, loss = 0.21939345
    Iteration 50, loss = 0.19196633
    Iteration 51, loss = 0.17564931
    Iteration 52, loss = 0.16365400
    Iteration 53, loss = 0.15720570
    Iteration 54, loss = 0.15089461
    Iteration 55, loss = 0.15253418
    Iteration 56, loss = 0.21570309
    Iteration 57, loss = 0.25596485
    Iteration 58, loss = 0.23439883
    Iteration 59, loss = 0.21030874
    Iteration 60, loss = 0.18922368
    Iteration 61, loss = 0.17187292
    Iteration 62, loss = 0.15862729
    Iteration 63, loss = 0.15529111
    Iteration 64, loss = 0.15321075
    Iteration 65, loss = 0.18437378
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=1, batch_size=auto, hidden_layer_sizes=(20, 20, 20, 20), max_iter=400, random_state=42, solver=adam, verbose=True, warm_start=False; total time= 1.9min
    Iteration 1, loss = 12.66367765
    Iteration 2, loss = 5.64740631
    Iteration 3, loss = 2.80172359
    Iteration 4, loss = 1.52354678
    Iteration 5, loss = 0.99342718
    Iteration 6, loss = 0.79263369
    Iteration 7, loss = 0.72321277
    Iteration 8, loss = 0.70114155
    Iteration 9, loss = 0.69454497
    Iteration 10, loss = 0.69262481
    Iteration 11, loss = 0.69206714
    Iteration 12, loss = 0.69190283
    Iteration 13, loss = 0.69183506
    Iteration 14, loss = 0.69185368
    Iteration 15, loss = 0.69183244
    Iteration 16, loss = 0.69184984
    Iteration 17, loss = 0.69182688
    Iteration 18, loss = 0.69180945
    Iteration 19, loss = 0.69182839
    Iteration 20, loss = 0.69182476
    Iteration 21, loss = 0.69183102
    Iteration 22, loss = 0.69181266
    Iteration 23, loss = 0.69181442
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=100, batch_size=auto, hidden_layer_sizes=(20, 20, 20, 20), max_iter=400, random_state=42, solver=adam, verbose=True, warm_start=False; total time=  40.8s
    Iteration 1, loss = 21.71156629
    Iteration 2, loss = 6.48182979
    Iteration 3, loss = 2.22060681
    Iteration 4, loss = 1.03505860
    Iteration 5, loss = 0.75778781
    Iteration 6, loss = 0.70315702
    Iteration 7, loss = 0.69374406
    Iteration 8, loss = 0.69218187
    Iteration 9, loss = 0.69191144
    Iteration 10, loss = 0.69183362
    Iteration 11, loss = 0.69182336
    Iteration 12, loss = 0.69181196
    Iteration 13, loss = 0.69182270
    Iteration 14, loss = 0.69182558
    Iteration 15, loss = 0.69183616
    Iteration 16, loss = 0.69183426
    Iteration 17, loss = 0.69181164
    Iteration 18, loss = 0.69182876
    Iteration 19, loss = 0.69184881
    Iteration 20, loss = 0.69182801
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=100, batch_size=auto, hidden_layer_sizes=(40, 40, 40, 40), max_iter=400, random_state=42, solver=adam, verbose=True, warm_start=False; total time=  56.0s
    Iteration 1, loss = 1.91123841
    Iteration 2, loss = 1.19427168
    Iteration 3, loss = 0.92154469
    Iteration 4, loss = 0.80468251
    Iteration 5, loss = 0.75244958
    Iteration 6, loss = 0.73206128
    Iteration 7, loss = 0.72253101
    Iteration 8, loss = 0.71885667
    Iteration 9, loss = 0.71564229
    Iteration 10, loss = 0.71371633
    Iteration 11, loss = 0.71344323
    Iteration 12, loss = 0.71197997
    Iteration 13, loss = 0.70917206
    Iteration 14, loss = 0.71009100
    Iteration 15, loss = 0.70813053
    Iteration 16, loss = 0.70708975
    Iteration 17, loss = 0.70557889
    Iteration 18, loss = 0.70607594
    Iteration 19, loss = 0.70437064
    Iteration 20, loss = 0.70481957
    Iteration 21, loss = 0.70486001
    Iteration 22, loss = 0.70207676
    Iteration 23, loss = 0.70441443
    Iteration 24, loss = 0.70091806
    Iteration 25, loss = 0.70101905
    Iteration 26, loss = 0.70301011
    Iteration 27, loss = 0.69973697
    Iteration 28, loss = 0.70073457
    Iteration 29, loss = 0.70027846
    Iteration 30, loss = 0.70055057
    Iteration 31, loss = 0.69924839
    Iteration 32, loss = 0.69872360
    Iteration 33, loss = 0.69948550
    Iteration 34, loss = 0.69861353
    Iteration 35, loss = 0.69847482
    Iteration 36, loss = 0.69836559
    Iteration 37, loss = 0.69746971
    Iteration 38, loss = 0.69751036
    Iteration 39, loss = 0.69693071
    Iteration 40, loss = 0.69751832
    Iteration 41, loss = 0.69705373
    Iteration 42, loss = 0.69653541
    Iteration 43, loss = 0.69533358
    Iteration 44, loss = 0.69621924
    Iteration 45, loss = 0.69622340
    Iteration 46, loss = 0.69741853
    Iteration 47, loss = 0.69458184
    Iteration 48, loss = 0.69402776
    Iteration 49, loss = 0.69549358
    Iteration 50, loss = 0.69423887
    Iteration 51, loss = 0.69470695
    Iteration 52, loss = 0.69273210
    Iteration 53, loss = 0.69408496
    Iteration 54, loss = 0.69334811
    Iteration 55, loss = 0.69346127
    Iteration 56, loss = 0.69226476
    Iteration 57, loss = 0.69273822
    Iteration 58, loss = 0.69227631
    Iteration 59, loss = 0.69090108
    Iteration 60, loss = 0.69053232
    Iteration 61, loss = 0.69076970
    Iteration 62, loss = 0.69167280
    Iteration 63, loss = 0.69054598
    Iteration 64, loss = 0.69026968
    Iteration 65, loss = 0.69059195
    Iteration 66, loss = 0.68979135
    Iteration 67, loss = 0.68845787
    Iteration 68, loss = 0.68803780
    Iteration 69, loss = 0.68943213
    Iteration 70, loss = 0.68938750
    Iteration 71, loss = 0.68847619
    Iteration 72, loss = 0.68798311
    Iteration 73, loss = 0.68784627
    Iteration 74, loss = 0.68633708
    Iteration 75, loss = 0.68913980
    Iteration 76, loss = 0.68968434
    Iteration 77, loss = 0.68715948
    Iteration 78, loss = 0.68803069
    Iteration 79, loss = 0.68613141
    Iteration 80, loss = 0.68790006
    Iteration 81, loss = 0.68697475
    Iteration 82, loss = 0.68638774
    Iteration 83, loss = 0.68696527
    Iteration 84, loss = 0.68720638
    Iteration 85, loss = 0.68558916
    Iteration 86, loss = 0.68820162
    Iteration 87, loss = 0.68424397
    Iteration 88, loss = 0.68527971
    Iteration 89, loss = 0.68564613
    Iteration 90, loss = 0.68662969
    Iteration 91, loss = 0.68449198
    Iteration 92, loss = 0.68441819
    Iteration 93, loss = 0.68642275
    Iteration 94, loss = 0.68364435
    Iteration 95, loss = 0.68561546
    Iteration 96, loss = 0.68476046
    Iteration 97, loss = 0.68401099
    Iteration 98, loss = 0.68433331
    Iteration 99, loss = 0.68469917
    Iteration 100, loss = 0.68520503
    Iteration 101, loss = 0.68512289
    Iteration 102, loss = 0.68317840
    Iteration 103, loss = 0.68360578
    Iteration 104, loss = 0.68391302
    Iteration 105, loss = 0.68413746
    Iteration 106, loss = 0.68268477
    Iteration 107, loss = 0.68321105
    Iteration 108, loss = 0.68417739
    Iteration 109, loss = 0.68361912
    Iteration 110, loss = 0.68321078
    Iteration 111, loss = 0.68280134
    Iteration 112, loss = 0.68370001
    Iteration 113, loss = 0.68231884
    Iteration 114, loss = 0.68436439
    Iteration 115, loss = 0.68131362
    Iteration 116, loss = 0.68245433
    Iteration 117, loss = 0.68286485
    Iteration 118, loss = 0.68312787
    Iteration 119, loss = 0.68346370
    Iteration 120, loss = 0.68239282
    Iteration 121, loss = 0.68187709
    Iteration 122, loss = 0.68211782
    Iteration 123, loss = 0.68191282
    Iteration 124, loss = 0.68188130
    Iteration 125, loss = 0.68201933
    Iteration 126, loss = 0.68178704
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=10, batch_size=auto, hidden_layer_sizes=(20, 20, 20, 20), max_iter=400, random_state=42, solver=adam, verbose=True, warm_start=False; total time= 3.5min
    Iteration 1, loss = 1.91027446
    Iteration 2, loss = 1.19580626
    Iteration 3, loss = 0.92333013
    Iteration 4, loss = 0.80544643
    Iteration 5, loss = 0.75496976
    Iteration 6, loss = 0.73419093
    Iteration 7, loss = 0.72501566
    Iteration 8, loss = 0.72047729
    Iteration 9, loss = 0.71777879
    Iteration 10, loss = 0.71623065
    Iteration 11, loss = 0.71591889
    Iteration 12, loss = 0.71369579
    Iteration 13, loss = 0.71212837
    Iteration 14, loss = 0.71112513
    Iteration 15, loss = 0.71183572
    Iteration 16, loss = 0.70973996
    Iteration 17, loss = 0.70932042
    Iteration 18, loss = 0.70883478
    Iteration 19, loss = 0.70760210
    Iteration 20, loss = 0.70800861
    Iteration 21, loss = 0.70934148
    Iteration 22, loss = 0.70616110
    Iteration 23, loss = 0.70556897
    Iteration 24, loss = 0.70512494
    Iteration 25, loss = 0.70595694
    Iteration 26, loss = 0.70476879
    Iteration 27, loss = 0.70447758
    Iteration 28, loss = 0.70384536
    Iteration 29, loss = 0.70414344
    Iteration 30, loss = 0.70344075
    Iteration 31, loss = 0.70265399
    Iteration 32, loss = 0.70235942
    Iteration 33, loss = 0.70317626
    Iteration 34, loss = 0.70002788
    Iteration 35, loss = 0.70093482
    Iteration 36, loss = 0.70102805
    Iteration 37, loss = 0.69952841
    Iteration 38, loss = 0.70034785
    Iteration 39, loss = 0.69878896
    Iteration 40, loss = 0.69802643
    Iteration 41, loss = 0.70014433
    Iteration 42, loss = 0.69738705
    Iteration 43, loss = 0.69797962
    Iteration 44, loss = 0.69725177
    Iteration 45, loss = 0.69702728
    Iteration 46, loss = 0.69740730
    Iteration 47, loss = 0.69600771
    Iteration 48, loss = 0.69721791
    Iteration 49, loss = 0.69559180
    Iteration 50, loss = 0.69638748
    Iteration 51, loss = 0.69428147
    Iteration 52, loss = 0.69413961
    Iteration 53, loss = 0.69486095
    Iteration 54, loss = 0.69396436
    Iteration 55, loss = 0.69252686
    Iteration 56, loss = 0.69334976
    Iteration 57, loss = 0.69342103
    Iteration 58, loss = 0.69345382
    Iteration 59, loss = 0.69197258
    Iteration 60, loss = 0.69150148
    Iteration 61, loss = 0.69315251
    Iteration 62, loss = 0.69184406
    Iteration 63, loss = 0.69200001
    Iteration 64, loss = 0.69067298
    Iteration 65, loss = 0.69201093
    Iteration 66, loss = 0.69176144
    Iteration 67, loss = 0.69054323
    Iteration 68, loss = 0.69050888
    Iteration 69, loss = 0.68975422
    Iteration 70, loss = 0.69293207
    Iteration 71, loss = 0.69055394
    Iteration 72, loss = 0.68858681
    Iteration 73, loss = 0.68973804
    Iteration 74, loss = 0.69007636
    Iteration 75, loss = 0.68928733
    Iteration 76, loss = 0.68937522
    Iteration 77, loss = 0.69277120
    Iteration 78, loss = 0.69038671
    Iteration 79, loss = 0.68765578
    Iteration 80, loss = 0.68787820
    Iteration 81, loss = 0.68800160
    Iteration 82, loss = 0.68842152
    Iteration 83, loss = 0.68846807
    Iteration 84, loss = 0.68758576
    Iteration 85, loss = 0.68875243
    Iteration 86, loss = 0.68806948
    Iteration 87, loss = 0.68834314
    Iteration 88, loss = 0.68750948
    Iteration 89, loss = 0.68764349
    Iteration 90, loss = 0.68765594
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=10, batch_size=auto, hidden_layer_sizes=(20, 20, 20, 20), max_iter=400, random_state=42, solver=adam, verbose=True, warm_start=False; total time= 2.7min
    Iteration 1, loss = 21.71280455
    Iteration 2, loss = 6.48199250
    Iteration 3, loss = 2.22058101
    Iteration 4, loss = 1.03501884
    Iteration 5, loss = 0.75777497
    Iteration 6, loss = 0.70315048
    Iteration 7, loss = 0.69374350
    Iteration 8, loss = 0.69218937
    Iteration 9, loss = 0.69191535
    Iteration 10, loss = 0.69185573
    Iteration 11, loss = 0.69183307
    Iteration 12, loss = 0.69183317
    Iteration 13, loss = 0.69183596
    Iteration 14, loss = 0.69182564
    Iteration 15, loss = 0.69182608
    Iteration 16, loss = 0.69182712
    Iteration 17, loss = 0.69181083
    Iteration 18, loss = 0.69184898
    Iteration 19, loss = 0.69183361
    Iteration 20, loss = 0.69183093
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=100, batch_size=auto, hidden_layer_sizes=(40, 40, 40, 40), max_iter=400, random_state=42, solver=adam, verbose=True, warm_start=False; total time=  53.3s
    Iteration 1, loss = 0.78978245
    Iteration 2, loss = 0.59829252
    Iteration 3, loss = 0.51450391
    Iteration 4, loss = 0.46194768
    Iteration 5, loss = 0.41287823
    Iteration 6, loss = 0.37504217
    Iteration 7, loss = 0.33865973
    Iteration 8, loss = 0.31467027
    Iteration 9, loss = 0.28789343
    Iteration 10, loss = 0.27149018
    Iteration 11, loss = 0.27327907
    Iteration 12, loss = 0.25321859
    Iteration 13, loss = 0.24812767
    Iteration 14, loss = 0.23883192
    Iteration 15, loss = 0.24690136
    Iteration 16, loss = 0.24467609
    Iteration 17, loss = 0.24668804
    Iteration 18, loss = 0.23781528
    Iteration 19, loss = 0.22661645
    Iteration 20, loss = 0.22316960
    Iteration 21, loss = 0.22117634
    Iteration 22, loss = 0.22816611
    Iteration 23, loss = 0.22801499
    Iteration 24, loss = 0.23299101
    Iteration 25, loss = 0.21616765
    Iteration 26, loss = 0.20607947
    Iteration 27, loss = 0.19519548
    Iteration 28, loss = 0.20360098
    Iteration 29, loss = 0.22335601
    Iteration 30, loss = 0.22295985
    Iteration 31, loss = 0.21692097
    Iteration 32, loss = 0.21725097
    Iteration 33, loss = 0.21235312
    Iteration 34, loss = 0.20169296
    Iteration 35, loss = 0.19699338
    Iteration 36, loss = 0.19211992
    Iteration 37, loss = 0.18993706
    Iteration 38, loss = 0.20900223
    Iteration 39, loss = 0.22580595
    Iteration 40, loss = 0.21916168
    Iteration 41, loss = 0.21155140
    Iteration 42, loss = 0.20253863
    Iteration 43, loss = 0.18536712
    Iteration 44, loss = 0.18192111
    Iteration 45, loss = 0.18584947
    Iteration 46, loss = 0.20124449
    Iteration 47, loss = 0.22614364
    Iteration 48, loss = 0.22318956
    Iteration 49, loss = 0.20809316
    Iteration 50, loss = 0.19949315
    Iteration 51, loss = 0.19123385
    Iteration 52, loss = 0.18337302
    Iteration 53, loss = 0.17300707
    Iteration 54, loss = 0.17611040
    Iteration 55, loss = 0.18456481
    Iteration 56, loss = 0.23732094
    Iteration 57, loss = 0.23656444
    Iteration 58, loss = 0.21472240
    Iteration 59, loss = 0.19146420
    Iteration 60, loss = 0.17718121
    Iteration 61, loss = 0.16411696
    Iteration 62, loss = 0.15618089
    Iteration 63, loss = 0.15151734
    Iteration 64, loss = 0.16062706
    Iteration 65, loss = 0.24370328
    Iteration 66, loss = 0.25711011
    Iteration 67, loss = 0.22302623
    Iteration 68, loss = 0.19525123
    Iteration 69, loss = 0.18099829
    Iteration 70, loss = 0.17332590
    Iteration 71, loss = 0.16917408
    Iteration 72, loss = 0.17559556
    Iteration 73, loss = 0.19116831
    Iteration 74, loss = 0.21202165
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=1, batch_size=auto, hidden_layer_sizes=(20, 20, 20, 20), max_iter=400, random_state=42, solver=adam, verbose=True, warm_start=False; total time= 2.1min
    Iteration 1, loss = 12.66414284
    Iteration 2, loss = 5.64750599
    Iteration 3, loss = 2.80173933
    Iteration 4, loss = 1.52357712
    Iteration 5, loss = 0.99342131
    Iteration 6, loss = 0.79262675
    Iteration 7, loss = 0.72321627
    Iteration 8, loss = 0.70111669
    Iteration 9, loss = 0.69452047
    Iteration 10, loss = 0.69263033
    Iteration 11, loss = 0.69205972
    Iteration 12, loss = 0.69188829
    Iteration 13, loss = 0.69183770
    Iteration 14, loss = 0.69182605
    Iteration 15, loss = 0.69183325
    Iteration 16, loss = 0.69184166
    Iteration 17, loss = 0.69181535
    Iteration 18, loss = 0.69180517
    Iteration 19, loss = 0.69181175
    Iteration 20, loss = 0.69181265
    Iteration 21, loss = 0.69181454
    Iteration 22, loss = 0.69183204
    Iteration 23, loss = 0.69182190
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=100, batch_size=auto, hidden_layer_sizes=(20, 20, 20, 20), max_iter=400, random_state=42, solver=adam, verbose=True, warm_start=False; total time=  41.4s
    Iteration 1, loss = 21.71313254
    Iteration 2, loss = 6.48359541
    Iteration 3, loss = 2.22126202
    Iteration 4, loss = 1.03520714
    Iteration 5, loss = 0.75780798
    Iteration 6, loss = 0.70317250
    Iteration 7, loss = 0.69373835
    Iteration 8, loss = 0.69218759
    Iteration 9, loss = 0.69189632
    Iteration 10, loss = 0.69183686
    Iteration 11, loss = 0.69182559
    Iteration 12, loss = 0.69183143
    Iteration 13, loss = 0.69183428
    Iteration 14, loss = 0.69183268
    Iteration 15, loss = 0.69182190
    Iteration 16, loss = 0.69185599
    Iteration 17, loss = 0.69183851
    Iteration 18, loss = 0.69182813
    Iteration 19, loss = 0.69182980
    Iteration 20, loss = 0.69183330
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=100, batch_size=auto, hidden_layer_sizes=(40, 40, 40, 40), max_iter=400, random_state=42, solver=adam, verbose=True, warm_start=False; total time=  46.4s
    Iteration 1, loss = 1.90586282
    Iteration 2, loss = 1.18932736
    Iteration 3, loss = 0.92272504
    Iteration 4, loss = 0.80118485
    Iteration 5, loss = 0.75204426
    Iteration 6, loss = 0.72926948
    Iteration 7, loss = 0.72115148
    Iteration 8, loss = 0.71726816
    Iteration 9, loss = 0.71203117
    Iteration 10, loss = 0.71097650
    Iteration 11, loss = 0.70954312
    Iteration 12, loss = 0.70942543
    Iteration 13, loss = 0.70707792
    Iteration 14, loss = 0.70616833
    Iteration 15, loss = 0.70586544
    Iteration 16, loss = 0.70406833
    Iteration 17, loss = 0.70449573
    Iteration 18, loss = 0.70510602
    Iteration 19, loss = 0.70319921
    Iteration 20, loss = 0.70188773
    Iteration 21, loss = 0.70162036
    Iteration 22, loss = 0.70119476
    Iteration 23, loss = 0.70060447
    Iteration 24, loss = 0.70061288
    Iteration 25, loss = 0.70128912
    Iteration 26, loss = 0.69895494
    Iteration 27, loss = 0.69948464
    Iteration 28, loss = 0.69699149
    Iteration 29, loss = 0.69743572
    Iteration 30, loss = 0.69744236
    Iteration 31, loss = 0.69768194
    Iteration 32, loss = 0.69632334
    Iteration 33, loss = 0.69862631
    Iteration 34, loss = 0.69552620
    Iteration 35, loss = 0.69583515
    Iteration 36, loss = 0.69450696
    Iteration 37, loss = 0.69549191
    Iteration 38, loss = 0.69615831
    Iteration 39, loss = 0.69425081
    Iteration 40, loss = 0.69482452
    Iteration 41, loss = 0.69418892
    Iteration 42, loss = 0.69466292
    Iteration 43, loss = 0.69296895
    Iteration 44, loss = 0.69366151
    Iteration 45, loss = 0.69304873
    Iteration 46, loss = 0.69442284
    Iteration 47, loss = 0.69453421
    Iteration 48, loss = 0.69261985
    Iteration 49, loss = 0.69321340
    Iteration 50, loss = 0.69075267
    Iteration 51, loss = 0.69267837
    Iteration 52, loss = 0.69146729
    Iteration 53, loss = 0.69269813
    Iteration 54, loss = 0.69223192
    Iteration 55, loss = 0.69107356
    Iteration 56, loss = 0.69159883
    Iteration 57, loss = 0.69146276
    Iteration 58, loss = 0.69093035
    Iteration 59, loss = 0.69111000
    Iteration 60, loss = 0.68976819
    Iteration 61, loss = 0.69094908
    Iteration 62, loss = 0.69043680
    Iteration 63, loss = 0.68961254
    Iteration 64, loss = 0.68981494
    Iteration 65, loss = 0.68814151
    Iteration 66, loss = 0.68903763
    Iteration 67, loss = 0.68995090
    Iteration 68, loss = 0.68831444
    Iteration 69, loss = 0.68865086
    Iteration 70, loss = 0.69012395
    Iteration 71, loss = 0.68720162
    Iteration 72, loss = 0.68592767
    Iteration 73, loss = 0.68754296
    Iteration 74, loss = 0.68565456
    Iteration 75, loss = 0.68726542
    Iteration 76, loss = 0.68554866
    Iteration 77, loss = 0.68710463
    Iteration 78, loss = 0.68696359
    Iteration 79, loss = 0.68444894
    Iteration 80, loss = 0.68484112
    Iteration 81, loss = 0.68662206
    Iteration 82, loss = 0.68425865
    Iteration 83, loss = 0.68636484
    Iteration 84, loss = 0.68485500
    Iteration 85, loss = 0.68464936
    Iteration 86, loss = 0.68457293
    Iteration 87, loss = 0.68484011
    Iteration 88, loss = 0.68299228
    Iteration 89, loss = 0.68483710
    Iteration 90, loss = 0.68474143
    Iteration 91, loss = 0.68447005
    Iteration 92, loss = 0.68285505
    Iteration 93, loss = 0.68328144
    Iteration 94, loss = 0.68394046
    Iteration 95, loss = 0.68282951
    Iteration 96, loss = 0.68271219
    Iteration 97, loss = 0.68337167
    Iteration 98, loss = 0.68257598
    Iteration 99, loss = 0.68634098
    Iteration 100, loss = 0.68200583
    Iteration 101, loss = 0.68278873
    Iteration 102, loss = 0.68280062
    Iteration 103, loss = 0.68233337
    Iteration 104, loss = 0.68283558
    Iteration 105, loss = 0.68232220
    Iteration 106, loss = 0.68207142
    Iteration 107, loss = 0.68184817
    Iteration 108, loss = 0.68280183
    Iteration 109, loss = 0.68339215
    Iteration 110, loss = 0.68202537
    Iteration 111, loss = 0.68115637
    Iteration 112, loss = 0.68203858
    Iteration 113, loss = 0.68124129
    Iteration 114, loss = 0.68156885
    Iteration 115, loss = 0.68132906
    Iteration 116, loss = 0.68124985
    Iteration 117, loss = 0.68080132
    Iteration 118, loss = 0.68329906
    Iteration 119, loss = 0.68013698
    Iteration 120, loss = 0.68076484
    Iteration 121, loss = 0.68100655
    Iteration 122, loss = 0.67992274
    Iteration 123, loss = 0.68087485
    Iteration 124, loss = 0.68086455
    Iteration 125, loss = 0.68153768
    Iteration 126, loss = 0.67950901
    Iteration 127, loss = 0.67991229
    Iteration 128, loss = 0.68040532
    Iteration 129, loss = 0.67930335
    Iteration 130, loss = 0.68105405
    Iteration 131, loss = 0.68145538
    Iteration 132, loss = 0.67943714
    Iteration 133, loss = 0.67924881
    Iteration 134, loss = 0.67975619
    Iteration 135, loss = 0.68011589
    Iteration 136, loss = 0.67952762
    Iteration 137, loss = 0.67825861
    Iteration 138, loss = 0.67911815
    Iteration 139, loss = 0.67964758
    Iteration 140, loss = 0.67928448
    Iteration 141, loss = 0.67882613
    Iteration 142, loss = 0.67892388
    Iteration 143, loss = 0.67846770
    Iteration 144, loss = 0.68007626
    Iteration 145, loss = 0.67833453
    Iteration 146, loss = 0.67939536
    Iteration 147, loss = 0.67877742
    Iteration 148, loss = 0.67827766
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=10, batch_size=auto, hidden_layer_sizes=(20, 20, 20, 20), max_iter=400, random_state=42, solver=adam, verbose=True, warm_start=False; total time= 3.8min
    Iteration 1, loss = 0.89589522
    Iteration 2, loss = 0.65162266
    Iteration 3, loss = 0.55119551
    Iteration 4, loss = 0.48043140
    Iteration 5, loss = 0.41973146
    Iteration 6, loss = 0.37167171
    Iteration 7, loss = 0.33836642
    Iteration 8, loss = 0.30791309
    Iteration 9, loss = 0.29771111
    Iteration 10, loss = 0.28758797
    Iteration 11, loss = 0.27618941
    Iteration 12, loss = 0.27176147
    Iteration 13, loss = 0.25697824
    Iteration 14, loss = 0.25142269
    Iteration 15, loss = 0.25381998
    Iteration 16, loss = 0.24882232
    Iteration 17, loss = 0.24304735
    Iteration 18, loss = 0.24301361
    Iteration 19, loss = 0.24391681
    Iteration 20, loss = 0.23517953
    Iteration 21, loss = 0.22582058
    Iteration 22, loss = 0.22173827
    Iteration 23, loss = 0.23038068
    Iteration 24, loss = 0.23769681
    Iteration 25, loss = 0.23987423
    Iteration 26, loss = 0.22903973
    Iteration 27, loss = 0.21411744
    Iteration 28, loss = 0.20986399
    Iteration 29, loss = 0.21133795
    Iteration 30, loss = 0.20734803
    Iteration 31, loss = 0.21985850
    Iteration 32, loss = 0.21630619
    Iteration 33, loss = 0.23009500
    Iteration 34, loss = 0.22364450
    Iteration 35, loss = 0.21461518
    Iteration 36, loss = 0.20914764
    Iteration 37, loss = 0.20216303
    Iteration 38, loss = 0.21154461
    Iteration 39, loss = 0.21736265
    Iteration 40, loss = 0.21229184
    Iteration 41, loss = 0.21240110
    Iteration 42, loss = 0.20549371
    Iteration 43, loss = 0.20792287
    Iteration 44, loss = 0.21474824
    Iteration 45, loss = 0.21003491
    Iteration 46, loss = 0.20368173
    Iteration 47, loss = 0.20224310
    Iteration 48, loss = 0.19904986
    Iteration 49, loss = 0.19747128
    Iteration 50, loss = 0.20192784
    Iteration 51, loss = 0.20157550
    Iteration 52, loss = 0.21679277
    Iteration 53, loss = 0.21677266
    Iteration 54, loss = 0.20278558
    Iteration 55, loss = 0.19451946
    Iteration 56, loss = 0.18777925
    Iteration 57, loss = 0.19077773
    Iteration 58, loss = 0.20717566
    Iteration 59, loss = 0.22538098
    Iteration 60, loss = 0.22887940
    Iteration 61, loss = 0.22714875
    Iteration 62, loss = 0.21298989
    Iteration 63, loss = 0.19023952
    Iteration 64, loss = 0.17273950
    Iteration 65, loss = 0.15936491
    Iteration 66, loss = 0.14983591
    Iteration 67, loss = 0.14928285
    Iteration 68, loss = 0.19790320
    Iteration 69, loss = 0.28288629
    Iteration 70, loss = 0.24771305
    Iteration 71, loss = 0.21642776
    Iteration 72, loss = 0.19073010
    Iteration 73, loss = 0.17126303
    Iteration 74, loss = 0.15467780
    Iteration 75, loss = 0.13977820
    Iteration 76, loss = 0.13246822
    Iteration 77, loss = 0.13209628
    Iteration 78, loss = 0.19724563
    Iteration 79, loss = 0.31044540
    Iteration 80, loss = 0.25059672
    Iteration 81, loss = 0.21926999
    Iteration 82, loss = 0.19784260
    Iteration 83, loss = 0.17966857
    Iteration 84, loss = 0.17177233
    Iteration 85, loss = 0.16363661
    Iteration 86, loss = 0.18033662
    Iteration 87, loss = 0.22882112
    Iteration 88, loss = 0.24135679
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=1, batch_size=auto, hidden_layer_sizes=(40, 40, 40, 40), max_iter=400, random_state=42, solver=adam, verbose=True, warm_start=False; total time= 3.9min
    Iteration 1, loss = 2.81643665
    Iteration 2, loss = 1.29565053
    Iteration 3, loss = 0.88123481
    Iteration 4, loss = 0.75868125
    Iteration 5, loss = 0.73005098
    Iteration 6, loss = 0.72063630
    Iteration 7, loss = 0.71945064
    Iteration 8, loss = 0.71767021
    Iteration 9, loss = 0.71657561
    Iteration 10, loss = 0.71582005
    Iteration 11, loss = 0.71517548
    Iteration 12, loss = 0.71417534
    Iteration 13, loss = 0.71453317
    Iteration 14, loss = 0.71250836
    Iteration 15, loss = 0.71148531
    Iteration 16, loss = 0.71179733
    Iteration 17, loss = 0.70957677
    Iteration 18, loss = 0.71014278
    Iteration 19, loss = 0.70850685
    Iteration 20, loss = 0.70821039
    Iteration 21, loss = 0.70781032
    Iteration 22, loss = 0.70796328
    Iteration 23, loss = 0.70835280
    Iteration 24, loss = 0.70572960
    Iteration 25, loss = 0.70590879
    Iteration 26, loss = 0.70549304
    Iteration 27, loss = 0.70660439
    Iteration 28, loss = 0.70460004
    Iteration 29, loss = 0.70442086
    Iteration 30, loss = 0.70255813
    Iteration 31, loss = 0.70244826
    Iteration 32, loss = 0.70256575
    Iteration 33, loss = 0.70041669
    Iteration 34, loss = 0.70098506
    Iteration 35, loss = 0.70044587
    Iteration 36, loss = 0.69944296
    Iteration 37, loss = 0.69994945
    Iteration 38, loss = 0.69912212
    Iteration 39, loss = 0.70022139
    Iteration 40, loss = 0.69868429
    Iteration 41, loss = 0.69698122
    Iteration 42, loss = 0.69835648
    Iteration 43, loss = 0.69687352
    Iteration 44, loss = 0.69832997
    Iteration 45, loss = 0.69717180
    Iteration 46, loss = 0.69751416
    Iteration 47, loss = 0.69632981
    Iteration 48, loss = 0.69625312
    Iteration 49, loss = 0.69655391
    Iteration 50, loss = 0.69590349
    Iteration 51, loss = 0.69688728
    Iteration 52, loss = 0.69384483
    Iteration 53, loss = 0.69691722
    Iteration 54, loss = 0.69487632
    Iteration 55, loss = 0.69439195
    Iteration 56, loss = 0.69401509
    Iteration 57, loss = 0.69532768
    Iteration 58, loss = 0.69346939
    Iteration 59, loss = 0.69425285
    Iteration 60, loss = 0.69420084
    Iteration 61, loss = 0.69450287
    Iteration 62, loss = 0.69588159
    Iteration 63, loss = 0.69303476
    Iteration 64, loss = 0.69426867
    Iteration 65, loss = 0.69316862
    Iteration 66, loss = 0.69378200
    Iteration 67, loss = 0.69521470
    Iteration 68, loss = 0.69427993
    Iteration 69, loss = 0.69202860
    Iteration 70, loss = 0.69485537
    Iteration 71, loss = 0.69369362
    Iteration 72, loss = 0.69380066
    Iteration 73, loss = 0.69455110
    Iteration 74, loss = 0.69268928
    Iteration 75, loss = 0.69353330
    Iteration 76, loss = 0.69306448
    Iteration 77, loss = 0.69294392
    Iteration 78, loss = 0.69360875
    Iteration 79, loss = 0.69290178
    Iteration 80, loss = 0.69149237
    Iteration 81, loss = 0.69324378
    Iteration 82, loss = 0.69340404
    Iteration 83, loss = 0.69202083
    Iteration 84, loss = 0.69226801
    Iteration 85, loss = 0.69164213
    Iteration 86, loss = 0.69153399
    Iteration 87, loss = 0.69122808
    Iteration 88, loss = 0.69253954
    Iteration 89, loss = 0.69151682
    Iteration 90, loss = 0.69169417
    Iteration 91, loss = 0.69269630
    Iteration 92, loss = 0.69284488
    Iteration 93, loss = 0.69150323
    Iteration 94, loss = 0.69231567
    Iteration 95, loss = 0.69102604
    Iteration 96, loss = 0.69336572
    Iteration 97, loss = 0.69010277
    Iteration 98, loss = 0.69335146
    Iteration 99, loss = 0.69051584
    Iteration 100, loss = 0.69217319
    Iteration 101, loss = 0.69166149
    Iteration 102, loss = 0.69142848
    Iteration 103, loss = 0.68930683
    Iteration 104, loss = 0.69139679
    Iteration 105, loss = 0.69017263
    Iteration 106, loss = 0.69205276
    Iteration 107, loss = 0.69142901
    Iteration 108, loss = 0.69209542
    Iteration 109, loss = 0.69122512
    Iteration 110, loss = 0.68991477
    Iteration 111, loss = 0.68998204
    Iteration 112, loss = 0.69104660
    Iteration 113, loss = 0.69035607
    Iteration 114, loss = 0.69092909
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=10, batch_size=auto, hidden_layer_sizes=(40, 40, 40, 40), max_iter=400, random_state=42, solver=adam, verbose=True, warm_start=False; total time= 4.2min
    Iteration 1, loss = 2.82052486
    Iteration 2, loss = 1.29907492
    Iteration 3, loss = 0.88207427
    Iteration 4, loss = 0.75825847
    Iteration 5, loss = 0.72923837
    Iteration 6, loss = 0.72115147
    Iteration 7, loss = 0.71969736
    Iteration 8, loss = 0.71823172
    Iteration 9, loss = 0.71671160
    Iteration 10, loss = 0.71599086
    Iteration 11, loss = 0.71638779
    Iteration 12, loss = 0.71377191
    Iteration 13, loss = 0.71398629
    Iteration 14, loss = 0.71144875
    Iteration 15, loss = 0.71186178
    Iteration 16, loss = 0.71135839
    Iteration 17, loss = 0.71093789
    Iteration 18, loss = 0.71035952
    Iteration 19, loss = 0.70982052
    Iteration 20, loss = 0.70795394
    Iteration 21, loss = 0.70937766
    Iteration 22, loss = 0.70970823
    Iteration 23, loss = 0.70924121
    Iteration 24, loss = 0.70859154
    Iteration 25, loss = 0.70773274
    Iteration 26, loss = 0.70644494
    Iteration 27, loss = 0.70772969
    Iteration 28, loss = 0.70811244
    Iteration 29, loss = 0.70638216
    Iteration 30, loss = 0.70466340
    Iteration 31, loss = 0.70340628
    Iteration 32, loss = 0.70344396
    Iteration 33, loss = 0.70314552
    Iteration 34, loss = 0.70272043
    Iteration 35, loss = 0.70159523
    Iteration 36, loss = 0.70058322
    Iteration 37, loss = 0.70096544
    Iteration 38, loss = 0.69972714
    Iteration 39, loss = 0.69930957
    Iteration 40, loss = 0.69740379
    Iteration 41, loss = 0.69725712
    Iteration 42, loss = 0.69720944
    Iteration 43, loss = 0.69612190
    Iteration 44, loss = 0.69608295
    Iteration 45, loss = 0.69710802
    Iteration 46, loss = 0.69659340
    Iteration 47, loss = 0.69399109
    Iteration 48, loss = 0.69491716
    Iteration 49, loss = 0.69660757
    Iteration 50, loss = 0.69569440
    Iteration 51, loss = 0.69397984
    Iteration 52, loss = 0.69308022
    Iteration 53, loss = 0.69463641
    Iteration 54, loss = 0.69435560
    Iteration 55, loss = 0.69457888
    Iteration 56, loss = 0.69205123
    Iteration 57, loss = 0.69365104
    Iteration 58, loss = 0.69205295
    Iteration 59, loss = 0.69176079
    Iteration 60, loss = 0.69369263
    Iteration 61, loss = 0.69173597
    Iteration 62, loss = 0.69151400
    Iteration 63, loss = 0.69181343
    Iteration 64, loss = 0.69265889
    Iteration 65, loss = 0.69228172
    Iteration 66, loss = 0.69076196
    Iteration 67, loss = 0.69376763
    Iteration 68, loss = 0.69059043
    Iteration 69, loss = 0.68994139
    Iteration 70, loss = 0.69122974
    Iteration 71, loss = 0.69253596
    Iteration 72, loss = 0.69011535
    Iteration 73, loss = 0.69148427
    Iteration 74, loss = 0.69105071
    Iteration 75, loss = 0.69134058
    Iteration 76, loss = 0.69059538
    Iteration 77, loss = 0.69019867
    Iteration 78, loss = 0.68963663
    Iteration 79, loss = 0.69060694
    Iteration 80, loss = 0.68931206
    Iteration 81, loss = 0.69083293
    Iteration 82, loss = 0.69074651
    Iteration 83, loss = 0.68877167
    Iteration 84, loss = 0.69001883
    Iteration 85, loss = 0.69033297
    Iteration 86, loss = 0.68958145
    Iteration 87, loss = 0.68955838
    Iteration 88, loss = 0.68921417
    Iteration 89, loss = 0.68956716
    Iteration 90, loss = 0.68963480
    Iteration 91, loss = 0.68922302
    Iteration 92, loss = 0.68964092
    Iteration 93, loss = 0.69004568
    Iteration 94, loss = 0.68866204
    Iteration 95, loss = 0.68790056
    Iteration 96, loss = 0.68981746
    Iteration 97, loss = 0.68835574
    Iteration 98, loss = 0.68922086
    Iteration 99, loss = 0.68727219
    Iteration 100, loss = 0.68881440
    Iteration 101, loss = 0.68878946
    Iteration 102, loss = 0.68899721
    Iteration 103, loss = 0.68836042
    Iteration 104, loss = 0.69035498
    Iteration 105, loss = 0.68692722
    Iteration 106, loss = 0.68834195
    Iteration 107, loss = 0.68844195
    Iteration 108, loss = 0.68980957
    Iteration 109, loss = 0.68843951
    Iteration 110, loss = 0.68761664
    Iteration 111, loss = 0.68874131
    Iteration 112, loss = 0.68881883
    Iteration 113, loss = 0.68780119
    Iteration 114, loss = 0.68864542
    Iteration 115, loss = 0.68754161
    Iteration 116, loss = 0.68795435
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=10, batch_size=auto, hidden_layer_sizes=(40, 40, 40, 40), max_iter=400, random_state=42, solver=adam, verbose=True, warm_start=False; total time= 4.2min
    Iteration 1, loss = 2.81854333
    Iteration 2, loss = 1.29587713
    Iteration 3, loss = 0.87815060
    Iteration 4, loss = 0.75750107
    Iteration 5, loss = 0.72865995
    Iteration 6, loss = 0.71961198
    Iteration 7, loss = 0.71755950
    Iteration 8, loss = 0.71565534
    Iteration 9, loss = 0.71358897
    Iteration 10, loss = 0.71357058
    Iteration 11, loss = 0.71329185
    Iteration 12, loss = 0.71219445
    Iteration 13, loss = 0.71064837
    Iteration 14, loss = 0.71007821
    Iteration 15, loss = 0.71084271
    Iteration 16, loss = 0.70939830
    Iteration 17, loss = 0.70965807
    Iteration 18, loss = 0.70807863
    Iteration 19, loss = 0.70879689
    Iteration 20, loss = 0.70888793
    Iteration 21, loss = 0.70674558
    Iteration 22, loss = 0.70812072
    Iteration 23, loss = 0.70606623
    Iteration 24, loss = 0.70563574
    Iteration 25, loss = 0.70585651
    Iteration 26, loss = 0.70586843
    Iteration 27, loss = 0.70463831
    Iteration 28, loss = 0.70414546
    Iteration 29, loss = 0.70351957
    Iteration 30, loss = 0.70228371
    Iteration 31, loss = 0.70241038
    Iteration 32, loss = 0.70138563
    Iteration 33, loss = 0.70097148
    Iteration 34, loss = 0.70001771
    Iteration 35, loss = 0.69977903
    Iteration 36, loss = 0.69743852
    Iteration 37, loss = 0.69799820
    Iteration 38, loss = 0.69812322
    Iteration 39, loss = 0.69651661
    Iteration 40, loss = 0.69729918
    Iteration 41, loss = 0.69520544
    Iteration 42, loss = 0.69402517
    Iteration 43, loss = 0.69390966
    Iteration 44, loss = 0.69372598
    Iteration 45, loss = 0.69289348
    Iteration 46, loss = 0.69143830
    Iteration 47, loss = 0.69273103
    Iteration 48, loss = 0.69241884
    Iteration 49, loss = 0.69081374
    Iteration 50, loss = 0.69298720
    Iteration 51, loss = 0.68988030
    Iteration 52, loss = 0.68929488
    Iteration 53, loss = 0.69108058
    Iteration 54, loss = 0.68930287
    Iteration 55, loss = 0.68904454
    Iteration 56, loss = 0.68952722
    Iteration 57, loss = 0.68910564
    Iteration 58, loss = 0.68728192
    Iteration 59, loss = 0.68907004
    Iteration 60, loss = 0.68803812
    Iteration 61, loss = 0.68760763
    Iteration 62, loss = 0.68854587
    Iteration 63, loss = 0.68674619
    Iteration 64, loss = 0.68757762
    Iteration 65, loss = 0.68739391
    Iteration 66, loss = 0.68632829
    Iteration 67, loss = 0.68581493
    Iteration 68, loss = 0.68616001
    Iteration 69, loss = 0.68658100
    Iteration 70, loss = 0.68531990
    Iteration 71, loss = 0.68586798
    Iteration 72, loss = 0.68448730
    Iteration 73, loss = 0.68600773
    Iteration 74, loss = 0.68458355
    Iteration 75, loss = 0.68439183
    Iteration 76, loss = 0.68502065
    Iteration 77, loss = 0.68498007
    Iteration 78, loss = 0.68320854
    Iteration 79, loss = 0.68439178
    Iteration 80, loss = 0.68350528
    Iteration 81, loss = 0.68444105
    Iteration 82, loss = 0.68333626
    Iteration 83, loss = 0.68312129
    Iteration 84, loss = 0.68386852
    Iteration 85, loss = 0.68224697
    Iteration 86, loss = 0.68424559
    Iteration 87, loss = 0.68187286
    Iteration 88, loss = 0.68219292
    Iteration 89, loss = 0.68311315
    Iteration 90, loss = 0.68270698
    Iteration 91, loss = 0.68172961
    Iteration 92, loss = 0.68303985
    Iteration 93, loss = 0.68286428
    Iteration 94, loss = 0.68160937
    Iteration 95, loss = 0.68099267
    Iteration 96, loss = 0.68127777
    Iteration 97, loss = 0.68116170
    Iteration 98, loss = 0.68288809
    Iteration 99, loss = 0.67968648
    Iteration 100, loss = 0.68204703
    Iteration 101, loss = 0.68114064
    Iteration 102, loss = 0.67954014
    Iteration 103, loss = 0.68009567
    Iteration 104, loss = 0.68034302
    Iteration 105, loss = 0.67974079
    Iteration 106, loss = 0.67910560
    Iteration 107, loss = 0.68069671
    Iteration 108, loss = 0.67927127
    Iteration 109, loss = 0.68204529
    Iteration 110, loss = 0.68004608
    Iteration 111, loss = 0.67866918
    Iteration 112, loss = 0.67878698
    Iteration 113, loss = 0.68033388
    Iteration 114, loss = 0.67849603
    Iteration 115, loss = 0.67815586
    Iteration 116, loss = 0.67783021
    Iteration 117, loss = 0.67961652
    Iteration 118, loss = 0.67695620
    Iteration 119, loss = 0.67783410
    Iteration 120, loss = 0.67764373
    Iteration 121, loss = 0.67739344
    Iteration 122, loss = 0.67727502
    Iteration 123, loss = 0.67646714
    Iteration 124, loss = 0.67754729
    Iteration 125, loss = 0.67872144
    Iteration 126, loss = 0.67755605
    Iteration 127, loss = 0.67661787
    Iteration 128, loss = 0.67764321
    Iteration 129, loss = 0.67827155
    Iteration 130, loss = 0.67607534
    Iteration 131, loss = 0.67633806
    Iteration 132, loss = 0.67766256
    Iteration 133, loss = 0.67798699
    Iteration 134, loss = 0.67758874
    Iteration 135, loss = 0.67642547
    Iteration 136, loss = 0.67609614
    Iteration 137, loss = 0.67601386
    Iteration 138, loss = 0.67803485
    Iteration 139, loss = 0.67747990
    Iteration 140, loss = 0.67561336
    Iteration 141, loss = 0.67511995
    Iteration 142, loss = 0.67646551
    Iteration 143, loss = 0.67531272
    Iteration 144, loss = 0.67621159
    Iteration 145, loss = 0.67750676
    Iteration 146, loss = 0.67477917
    Iteration 147, loss = 0.67653036
    Iteration 148, loss = 0.67568240
    Iteration 149, loss = 0.67537529
    Iteration 150, loss = 0.67721543
    Iteration 151, loss = 0.67642424
    Iteration 152, loss = 0.67445476
    Iteration 153, loss = 0.67590850
    Iteration 154, loss = 0.67568445
    Iteration 155, loss = 0.67563501
    Iteration 156, loss = 0.67549570
    Iteration 157, loss = 0.67429686
    Iteration 158, loss = 0.67451127
    Iteration 159, loss = 0.67659170
    Iteration 160, loss = 0.67585822
    Iteration 161, loss = 0.67513881
    Iteration 162, loss = 0.67519440
    Iteration 163, loss = 0.67539971
    Iteration 164, loss = 0.67405246
    Iteration 165, loss = 0.67505500
    Iteration 166, loss = 0.67399924
    Iteration 167, loss = 0.67393971
    Iteration 168, loss = 0.67330458
    Iteration 169, loss = 0.67542024
    Iteration 170, loss = 0.67515123
    Iteration 171, loss = 0.67418775
    Iteration 172, loss = 0.67459490
    Iteration 173, loss = 0.67419494
    Iteration 174, loss = 0.67393232
    Iteration 175, loss = 0.67447241
    Iteration 176, loss = 0.67369032
    Iteration 177, loss = 0.67388255
    Iteration 178, loss = 0.67340784
    Iteration 179, loss = 0.67306012
    Iteration 180, loss = 0.67374898
    Iteration 181, loss = 0.67313992
    Iteration 182, loss = 0.67379919
    Iteration 183, loss = 0.67373168
    Iteration 184, loss = 0.67335384
    Iteration 185, loss = 0.67483134
    Iteration 186, loss = 0.67459075
    Iteration 187, loss = 0.67374126
    Iteration 188, loss = 0.67470098
    Iteration 189, loss = 0.67309519
    Iteration 190, loss = 0.67297123
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=10, batch_size=auto, hidden_layer_sizes=(40, 40, 40, 40), max_iter=400, random_state=42, solver=adam, verbose=True, warm_start=False; total time= 4.7min


#### 9.3.7. Neural Network modes: Multilayer Perceptron Feed-Forward network - activation: 'relu', Grid Search
- max_feat: 5000
- activation: 'relu', 
- hidden_layer: [(20, 20, 20, 20,20), (25,25,25,25,25),(20,20,20)]
- alpha: [2, 10,15],
- max iterations: default (400)

Results:
- run time: 5 mins
- MSE: 74.1%


```python
params = {'solver':['adam'],
                'alpha':[2, 10,15],
                'hidden_layer_sizes':[(20, 20, 20, 20,20), (20,20,20)],
                'activation':['relu'],
                'random_state':[42],
                'batch_size':['auto'],
                'max_iter':[500], 'verbose':[True],'warm_start':[True]}


skf = StratifiedKFold(n_splits=4)
skf.get_n_splits(X_train, y_train)



gs = GridSearchCV(MLPClassifier(), param_grid=params, cv=skf, n_jobs=-1, verbose=2)
gs.fit(X_train, y_train)

model = gs.best_estimator_
```

    Fitting 4 folds for each of 6 candidates, totalling 24 fits
    Iteration 1, loss = 1.38387710
    Iteration 2, loss = 0.86781597
    Iteration 3, loss = 0.73673483
    Iteration 4, loss = 0.69981619
    Iteration 5, loss = 0.69117742
    Iteration 6, loss = 0.68901145
    Iteration 7, loss = 0.68860612
    Iteration 8, loss = 0.68712426
    Iteration 9, loss = 0.68692999
    Iteration 10, loss = 0.68677188
    Iteration 11, loss = 0.68584978
    Iteration 12, loss = 0.68535792
    Iteration 13, loss = 0.68469554
    Iteration 14, loss = 0.68501983
    Iteration 15, loss = 0.68481742
    Iteration 16, loss = 0.68445238
    Iteration 17, loss = 0.68444516
    Iteration 18, loss = 0.68404435
    Iteration 19, loss = 0.68344713
    Iteration 20, loss = 0.68344785
    Iteration 21, loss = 0.68418872
    Iteration 22, loss = 0.68175452
    Iteration 23, loss = 0.68249407
    Iteration 24, loss = 0.68330110
    Iteration 25, loss = 0.68235641
    Iteration 26, loss = 0.68191326
    Iteration 27, loss = 0.68198127
    Iteration 28, loss = 0.68195521
    Iteration 29, loss = 0.68163074
    Iteration 30, loss = 0.68061458
    Iteration 31, loss = 0.68074909
    Iteration 32, loss = 0.68024776
    Iteration 33, loss = 0.67937408
    Iteration 34, loss = 0.68086267
    Iteration 35, loss = 0.67816413
    Iteration 36, loss = 0.67907293
    Iteration 37, loss = 0.67903070
    Iteration 38, loss = 0.67829194
    Iteration 39, loss = 0.67756589
    Iteration 40, loss = 0.67700158
    Iteration 41, loss = 0.67728970
    Iteration 42, loss = 0.67640244
    Iteration 43, loss = 0.67700707
    Iteration 44, loss = 0.67607477
    Iteration 45, loss = 0.67633660
    Iteration 46, loss = 0.67497012
    Iteration 47, loss = 0.67516172
    Iteration 48, loss = 0.67590460
    Iteration 49, loss = 0.67479366
    Iteration 50, loss = 0.67463691
    Iteration 51, loss = 0.67450668
    Iteration 52, loss = 0.67502663
    Iteration 53, loss = 0.67418689
    Iteration 54, loss = 0.67475829
    Iteration 55, loss = 0.67401815
    Iteration 56, loss = 0.67390618
    Iteration 57, loss = 0.67467977
    Iteration 58, loss = 0.67426180
    Iteration 59, loss = 0.67415117
    Iteration 60, loss = 0.67397518
    Iteration 61, loss = 0.67388987
    Iteration 62, loss = 0.67395693
    Iteration 63, loss = 0.67351301
    Iteration 64, loss = 0.67386052
    Iteration 65, loss = 0.67356563
    Iteration 66, loss = 0.67325003
    Iteration 67, loss = 0.67478531
    Iteration 68, loss = 0.67321311
    Iteration 69, loss = 0.67361007
    Iteration 70, loss = 0.67461627
    Iteration 71, loss = 0.67338052
    Iteration 72, loss = 0.67413055
    Iteration 73, loss = 0.67322289
    Iteration 74, loss = 0.67335412
    Iteration 75, loss = 0.67425018
    Iteration 76, loss = 0.67260021
    Iteration 77, loss = 0.67310527
    Iteration 78, loss = 0.67502093
    Iteration 79, loss = 0.67268207
    Iteration 80, loss = 0.67311839
    Iteration 81, loss = 0.67336959
    Iteration 82, loss = 0.67344804
    Iteration 83, loss = 0.67341483
    Iteration 84, loss = 0.67270245
    Iteration 85, loss = 0.67282625
    Iteration 86, loss = 0.67283526
    Iteration 87, loss = 0.67446428
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.



```python
print(gs.best_estimator_)
print('R2 training: ', gs.score(X_train, y_train))
print('R2 CV training: ', gs.best_score_)
print('R2 test: ', gs.score(X_test, y_test))
```

    MLPClassifier(alpha=10, hidden_layer_sizes=(20, 20, 20), max_iter=500,
                  random_state=42, verbose=True, warm_start=True)
    R2 training:  0.7639546213808464
    R2 CV training:  0.7407781180400891
    R2 test:  0.7284620737647878
    Iteration 1, loss = 0.82313584
    Iteration 2, loss = 0.64606239
    Iteration 3, loss = 0.58416770
    Iteration 4, loss = 0.55567961
    Iteration 5, loss = 0.53483693
    Iteration 6, loss = 0.51972015
    Iteration 7, loss = 0.50453162
    Iteration 8, loss = 0.49113676
    Iteration 9, loss = 0.47911761
    Iteration 10, loss = 0.45991106
    Iteration 11, loss = 0.44131047
    Iteration 12, loss = 0.42730374
    Iteration 13, loss = 0.42218497
    Iteration 14, loss = 0.40290747
    Iteration 15, loss = 0.38782275
    Iteration 16, loss = 0.37983283
    Iteration 17, loss = 0.37175430
    Iteration 18, loss = 0.36636781
    Iteration 19, loss = 0.34879882
    Iteration 20, loss = 0.35393222
    Iteration 21, loss = 0.35189440
    Iteration 22, loss = 0.33881273
    Iteration 23, loss = 0.34132728
    Iteration 24, loss = 0.33284282
    Iteration 25, loss = 0.32909921
    Iteration 26, loss = 0.33479725
    Iteration 27, loss = 0.33380360
    Iteration 28, loss = 0.32947890
    Iteration 29, loss = 0.32115857
    Iteration 30, loss = 0.31208939
    Iteration 31, loss = 0.31271402
    Iteration 32, loss = 0.32358994
    Iteration 33, loss = 0.32455936
    Iteration 34, loss = 0.31532699
    Iteration 35, loss = 0.31422226
    Iteration 36, loss = 0.32093659
    Iteration 37, loss = 0.31015093
    Iteration 38, loss = 0.30727364
    Iteration 39, loss = 0.30073564
    Iteration 40, loss = 0.30394945
    Iteration 41, loss = 0.31778449
    Iteration 42, loss = 0.32352113
    Iteration 43, loss = 0.31091961
    Iteration 44, loss = 0.30742868
    Iteration 45, loss = 0.31161107
    Iteration 46, loss = 0.30765556
    Iteration 47, loss = 0.30200776
    Iteration 48, loss = 0.30007637
    Iteration 49, loss = 0.30112910
    Iteration 50, loss = 0.30475939
    Iteration 51, loss = 0.30716866
    Iteration 52, loss = 0.31086473
    Iteration 53, loss = 0.31607619
    Iteration 54, loss = 0.31161242
    Iteration 55, loss = 0.30258738
    Iteration 56, loss = 0.29775457
    Iteration 57, loss = 0.29467436
    Iteration 58, loss = 0.29307734
    Iteration 59, loss = 0.29588622
    Iteration 60, loss = 0.30817130
    Iteration 61, loss = 0.30852550
    Iteration 62, loss = 0.30179151
    Iteration 63, loss = 0.29729818
    Iteration 64, loss = 0.29292162
    Iteration 65, loss = 0.29457632
    Iteration 66, loss = 0.30819046
    Iteration 67, loss = 0.30337651
    Iteration 68, loss = 0.30380510
    Iteration 69, loss = 0.30241470
    Iteration 70, loss = 0.30221118
    Iteration 71, loss = 0.29604030
    Iteration 72, loss = 0.29034838
    Iteration 73, loss = 0.29578808
    Iteration 74, loss = 0.30636270
    Iteration 75, loss = 0.30675132
    Iteration 76, loss = 0.29505508
    Iteration 77, loss = 0.29897425
    Iteration 78, loss = 0.30505506
    Iteration 79, loss = 0.30002428
    Iteration 80, loss = 0.28860908
    Iteration 81, loss = 0.28343635
    Iteration 82, loss = 0.29277192
    Iteration 83, loss = 0.30923257
    Iteration 84, loss = 0.30991086
    Iteration 85, loss = 0.29837548
    Iteration 86, loss = 0.28994800
    Iteration 87, loss = 0.29435592
    Iteration 88, loss = 0.30042266
    Iteration 89, loss = 0.31287458
    Iteration 90, loss = 0.30386389
    Iteration 91, loss = 0.29480793
    Iteration 92, loss = 0.28784014
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=2, batch_size=auto, hidden_layer_sizes=(20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=True; total time= 2.6min
    Iteration 1, loss = 0.97361328
    Iteration 2, loss = 0.71740311
    Iteration 3, loss = 0.61301692
    Iteration 4, loss = 0.56451148
    Iteration 5, loss = 0.53133803
    Iteration 6, loss = 0.50921551
    Iteration 7, loss = 0.48881014
    Iteration 8, loss = 0.46815157
    Iteration 9, loss = 0.45593082
    Iteration 10, loss = 0.43171267
    Iteration 11, loss = 0.42163200
    Iteration 12, loss = 0.41495585
    Iteration 13, loss = 0.39903387
    Iteration 14, loss = 0.38572555
    Iteration 15, loss = 0.38220937
    Iteration 16, loss = 0.37456484
    Iteration 17, loss = 0.36867844
    Iteration 18, loss = 0.36383618
    Iteration 19, loss = 0.35531058
    Iteration 20, loss = 0.35374191
    Iteration 21, loss = 0.35864908
    Iteration 22, loss = 0.34546081
    Iteration 23, loss = 0.34035416
    Iteration 24, loss = 0.34041339
    Iteration 25, loss = 0.33904602
    Iteration 26, loss = 0.34377932
    Iteration 27, loss = 0.32969628
    Iteration 28, loss = 0.31480572
    Iteration 29, loss = 0.32120179
    Iteration 30, loss = 0.33141239
    Iteration 31, loss = 0.32374341
    Iteration 32, loss = 0.31863103
    Iteration 33, loss = 0.31645729
    Iteration 34, loss = 0.31152522
    Iteration 35, loss = 0.33611964
    Iteration 36, loss = 0.33380898
    Iteration 37, loss = 0.30960166
    Iteration 38, loss = 0.30559326
    Iteration 39, loss = 0.30400009
    Iteration 40, loss = 0.31026625
    Iteration 41, loss = 0.32246152
    Iteration 42, loss = 0.31606609
    Iteration 43, loss = 0.31188404
    Iteration 44, loss = 0.29769909
    Iteration 45, loss = 0.30911956
    Iteration 46, loss = 0.31266883
    Iteration 47, loss = 0.31030739
    Iteration 48, loss = 0.30401767
    Iteration 49, loss = 0.29083421
    Iteration 50, loss = 0.29168204
    Iteration 51, loss = 0.30576404
    Iteration 52, loss = 0.30892561
    Iteration 53, loss = 0.31083660
    Iteration 54, loss = 0.30115307
    Iteration 55, loss = 0.30983056
    Iteration 56, loss = 0.30751835
    Iteration 57, loss = 0.28904823
    Iteration 58, loss = 0.29674596
    Iteration 59, loss = 0.29774363
    Iteration 60, loss = 0.30750583
    Iteration 61, loss = 0.31492693
    Iteration 62, loss = 0.30351968
    Iteration 63, loss = 0.28552374
    Iteration 64, loss = 0.28677163
    Iteration 65, loss = 0.29054859
    Iteration 66, loss = 0.29743235
    Iteration 67, loss = 0.30388269
    Iteration 68, loss = 0.29234070
    Iteration 69, loss = 0.28394638
    Iteration 70, loss = 0.28784814
    Iteration 71, loss = 0.30578693
    Iteration 72, loss = 0.30221271
    Iteration 73, loss = 0.30031052
    Iteration 74, loss = 0.29137421
    Iteration 75, loss = 0.29247308
    Iteration 76, loss = 0.28509422
    Iteration 77, loss = 0.27886330
    Iteration 78, loss = 0.27932853
    Iteration 79, loss = 0.29471879
    Iteration 80, loss = 0.30909982
    Iteration 81, loss = 0.30424042
    Iteration 82, loss = 0.28637139
    Iteration 83, loss = 0.27693954
    Iteration 84, loss = 0.27884157
    Iteration 85, loss = 0.28921846
    Iteration 86, loss = 0.29294159
    Iteration 87, loss = 0.29908038
    Iteration 88, loss = 0.29048145
    Iteration 89, loss = 0.28584499
    Iteration 90, loss = 0.28886725
    Iteration 91, loss = 0.28955973
    Iteration 92, loss = 0.28508750
    Iteration 93, loss = 0.29093640
    Iteration 94, loss = 0.28825537
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=2, batch_size=auto, hidden_layer_sizes=(20, 20, 20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=True; total time= 2.9min
    Iteration 1, loss = 0.97542117
    Iteration 2, loss = 0.71948027
    Iteration 3, loss = 0.61760158
    Iteration 4, loss = 0.56995391
    Iteration 5, loss = 0.53976187
    Iteration 6, loss = 0.51983956
    Iteration 7, loss = 0.50277665
    Iteration 8, loss = 0.48547339
    Iteration 9, loss = 0.46871263
    Iteration 10, loss = 0.44816565
    Iteration 11, loss = 0.44122960
    Iteration 12, loss = 0.42391904
    Iteration 13, loss = 0.41509641
    Iteration 14, loss = 0.40125579
    Iteration 15, loss = 0.39240185
    Iteration 16, loss = 0.38443181
    Iteration 17, loss = 0.38321550
    Iteration 18, loss = 0.37797751
    Iteration 19, loss = 0.36900871
    Iteration 20, loss = 0.36370201
    Iteration 21, loss = 0.35278091
    Iteration 22, loss = 0.35190399
    Iteration 23, loss = 0.35320945
    Iteration 24, loss = 0.35018542
    Iteration 25, loss = 0.34052703
    Iteration 26, loss = 0.33736510
    Iteration 27, loss = 0.34158064
    Iteration 28, loss = 0.33244595
    Iteration 29, loss = 0.32575307
    Iteration 30, loss = 0.33018288
    Iteration 31, loss = 0.32706754
    Iteration 32, loss = 0.33116803
    Iteration 33, loss = 0.32373287
    Iteration 34, loss = 0.32004614
    Iteration 35, loss = 0.33262457
    Iteration 36, loss = 0.31930249
    Iteration 37, loss = 0.31962642
    Iteration 38, loss = 0.32523520
    Iteration 39, loss = 0.31792432
    Iteration 40, loss = 0.30865697
    Iteration 41, loss = 0.30146143
    Iteration 42, loss = 0.30236168
    Iteration 43, loss = 0.33103449
    Iteration 44, loss = 0.31237042
    Iteration 45, loss = 0.30428290
    Iteration 46, loss = 0.30078259
    Iteration 47, loss = 0.31403333
    Iteration 48, loss = 0.32080579
    Iteration 49, loss = 0.30789634
    Iteration 50, loss = 0.30148219
    Iteration 51, loss = 0.29587609
    Iteration 52, loss = 0.30387806
    Iteration 53, loss = 0.31685662
    Iteration 54, loss = 0.30909187
    Iteration 55, loss = 0.30189936
    Iteration 56, loss = 0.29715548
    Iteration 57, loss = 0.29230332
    Iteration 58, loss = 0.29252785
    Iteration 59, loss = 0.29643650
    Iteration 60, loss = 0.30188040
    Iteration 61, loss = 0.30702127
    Iteration 62, loss = 0.29836178
    Iteration 63, loss = 0.29927745
    Iteration 64, loss = 0.29223656
    Iteration 65, loss = 0.28614556
    Iteration 66, loss = 0.29246645
    Iteration 67, loss = 0.30724265
    Iteration 68, loss = 0.30540191
    Iteration 69, loss = 0.29738981
    Iteration 70, loss = 0.28965675
    Iteration 71, loss = 0.29167486
    Iteration 72, loss = 0.29405245
    Iteration 73, loss = 0.30339758
    Iteration 74, loss = 0.31232916
    Iteration 75, loss = 0.30019472
    Iteration 76, loss = 0.28645998
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=2, batch_size=auto, hidden_layer_sizes=(20, 20, 20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=True; total time= 2.3min
    Iteration 1, loss = 2.93596926
    Iteration 2, loss = 1.55631227
    Iteration 3, loss = 1.03846281
    Iteration 4, loss = 0.82998402
    Iteration 5, loss = 0.74279997
    Iteration 6, loss = 0.70672650
    Iteration 7, loss = 0.69670001
    Iteration 8, loss = 0.69385601
    Iteration 9, loss = 0.69298119
    Iteration 10, loss = 0.69254020
    Iteration 11, loss = 0.69228923
    Iteration 12, loss = 0.69221853
    Iteration 13, loss = 0.69212129
    Iteration 14, loss = 0.69203203
    Iteration 15, loss = 0.69197188
    Iteration 16, loss = 0.69194754
    Iteration 17, loss = 0.69192037
    Iteration 18, loss = 0.69190912
    Iteration 19, loss = 0.69188356
    Iteration 20, loss = 0.69187562
    Iteration 21, loss = 0.69194278
    Iteration 22, loss = 0.69188197
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=15, batch_size=auto, hidden_layer_sizes=(20, 20, 20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=True; total time=  38.8s
    Iteration 1, loss = 0.82740499
    Iteration 2, loss = 0.64588678
    Iteration 3, loss = 0.58123565
    Iteration 4, loss = 0.54818809
    Iteration 5, loss = 0.52783426
    Iteration 6, loss = 0.51012256
    Iteration 7, loss = 0.49277952
    Iteration 8, loss = 0.47850672
    Iteration 9, loss = 0.46698018
    Iteration 10, loss = 0.45005142
    Iteration 11, loss = 0.43777341
    Iteration 12, loss = 0.42820782
    Iteration 13, loss = 0.41723816
    Iteration 14, loss = 0.39979327
    Iteration 15, loss = 0.39118963
    Iteration 16, loss = 0.39625163
    Iteration 17, loss = 0.37552826
    Iteration 18, loss = 0.36076456
    Iteration 19, loss = 0.35818242
    Iteration 20, loss = 0.35886375
    Iteration 21, loss = 0.35485110
    Iteration 22, loss = 0.34243343
    Iteration 23, loss = 0.33975237
    Iteration 24, loss = 0.34261061
    Iteration 25, loss = 0.33916073
    Iteration 26, loss = 0.33426385
    Iteration 27, loss = 0.33012918
    Iteration 28, loss = 0.32466238
    Iteration 29, loss = 0.32320949
    Iteration 30, loss = 0.32324221
    Iteration 31, loss = 0.32128618
    Iteration 32, loss = 0.32823819
    Iteration 33, loss = 0.32090272
    Iteration 34, loss = 0.31553659
    Iteration 35, loss = 0.32455432
    Iteration 36, loss = 0.31948430
    Iteration 37, loss = 0.31256241
    Iteration 38, loss = 0.31454795
    Iteration 39, loss = 0.31773522
    Iteration 40, loss = 0.32084836
    Iteration 41, loss = 0.31619128
    Iteration 42, loss = 0.30637993
    Iteration 43, loss = 0.31016885
    Iteration 44, loss = 0.30479711
    Iteration 45, loss = 0.30459075
    Iteration 46, loss = 0.31651734
    Iteration 47, loss = 0.31178251
    Iteration 48, loss = 0.31268390
    Iteration 49, loss = 0.31275050
    Iteration 50, loss = 0.30743169
    Iteration 51, loss = 0.30431736
    Iteration 52, loss = 0.29538234
    Iteration 53, loss = 0.30194242
    Iteration 54, loss = 0.30431553
    Iteration 55, loss = 0.29880412
    Iteration 56, loss = 0.29647024
    Iteration 57, loss = 0.31251727
    Iteration 58, loss = 0.31236523
    Iteration 59, loss = 0.30520403
    Iteration 60, loss = 0.30221865
    Iteration 61, loss = 0.30059861
    Iteration 62, loss = 0.30146802
    Iteration 63, loss = 0.30153097
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=2, batch_size=auto, hidden_layer_sizes=(20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=True; total time= 1.7min
    Iteration 1, loss = 2.93725920
    Iteration 2, loss = 1.55793003
    Iteration 3, loss = 1.03831293
    Iteration 4, loss = 0.82887300
    Iteration 5, loss = 0.74014060
    Iteration 6, loss = 0.70580615
    Iteration 7, loss = 0.69635097
    Iteration 8, loss = 0.69370519
    Iteration 9, loss = 0.69284364
    Iteration 10, loss = 0.69247041
    Iteration 11, loss = 0.69227424
    Iteration 12, loss = 0.69216542
    Iteration 13, loss = 0.69205379
    Iteration 14, loss = 0.69203375
    Iteration 15, loss = 0.69194819
    Iteration 16, loss = 0.69199509
    Iteration 17, loss = 0.69194551
    Iteration 18, loss = 0.69190395
    Iteration 19, loss = 0.69190184
    Iteration 20, loss = 0.69187995
    Iteration 21, loss = 0.69185828
    Iteration 22, loss = 0.69186545
    Iteration 23, loss = 0.69187175
    Iteration 24, loss = 0.69187443
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=15, batch_size=auto, hidden_layer_sizes=(20, 20, 20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=True; total time=  43.6s
    Iteration 1, loss = 1.89911871
    Iteration 2, loss = 1.12314262
    Iteration 3, loss = 0.86266738
    Iteration 4, loss = 0.76007035
    Iteration 5, loss = 0.72182507
    Iteration 6, loss = 0.70807760
    Iteration 7, loss = 0.70308243
    Iteration 8, loss = 0.70108446
    Iteration 9, loss = 0.69990605
    Iteration 10, loss = 0.69937735
    Iteration 11, loss = 0.69900004
    Iteration 12, loss = 0.69876667
    Iteration 13, loss = 0.69884562
    Iteration 14, loss = 0.69867004
    Iteration 15, loss = 0.69869142
    Iteration 16, loss = 0.69860741
    Iteration 17, loss = 0.69861302
    Iteration 18, loss = 0.69876690
    Iteration 19, loss = 0.69863906
    Iteration 20, loss = 0.69877674
    Iteration 21, loss = 0.69873997
    Iteration 22, loss = 0.69901450
    Iteration 23, loss = 0.69880441
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=15, batch_size=auto, hidden_layer_sizes=(20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=True; total time=  34.3s
    Iteration 1, loss = 0.82649444
    Iteration 2, loss = 0.64969114
    Iteration 3, loss = 0.58309404
    Iteration 4, loss = 0.55030478
    Iteration 5, loss = 0.52553706
    Iteration 6, loss = 0.50495873
    Iteration 7, loss = 0.48579515
    Iteration 8, loss = 0.47061640
    Iteration 9, loss = 0.45489601
    Iteration 10, loss = 0.44011582
    Iteration 11, loss = 0.42690859
    Iteration 12, loss = 0.40776392
    Iteration 13, loss = 0.39967161
    Iteration 14, loss = 0.39184700
    Iteration 15, loss = 0.37480206
    Iteration 16, loss = 0.36198568
    Iteration 17, loss = 0.37251516
    Iteration 18, loss = 0.36051779
    Iteration 19, loss = 0.34710106
    Iteration 20, loss = 0.34423987
    Iteration 21, loss = 0.34041905
    Iteration 22, loss = 0.33032898
    Iteration 23, loss = 0.32911463
    Iteration 24, loss = 0.33376981
    Iteration 25, loss = 0.32521718
    Iteration 26, loss = 0.32885510
    Iteration 27, loss = 0.32124626
    Iteration 28, loss = 0.31142250
    Iteration 29, loss = 0.31316655
    Iteration 30, loss = 0.31917881
    Iteration 31, loss = 0.31751897
    Iteration 32, loss = 0.32332536
    Iteration 33, loss = 0.32236213
    Iteration 34, loss = 0.31407133
    Iteration 35, loss = 0.31026036
    Iteration 36, loss = 0.30029855
    Iteration 37, loss = 0.30437486
    Iteration 38, loss = 0.32765125
    Iteration 39, loss = 0.32259971
    Iteration 40, loss = 0.30754173
    Iteration 41, loss = 0.30151234
    Iteration 42, loss = 0.30597297
    Iteration 43, loss = 0.32047507
    Iteration 44, loss = 0.31059166
    Iteration 45, loss = 0.29608344
    Iteration 46, loss = 0.29693819
    Iteration 47, loss = 0.30511174
    Iteration 48, loss = 0.31778391
    Iteration 49, loss = 0.31451543
    Iteration 50, loss = 0.30127744
    Iteration 51, loss = 0.29230491
    Iteration 52, loss = 0.29942175
    Iteration 53, loss = 0.30108144
    Iteration 54, loss = 0.30540288
    Iteration 55, loss = 0.30284450
    Iteration 56, loss = 0.30774718
    Iteration 57, loss = 0.30415789
    Iteration 58, loss = 0.29923173
    Iteration 59, loss = 0.29635822
    Iteration 60, loss = 0.30644365
    Iteration 61, loss = 0.30501456
    Iteration 62, loss = 0.30612581
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=2, batch_size=auto, hidden_layer_sizes=(20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=True; total time= 1.7min
    Iteration 1, loss = 2.93756276
    Iteration 2, loss = 1.55701346
    Iteration 3, loss = 1.03620869
    Iteration 4, loss = 0.82610226
    Iteration 5, loss = 0.73719176
    Iteration 6, loss = 0.70498631
    Iteration 7, loss = 0.69607397
    Iteration 8, loss = 0.69359236
    Iteration 9, loss = 0.69278219
    Iteration 10, loss = 0.69241241
    Iteration 11, loss = 0.69227411
    Iteration 12, loss = 0.69213125
    Iteration 13, loss = 0.69205628
    Iteration 14, loss = 0.69202908
    Iteration 15, loss = 0.69196885
    Iteration 16, loss = 0.69198089
    Iteration 17, loss = 0.69193321
    Iteration 18, loss = 0.69192984
    Iteration 19, loss = 0.69193008
    Iteration 20, loss = 0.69187268
    Iteration 21, loss = 0.69185438
    Iteration 22, loss = 0.69185122
    Iteration 23, loss = 0.69187498
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=15, batch_size=auto, hidden_layer_sizes=(20, 20, 20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=True; total time=  41.7s
    Iteration 1, loss = 1.89795258
    Iteration 2, loss = 1.12308022
    Iteration 3, loss = 0.86441013
    Iteration 4, loss = 0.76151420
    Iteration 5, loss = 0.72278837
    Iteration 6, loss = 0.70882494
    Iteration 7, loss = 0.70326707
    Iteration 8, loss = 0.70123136
    Iteration 9, loss = 0.70015571
    Iteration 10, loss = 0.69964760
    Iteration 11, loss = 0.69912067
    Iteration 12, loss = 0.69905158
    Iteration 13, loss = 0.69913023
    Iteration 14, loss = 0.69883089
    Iteration 15, loss = 0.69897874
    Iteration 16, loss = 0.69887324
    Iteration 17, loss = 0.69896190
    Iteration 18, loss = 0.69897401
    Iteration 19, loss = 0.69891881
    Iteration 20, loss = 0.69904493
    Iteration 21, loss = 0.69905061
    Iteration 22, loss = 0.69911478
    Iteration 23, loss = 0.69884925
    Iteration 24, loss = 0.69912858
    Iteration 25, loss = 0.69925854
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=15, batch_size=auto, hidden_layer_sizes=(20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=True; total time=  37.8s
    Iteration 1, loss = 0.82540952
    Iteration 2, loss = 0.64805157
    Iteration 3, loss = 0.58211620
    Iteration 4, loss = 0.54696295
    Iteration 5, loss = 0.52412253
    Iteration 6, loss = 0.50717218
    Iteration 7, loss = 0.49060328
    Iteration 8, loss = 0.47620233
    Iteration 9, loss = 0.45298709
    Iteration 10, loss = 0.44087617
    Iteration 11, loss = 0.42600006
    Iteration 12, loss = 0.40838546
    Iteration 13, loss = 0.39494180
    Iteration 14, loss = 0.38793892
    Iteration 15, loss = 0.37832659
    Iteration 16, loss = 0.36287449
    Iteration 17, loss = 0.36515337
    Iteration 18, loss = 0.36071691
    Iteration 19, loss = 0.34998629
    Iteration 20, loss = 0.35269760
    Iteration 21, loss = 0.34675755
    Iteration 22, loss = 0.34367977
    Iteration 23, loss = 0.34161236
    Iteration 24, loss = 0.33439721
    Iteration 25, loss = 0.33080060
    Iteration 26, loss = 0.33602142
    Iteration 27, loss = 0.32443131
    Iteration 28, loss = 0.31758629
    Iteration 29, loss = 0.32444303
    Iteration 30, loss = 0.32400574
    Iteration 31, loss = 0.32718273
    Iteration 32, loss = 0.32183623
    Iteration 33, loss = 0.31605091
    Iteration 34, loss = 0.32496470
    Iteration 35, loss = 0.32624698
    Iteration 36, loss = 0.31537107
    Iteration 37, loss = 0.32197145
    Iteration 38, loss = 0.31632433
    Iteration 39, loss = 0.31119298
    Iteration 40, loss = 0.30803291
    Iteration 41, loss = 0.30801192
    Iteration 42, loss = 0.32107156
    Iteration 43, loss = 0.31491146
    Iteration 44, loss = 0.31178583
    Iteration 45, loss = 0.30821915
    Iteration 46, loss = 0.30426946
    Iteration 47, loss = 0.31197750
    Iteration 48, loss = 0.32151639
    Iteration 49, loss = 0.32119046
    Iteration 50, loss = 0.30544194
    Iteration 51, loss = 0.30517641
    Iteration 52, loss = 0.30619184
    Iteration 53, loss = 0.31134838
    Iteration 54, loss = 0.31825858
    Iteration 55, loss = 0.31335210
    Iteration 56, loss = 0.31114287
    Iteration 57, loss = 0.29933111
    Iteration 58, loss = 0.29840127
    Iteration 59, loss = 0.30246062
    Iteration 60, loss = 0.30290226
    Iteration 61, loss = 0.30576269
    Iteration 62, loss = 0.30876920
    Iteration 63, loss = 0.31597809
    Iteration 64, loss = 0.31325463
    Iteration 65, loss = 0.30569901
    Iteration 66, loss = 0.29840254
    Iteration 67, loss = 0.29829764
    Iteration 68, loss = 0.30651800
    Iteration 69, loss = 0.31007327
    Iteration 70, loss = 0.31068573
    Iteration 71, loss = 0.30614833
    Iteration 72, loss = 0.30123143
    Iteration 73, loss = 0.30193165
    Iteration 74, loss = 0.30183960
    Iteration 75, loss = 0.30171431
    Iteration 76, loss = 0.29560914
    Iteration 77, loss = 0.29751095
    Iteration 78, loss = 0.31099946
    Iteration 79, loss = 0.30847648
    Iteration 80, loss = 0.30836531
    Iteration 81, loss = 0.29083869
    Iteration 82, loss = 0.28833910
    Iteration 83, loss = 0.29380078
    Iteration 84, loss = 0.31094033
    Iteration 85, loss = 0.31102089
    Iteration 86, loss = 0.30147802
    Iteration 87, loss = 0.29354587
    Iteration 88, loss = 0.28854855
    Iteration 89, loss = 0.30076955
    Iteration 90, loss = 0.31252703
    Iteration 91, loss = 0.30714492
    Iteration 92, loss = 0.30154056
    Iteration 93, loss = 0.28771169
    Iteration 94, loss = 0.28850996
    Iteration 95, loss = 0.30828136
    Iteration 96, loss = 0.30784149
    Iteration 97, loss = 0.30180749
    Iteration 98, loss = 0.29901868
    Iteration 99, loss = 0.30389196
    Iteration 100, loss = 0.30842023
    Iteration 101, loss = 0.30836836
    Iteration 102, loss = 0.30368654
    Iteration 103, loss = 0.28914021
    Iteration 104, loss = 0.28636242
    Iteration 105, loss = 0.28728576
    Iteration 106, loss = 0.29445467
    Iteration 107, loss = 0.30095033
    Iteration 108, loss = 0.31018010
    Iteration 109, loss = 0.31093161
    Iteration 110, loss = 0.30218077
    Iteration 111, loss = 0.29298089
    Iteration 112, loss = 0.29082981
    Iteration 113, loss = 0.29372512
    Iteration 114, loss = 0.29767120
    Iteration 115, loss = 0.30412373
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=2, batch_size=auto, hidden_layer_sizes=(20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=True; total time= 3.1min
    Iteration 1, loss = 0.97594132
    Iteration 2, loss = 0.72275339
    Iteration 3, loss = 0.62412867
    Iteration 4, loss = 0.57535276
    Iteration 5, loss = 0.54968683
    Iteration 6, loss = 0.52955136
    Iteration 7, loss = 0.51347876
    Iteration 8, loss = 0.49586573
    Iteration 9, loss = 0.48091350
    Iteration 10, loss = 0.46662179
    Iteration 11, loss = 0.45786503
    Iteration 12, loss = 0.44009923
    Iteration 13, loss = 0.42437730
    Iteration 14, loss = 0.41244759
    Iteration 15, loss = 0.40164164
    Iteration 16, loss = 0.40732394
    Iteration 17, loss = 0.39781624
    Iteration 18, loss = 0.37440687
    Iteration 19, loss = 0.37965827
    Iteration 20, loss = 0.37794523
    Iteration 21, loss = 0.37273141
    Iteration 22, loss = 0.36286020
    Iteration 23, loss = 0.36788669
    Iteration 24, loss = 0.35212801
    Iteration 25, loss = 0.34217393
    Iteration 26, loss = 0.34567008
    Iteration 27, loss = 0.34536375
    Iteration 28, loss = 0.34102859
    Iteration 29, loss = 0.34891184
    Iteration 30, loss = 0.34729567
    Iteration 31, loss = 0.33778607
    Iteration 32, loss = 0.33442714
    Iteration 33, loss = 0.33199639
    Iteration 34, loss = 0.33537192
    Iteration 35, loss = 0.32476337
    Iteration 36, loss = 0.32403533
    Iteration 37, loss = 0.33242199
    Iteration 38, loss = 0.32533930
    Iteration 39, loss = 0.32814305
    Iteration 40, loss = 0.31098342
    Iteration 41, loss = 0.30836852
    Iteration 42, loss = 0.31956477
    Iteration 43, loss = 0.32002200
    Iteration 44, loss = 0.31260946
    Iteration 45, loss = 0.31669156
    Iteration 46, loss = 0.31582555
    Iteration 47, loss = 0.31118557
    Iteration 48, loss = 0.31611897
    Iteration 49, loss = 0.32334858
    Iteration 50, loss = 0.31293479
    Iteration 51, loss = 0.30543775
    Iteration 52, loss = 0.30700634
    Iteration 53, loss = 0.31820453
    Iteration 54, loss = 0.31625041
    Iteration 55, loss = 0.29963288
    Iteration 56, loss = 0.29801115
    Iteration 57, loss = 0.30307396
    Iteration 58, loss = 0.31435550
    Iteration 59, loss = 0.31500370
    Iteration 60, loss = 0.30755862
    Iteration 61, loss = 0.30350605
    Iteration 62, loss = 0.29325501
    Iteration 63, loss = 0.29729813
    Iteration 64, loss = 0.30977083
    Iteration 65, loss = 0.31407325
    Iteration 66, loss = 0.30021278
    Iteration 67, loss = 0.30159637
    Iteration 68, loss = 0.30589338
    Iteration 69, loss = 0.30899451
    Iteration 70, loss = 0.30004394
    Iteration 71, loss = 0.29214888
    Iteration 72, loss = 0.28752294
    Iteration 73, loss = 0.30274401
    Iteration 74, loss = 0.30998165
    Iteration 75, loss = 0.29610602
    Iteration 76, loss = 0.28843049
    Iteration 77, loss = 0.29047357
    Iteration 78, loss = 0.29764047
    Iteration 79, loss = 0.30459377
    Iteration 80, loss = 0.30668129
    Iteration 81, loss = 0.29647710
    Iteration 82, loss = 0.29335221
    Iteration 83, loss = 0.29947192
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=2, batch_size=auto, hidden_layer_sizes=(20, 20, 20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=True; total time= 2.5min
    Iteration 1, loss = 1.89843297
    Iteration 2, loss = 1.12320100
    Iteration 3, loss = 0.86140510
    Iteration 4, loss = 0.75847397
    Iteration 5, loss = 0.71998456
    Iteration 6, loss = 0.70666296
    Iteration 7, loss = 0.70195842
    Iteration 8, loss = 0.69989938
    Iteration 9, loss = 0.69919346
    Iteration 10, loss = 0.69846587
    Iteration 11, loss = 0.69834956
    Iteration 12, loss = 0.69805927
    Iteration 13, loss = 0.69804730
    Iteration 14, loss = 0.69787117
    Iteration 15, loss = 0.69788569
    Iteration 16, loss = 0.69781318
    Iteration 17, loss = 0.69794914
    Iteration 18, loss = 0.69792830
    Iteration 19, loss = 0.69808605
    Iteration 20, loss = 0.69798035
    Iteration 21, loss = 0.69803114
    Iteration 22, loss = 0.69805099
    Iteration 23, loss = 0.69801137
    Iteration 24, loss = 0.69811828
    Iteration 25, loss = 0.69814607
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=15, batch_size=auto, hidden_layer_sizes=(20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=True; total time=  34.2s
    Iteration 1, loss = 0.98272990
    Iteration 2, loss = 0.72220917
    Iteration 3, loss = 0.62228836
    Iteration 4, loss = 0.57393519
    Iteration 5, loss = 0.54332156
    Iteration 6, loss = 0.52292738
    Iteration 7, loss = 0.50261385
    Iteration 8, loss = 0.49121687
    Iteration 9, loss = 0.47811811
    Iteration 10, loss = 0.44889890
    Iteration 11, loss = 0.45054573
    Iteration 12, loss = 0.42560216
    Iteration 13, loss = 0.42246416
    Iteration 14, loss = 0.41035401
    Iteration 15, loss = 0.39637294
    Iteration 16, loss = 0.38948716
    Iteration 17, loss = 0.38735585
    Iteration 18, loss = 0.37443944
    Iteration 19, loss = 0.37336033
    Iteration 20, loss = 0.36696359
    Iteration 21, loss = 0.34920456
    Iteration 22, loss = 0.35701607
    Iteration 23, loss = 0.35770637
    Iteration 24, loss = 0.34800748
    Iteration 25, loss = 0.33897478
    Iteration 26, loss = 0.32991995
    Iteration 27, loss = 0.35197373
    Iteration 28, loss = 0.35074589
    Iteration 29, loss = 0.32134080
    Iteration 30, loss = 0.31836142
    Iteration 31, loss = 0.32472862
    Iteration 32, loss = 0.32243775
    Iteration 33, loss = 0.33058249
    Iteration 34, loss = 0.33284587
    Iteration 35, loss = 0.32433352
    Iteration 36, loss = 0.31176116
    Iteration 37, loss = 0.31244971
    Iteration 38, loss = 0.31915597
    Iteration 39, loss = 0.31836467
    Iteration 40, loss = 0.31238696
    Iteration 41, loss = 0.31234349
    Iteration 42, loss = 0.31139418
    Iteration 43, loss = 0.31323135
    Iteration 44, loss = 0.30828482
    Iteration 45, loss = 0.30374796
    Iteration 46, loss = 0.30864174
    Iteration 47, loss = 0.31563920
    Iteration 48, loss = 0.31153641
    Iteration 49, loss = 0.30505257
    Iteration 50, loss = 0.29354002
    Iteration 51, loss = 0.31607718
    Iteration 52, loss = 0.32170403
    Iteration 53, loss = 0.30779503
    Iteration 54, loss = 0.30820022
    Iteration 55, loss = 0.30289720
    Iteration 56, loss = 0.30121781
    Iteration 57, loss = 0.30032131
    Iteration 58, loss = 0.29794913
    Iteration 59, loss = 0.29718116
    Iteration 60, loss = 0.29847097
    Iteration 61, loss = 0.30516558
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=2, batch_size=auto, hidden_layer_sizes=(20, 20, 20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=True; total time= 1.9min
    Iteration 1, loss = 2.93937073
    Iteration 2, loss = 1.55710054
    Iteration 3, loss = 1.03841732
    Iteration 4, loss = 0.82854761
    Iteration 5, loss = 0.74010106
    Iteration 6, loss = 0.70584692
    Iteration 7, loss = 0.69639289
    Iteration 8, loss = 0.69369212
    Iteration 9, loss = 0.69286921
    Iteration 10, loss = 0.69249748
    Iteration 11, loss = 0.69226142
    Iteration 12, loss = 0.69217711
    Iteration 13, loss = 0.69208549
    Iteration 14, loss = 0.69202315
    Iteration 15, loss = 0.69194023
    Iteration 16, loss = 0.69195944
    Iteration 17, loss = 0.69192372
    Iteration 18, loss = 0.69190145
    Iteration 19, loss = 0.69189778
    Iteration 20, loss = 0.69186609
    Iteration 21, loss = 0.69188849
    Iteration 22, loss = 0.69186075
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=15, batch_size=auto, hidden_layer_sizes=(20, 20, 20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=True; total time=  40.2s
    Iteration 1, loss = 1.90042229
    Iteration 2, loss = 1.12492020
    Iteration 3, loss = 0.86446484
    Iteration 4, loss = 0.76112468
    Iteration 5, loss = 0.72250418
    Iteration 6, loss = 0.70852151
    Iteration 7, loss = 0.70346810
    Iteration 8, loss = 0.70119151
    Iteration 9, loss = 0.70029130
    Iteration 10, loss = 0.69942385
    Iteration 11, loss = 0.69893575
    Iteration 12, loss = 0.69899771
    Iteration 13, loss = 0.69882676
    Iteration 14, loss = 0.69876203
    Iteration 15, loss = 0.69864636
    Iteration 16, loss = 0.69874876
    Iteration 17, loss = 0.69891406
    Iteration 18, loss = 0.69872276
    Iteration 19, loss = 0.69873317
    Iteration 20, loss = 0.69875400
    Iteration 21, loss = 0.69877178
    Iteration 22, loss = 0.69904532
    Iteration 23, loss = 0.69891469
    Iteration 24, loss = 0.69895295
    Iteration 25, loss = 0.69894094
    Iteration 26, loss = 0.69900521
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=15, batch_size=auto, hidden_layer_sizes=(20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=True; total time=  35.3s
    Iteration 1, loss = 2.19247402
    Iteration 2, loss = 1.27110599
    Iteration 3, loss = 0.93211423
    Iteration 4, loss = 0.79980324
    Iteration 5, loss = 0.75140740
    Iteration 6, loss = 0.73497388
    Iteration 7, loss = 0.72922396
    Iteration 8, loss = 0.72632749
    Iteration 9, loss = 0.72419434
    Iteration 10, loss = 0.72225851
    Iteration 11, loss = 0.72124894
    Iteration 12, loss = 0.72018045
    Iteration 13, loss = 0.71937776
    Iteration 14, loss = 0.71872412
    Iteration 15, loss = 0.71749664
    Iteration 16, loss = 0.71478030
    Iteration 17, loss = 0.70608044
    Iteration 18, loss = 0.69797313
    Iteration 19, loss = 0.69488123
    Iteration 20, loss = 0.69363840
    Iteration 21, loss = 0.69304301
    Iteration 22, loss = 0.69270907
    Iteration 23, loss = 0.69253669
    Iteration 24, loss = 0.69236576
    Iteration 25, loss = 0.69226313
    Iteration 26, loss = 0.69220259
    Iteration 27, loss = 0.69210483
    Iteration 28, loss = 0.69211335
    Iteration 29, loss = 0.69209447
    Iteration 30, loss = 0.69201824
    Iteration 31, loss = 0.69199141
    Iteration 32, loss = 0.69194309
    Iteration 33, loss = 0.69195686
    Iteration 34, loss = 0.69189879
    Iteration 35, loss = 0.69196416
    Iteration 36, loss = 0.69191414
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=10, batch_size=auto, hidden_layer_sizes=(20, 20, 20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=True; total time= 1.1min
    Iteration 1, loss = 1.49434302
    Iteration 2, loss = 0.96659078
    Iteration 3, loss = 0.79510837
    Iteration 4, loss = 0.72709978
    Iteration 5, loss = 0.70077570
    Iteration 6, loss = 0.69258144
    Iteration 7, loss = 0.68866928
    Iteration 8, loss = 0.68774916
    Iteration 9, loss = 0.68617178
    Iteration 10, loss = 0.68516968
    Iteration 11, loss = 0.68510484
    Iteration 12, loss = 0.68540729
    Iteration 13, loss = 0.68456080
    Iteration 14, loss = 0.68520587
    Iteration 15, loss = 0.68324793
    Iteration 16, loss = 0.68322620
    Iteration 17, loss = 0.68232799
    Iteration 18, loss = 0.68274405
    Iteration 19, loss = 0.68183655
    Iteration 20, loss = 0.68220209
    Iteration 21, loss = 0.68096241
    Iteration 22, loss = 0.68099708
    Iteration 23, loss = 0.68138062
    Iteration 24, loss = 0.68174130
    Iteration 25, loss = 0.68136468
    Iteration 26, loss = 0.68042518
    Iteration 27, loss = 0.67949317
    Iteration 28, loss = 0.67982077
    Iteration 29, loss = 0.67905390
    Iteration 30, loss = 0.67874299
    Iteration 31, loss = 0.67895807
    Iteration 32, loss = 0.67806824
    Iteration 33, loss = 0.67871650
    Iteration 34, loss = 0.67833717
    Iteration 35, loss = 0.67705899
    Iteration 36, loss = 0.67817443
    Iteration 37, loss = 0.67685748
    Iteration 38, loss = 0.67601376
    Iteration 39, loss = 0.67583244
    Iteration 40, loss = 0.67676789
    Iteration 41, loss = 0.67597644
    Iteration 42, loss = 0.67618952
    Iteration 43, loss = 0.67476397
    Iteration 44, loss = 0.67436229
    Iteration 45, loss = 0.67404029
    Iteration 46, loss = 0.67362565
    Iteration 47, loss = 0.67354406
    Iteration 48, loss = 0.67408849
    Iteration 49, loss = 0.67350859
    Iteration 50, loss = 0.67193167
    Iteration 51, loss = 0.67291297
    Iteration 52, loss = 0.67213677
    Iteration 53, loss = 0.67179370
    Iteration 54, loss = 0.67217343
    Iteration 55, loss = 0.67089950
    Iteration 56, loss = 0.67123534
    Iteration 57, loss = 0.67029028
    Iteration 58, loss = 0.67116895
    Iteration 59, loss = 0.67102640
    Iteration 60, loss = 0.66966527
    Iteration 61, loss = 0.66986386
    Iteration 62, loss = 0.67075807
    Iteration 63, loss = 0.66965464
    Iteration 64, loss = 0.66972157
    Iteration 65, loss = 0.66944181
    Iteration 66, loss = 0.67046072
    Iteration 67, loss = 0.66995653
    Iteration 68, loss = 0.66932844
    Iteration 69, loss = 0.66995898
    Iteration 70, loss = 0.66856849
    Iteration 71, loss = 0.66794226
    Iteration 72, loss = 0.66923822
    Iteration 73, loss = 0.66793815
    Iteration 74, loss = 0.66781572
    Iteration 75, loss = 0.66763327
    Iteration 76, loss = 0.66810279
    Iteration 77, loss = 0.66773741
    Iteration 78, loss = 0.66807481
    Iteration 79, loss = 0.66779016
    Iteration 80, loss = 0.66688860
    Iteration 81, loss = 0.66785671
    Iteration 82, loss = 0.66835074
    Iteration 83, loss = 0.66662718
    Iteration 84, loss = 0.66731796
    Iteration 85, loss = 0.66645638
    Iteration 86, loss = 0.66643609
    Iteration 87, loss = 0.66706151
    Iteration 88, loss = 0.66524277
    Iteration 89, loss = 0.66711776
    Iteration 90, loss = 0.66664295
    Iteration 91, loss = 0.66722467
    Iteration 92, loss = 0.66621910
    Iteration 93, loss = 0.66638807
    Iteration 94, loss = 0.66648508
    Iteration 95, loss = 0.66551577
    Iteration 96, loss = 0.66569751
    Iteration 97, loss = 0.66587056
    Iteration 98, loss = 0.66624940
    Iteration 99, loss = 0.66656787
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=10, batch_size=auto, hidden_layer_sizes=(20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=True; total time= 2.2min
    Iteration 1, loss = 2.19013573
    Iteration 2, loss = 1.26768243
    Iteration 3, loss = 0.93083302
    Iteration 4, loss = 0.79981186
    Iteration 5, loss = 0.75174412
    Iteration 6, loss = 0.73668524
    Iteration 7, loss = 0.73179667
    Iteration 8, loss = 0.72940337
    Iteration 9, loss = 0.72865036
    Iteration 10, loss = 0.72826780
    Iteration 11, loss = 0.72750410
    Iteration 12, loss = 0.72801073
    Iteration 13, loss = 0.72680445
    Iteration 14, loss = 0.72713009
    Iteration 15, loss = 0.72656514
    Iteration 16, loss = 0.72706766
    Iteration 17, loss = 0.72710149
    Iteration 18, loss = 0.72734331
    Iteration 19, loss = 0.72655078
    Iteration 20, loss = 0.72680468
    Iteration 21, loss = 0.72737689
    Iteration 22, loss = 0.72657609
    Iteration 23, loss = 0.72741942
    Iteration 24, loss = 0.72659892
    Iteration 25, loss = 0.72664810
    Iteration 26, loss = 0.72746771
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=10, batch_size=auto, hidden_layer_sizes=(20, 20, 20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=True; total time=  48.0s
    Iteration 1, loss = 1.49358838
    Iteration 2, loss = 0.96761801
    Iteration 3, loss = 0.79674255
    Iteration 4, loss = 0.72886739
    Iteration 5, loss = 0.70292379
    Iteration 6, loss = 0.69506822
    Iteration 7, loss = 0.69005632
    Iteration 8, loss = 0.69065025
    Iteration 9, loss = 0.68660648
    Iteration 10, loss = 0.68688151
    Iteration 11, loss = 0.68634663
    Iteration 12, loss = 0.68628524
    Iteration 13, loss = 0.68523775
    Iteration 14, loss = 0.68491321
    Iteration 15, loss = 0.68584068
    Iteration 16, loss = 0.68407163
    Iteration 17, loss = 0.68441801
    Iteration 18, loss = 0.68459389
    Iteration 19, loss = 0.68319678
    Iteration 20, loss = 0.68320454
    Iteration 21, loss = 0.68218878
    Iteration 22, loss = 0.68173087
    Iteration 23, loss = 0.68170350
    Iteration 24, loss = 0.68271506
    Iteration 25, loss = 0.68167728
    Iteration 26, loss = 0.67993805
    Iteration 27, loss = 0.68120758
    Iteration 28, loss = 0.68059255
    Iteration 29, loss = 0.67975877
    Iteration 30, loss = 0.67918815
    Iteration 31, loss = 0.67936962
    Iteration 32, loss = 0.67848456
    Iteration 33, loss = 0.67905771
    Iteration 34, loss = 0.67830135
    Iteration 35, loss = 0.67825093
    Iteration 36, loss = 0.67847847
    Iteration 37, loss = 0.67831232
    Iteration 38, loss = 0.67641414
    Iteration 39, loss = 0.67778758
    Iteration 40, loss = 0.67786977
    Iteration 41, loss = 0.67667392
    Iteration 42, loss = 0.67835355
    Iteration 43, loss = 0.67531415
    Iteration 44, loss = 0.67619422
    Iteration 45, loss = 0.67556484
    Iteration 46, loss = 0.67558385
    Iteration 47, loss = 0.67498966
    Iteration 48, loss = 0.67526727
    Iteration 49, loss = 0.67482345
    Iteration 50, loss = 0.67457137
    Iteration 51, loss = 0.67530219
    Iteration 52, loss = 0.67436118
    Iteration 53, loss = 0.67407034
    Iteration 54, loss = 0.67466202
    Iteration 55, loss = 0.67287919
    Iteration 56, loss = 0.67390136
    Iteration 57, loss = 0.67251566
    Iteration 58, loss = 0.67329348
    Iteration 59, loss = 0.67250092
    Iteration 60, loss = 0.67161514
    Iteration 61, loss = 0.67183522
    Iteration 62, loss = 0.67166499
    Iteration 63, loss = 0.67189231
    Iteration 64, loss = 0.67291735
    Iteration 65, loss = 0.67104862
    Iteration 66, loss = 0.67123448
    Iteration 67, loss = 0.67065625
    Iteration 68, loss = 0.67110956
    Iteration 69, loss = 0.67076566
    Iteration 70, loss = 0.67060368
    Iteration 71, loss = 0.66925211
    Iteration 72, loss = 0.67083696
    Iteration 73, loss = 0.66890139
    Iteration 74, loss = 0.66920865
    Iteration 75, loss = 0.67011790
    Iteration 76, loss = 0.66935089
    Iteration 77, loss = 0.66843177
    Iteration 78, loss = 0.66954046
    Iteration 79, loss = 0.66932984
    Iteration 80, loss = 0.66838859
    Iteration 81, loss = 0.66929445
    Iteration 82, loss = 0.66866380
    Iteration 83, loss = 0.66943600
    Iteration 84, loss = 0.66880754
    Iteration 85, loss = 0.66795078
    Iteration 86, loss = 0.66808409
    Iteration 87, loss = 0.66786729
    Iteration 88, loss = 0.66800383
    Iteration 89, loss = 0.66779047
    Iteration 90, loss = 0.66747470
    Iteration 91, loss = 0.66805492
    Iteration 92, loss = 0.66749367
    Iteration 93, loss = 0.66824578
    Iteration 94, loss = 0.66824105
    Iteration 95, loss = 0.66739928
    Iteration 96, loss = 0.66756589
    Iteration 97, loss = 0.66726679
    Iteration 98, loss = 0.66752613
    Iteration 99, loss = 0.66853229
    Iteration 100, loss = 0.66753268
    Iteration 101, loss = 0.66753683
    Iteration 102, loss = 0.66670643
    Iteration 103, loss = 0.66720975
    Iteration 104, loss = 0.66786991
    Iteration 105, loss = 0.66738969
    Iteration 106, loss = 0.66706825
    Iteration 107, loss = 0.66651693
    Iteration 108, loss = 0.66715039
    Iteration 109, loss = 0.66759047
    Iteration 110, loss = 0.66620989
    Iteration 111, loss = 0.66679426
    Iteration 112, loss = 0.66667804
    Iteration 113, loss = 0.66661713
    Iteration 114, loss = 0.66647942
    Iteration 115, loss = 0.66666586
    Iteration 116, loss = 0.66659453
    Iteration 117, loss = 0.66616808
    Iteration 118, loss = 0.66721764
    Iteration 119, loss = 0.66632134
    Iteration 120, loss = 0.66667990
    Iteration 121, loss = 0.66622581
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=10, batch_size=auto, hidden_layer_sizes=(20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=True; total time= 2.5min
    Iteration 1, loss = 2.19406137
    Iteration 2, loss = 1.26861468
    Iteration 3, loss = 0.93237767
    Iteration 4, loss = 0.80034764
    Iteration 5, loss = 0.75308995
    Iteration 6, loss = 0.73744415
    Iteration 7, loss = 0.73283125
    Iteration 8, loss = 0.73063354
    Iteration 9, loss = 0.72969322
    Iteration 10, loss = 0.72865035
    Iteration 11, loss = 0.72790792
    Iteration 12, loss = 0.72760155
    Iteration 13, loss = 0.72742439
    Iteration 14, loss = 0.72657198
    Iteration 15, loss = 0.72607109
    Iteration 16, loss = 0.72581361
    Iteration 17, loss = 0.72556733
    Iteration 18, loss = 0.72531957
    Iteration 19, loss = 0.72409388
    Iteration 20, loss = 0.72316581
    Iteration 21, loss = 0.72222049
    Iteration 22, loss = 0.72150463
    Iteration 23, loss = 0.72089315
    Iteration 24, loss = 0.72034296
    Iteration 25, loss = 0.71999753
    Iteration 26, loss = 0.71924321
    Iteration 27, loss = 0.71877035
    Iteration 28, loss = 0.71832710
    Iteration 29, loss = 0.71734530
    Iteration 30, loss = 0.71587214
    Iteration 31, loss = 0.71060355
    Iteration 32, loss = 0.70183807
    Iteration 33, loss = 0.69669506
    Iteration 34, loss = 0.69455899
    Iteration 35, loss = 0.69361832
    Iteration 36, loss = 0.69309093
    Iteration 37, loss = 0.69285929
    Iteration 38, loss = 0.69256225
    Iteration 39, loss = 0.69243219
    Iteration 40, loss = 0.69229027
    Iteration 41, loss = 0.69221465
    Iteration 42, loss = 0.69217768
    Iteration 43, loss = 0.69218181
    Iteration 44, loss = 0.69211474
    Iteration 45, loss = 0.69205302
    Iteration 46, loss = 0.69202859
    Iteration 47, loss = 0.69197839
    Iteration 48, loss = 0.69205574
    Iteration 49, loss = 0.69194838
    Iteration 50, loss = 0.69196978
    Iteration 51, loss = 0.69189918
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=10, batch_size=auto, hidden_layer_sizes=(20, 20, 20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=True; total time= 1.5min
    Iteration 1, loss = 1.49393425
    Iteration 2, loss = 0.96706299
    Iteration 3, loss = 0.79362970
    Iteration 4, loss = 0.72668602
    Iteration 5, loss = 0.69953709
    Iteration 6, loss = 0.69148513
    Iteration 7, loss = 0.68657090
    Iteration 8, loss = 0.68557057
    Iteration 9, loss = 0.68601383
    Iteration 10, loss = 0.68466826
    Iteration 11, loss = 0.68460096
    Iteration 12, loss = 0.68445206
    Iteration 13, loss = 0.68293694
    Iteration 14, loss = 0.68328508
    Iteration 15, loss = 0.68326658
    Iteration 16, loss = 0.68178205
    Iteration 17, loss = 0.68298588
    Iteration 18, loss = 0.68217164
    Iteration 19, loss = 0.68096700
    Iteration 20, loss = 0.68144087
    Iteration 21, loss = 0.68129702
    Iteration 22, loss = 0.68012487
    Iteration 23, loss = 0.68012396
    Iteration 24, loss = 0.67926631
    Iteration 25, loss = 0.67990014
    Iteration 26, loss = 0.67935111
    Iteration 27, loss = 0.67802329
    Iteration 28, loss = 0.67868677
    Iteration 29, loss = 0.67745384
    Iteration 30, loss = 0.67774418
    Iteration 31, loss = 0.67735572
    Iteration 32, loss = 0.67657982
    Iteration 33, loss = 0.67686741
    Iteration 34, loss = 0.67630193
    Iteration 35, loss = 0.67561680
    Iteration 36, loss = 0.67550823
    Iteration 37, loss = 0.67439743
    Iteration 38, loss = 0.67426952
    Iteration 39, loss = 0.67405250
    Iteration 40, loss = 0.67471399
    Iteration 41, loss = 0.67323198
    Iteration 42, loss = 0.67285040
    Iteration 43, loss = 0.67401497
    Iteration 44, loss = 0.67208274
    Iteration 45, loss = 0.67229715
    Iteration 46, loss = 0.67126295
    Iteration 47, loss = 0.67073959
    Iteration 48, loss = 0.67102900
    Iteration 49, loss = 0.67057408
    Iteration 50, loss = 0.67032571
    Iteration 51, loss = 0.67113599
    Iteration 52, loss = 0.67011830
    Iteration 53, loss = 0.66909624
    Iteration 54, loss = 0.67038991
    Iteration 55, loss = 0.66939110
    Iteration 56, loss = 0.66895541
    Iteration 57, loss = 0.66892931
    Iteration 58, loss = 0.66968045
    Iteration 59, loss = 0.66926545
    Iteration 60, loss = 0.66858130
    Iteration 61, loss = 0.66808396
    Iteration 62, loss = 0.66846054
    Iteration 63, loss = 0.66929335
    Iteration 64, loss = 0.66824408
    Iteration 65, loss = 0.66674411
    Iteration 66, loss = 0.66838964
    Iteration 67, loss = 0.66813314
    Iteration 68, loss = 0.66762048
    Iteration 69, loss = 0.66787635
    Iteration 70, loss = 0.66652342
    Iteration 71, loss = 0.66730785
    Iteration 72, loss = 0.66696814
    Iteration 73, loss = 0.66714313
    Iteration 74, loss = 0.66667385
    Iteration 75, loss = 0.66660118
    Iteration 76, loss = 0.66739740
    Iteration 77, loss = 0.66683816
    Iteration 78, loss = 0.66750498
    Iteration 79, loss = 0.66635352
    Iteration 80, loss = 0.66875751
    Iteration 81, loss = 0.66644397
    Iteration 82, loss = 0.66661851
    Iteration 83, loss = 0.66621897
    Iteration 84, loss = 0.66661116
    Iteration 85, loss = 0.66549791
    Iteration 86, loss = 0.66588185
    Iteration 87, loss = 0.66558828
    Iteration 88, loss = 0.66536240
    Iteration 89, loss = 0.66681573
    Iteration 90, loss = 0.66541454
    Iteration 91, loss = 0.66480000
    Iteration 92, loss = 0.66572209
    Iteration 93, loss = 0.66548335
    Iteration 94, loss = 0.66587509
    Iteration 95, loss = 0.66502108
    Iteration 96, loss = 0.66548313
    Iteration 97, loss = 0.66385450
    Iteration 98, loss = 0.66576198
    Iteration 99, loss = 0.66469092
    Iteration 100, loss = 0.66475234
    Iteration 101, loss = 0.66447253
    Iteration 102, loss = 0.66607029
    Iteration 103, loss = 0.66563792
    Iteration 104, loss = 0.66380266
    Iteration 105, loss = 0.66466872
    Iteration 106, loss = 0.66428593
    Iteration 107, loss = 0.66388714
    Iteration 108, loss = 0.66379797
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=10, batch_size=auto, hidden_layer_sizes=(20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=True; total time= 1.8min
    Iteration 1, loss = 2.19184898
    Iteration 2, loss = 1.27034163
    Iteration 3, loss = 0.93254944
    Iteration 4, loss = 0.80008832
    Iteration 5, loss = 0.75272182
    Iteration 6, loss = 0.73741889
    Iteration 7, loss = 0.73213779
    Iteration 8, loss = 0.73042971
    Iteration 9, loss = 0.72818098
    Iteration 10, loss = 0.72735277
    Iteration 11, loss = 0.72646408
    Iteration 12, loss = 0.72578471
    Iteration 13, loss = 0.72505858
    Iteration 14, loss = 0.72405645
    Iteration 15, loss = 0.72317222
    Iteration 16, loss = 0.72241059
    Iteration 17, loss = 0.72188015
    Iteration 18, loss = 0.72079477
    Iteration 19, loss = 0.71994107
    Iteration 20, loss = 0.71953427
    Iteration 21, loss = 0.71884448
    Iteration 22, loss = 0.71815561
    Iteration 23, loss = 0.71732036
    Iteration 24, loss = 0.71529581
    Iteration 25, loss = 0.70872520
    Iteration 26, loss = 0.70014689
    Iteration 27, loss = 0.69588341
    Iteration 28, loss = 0.69419226
    Iteration 29, loss = 0.69340919
    Iteration 30, loss = 0.69290673
    Iteration 31, loss = 0.69261117
    Iteration 32, loss = 0.69241689
    Iteration 33, loss = 0.69230369
    Iteration 34, loss = 0.69221722
    Iteration 35, loss = 0.69219423
    Iteration 36, loss = 0.69211516
    Iteration 37, loss = 0.69209767
    Iteration 38, loss = 0.69203659
    Iteration 39, loss = 0.69201190
    Iteration 40, loss = 0.69196006
    Iteration 41, loss = 0.69194359
    Iteration 42, loss = 0.69197203
    Iteration 43, loss = 0.69194224
    Iteration 44, loss = 0.69194029
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=10, batch_size=auto, hidden_layer_sizes=(20, 20, 20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=True; total time= 1.3min
    Iteration 1, loss = 1.49581270
    Iteration 2, loss = 0.96739292
    Iteration 3, loss = 0.79449877
    Iteration 4, loss = 0.72660257
    Iteration 5, loss = 0.70057540
    Iteration 6, loss = 0.69161009
    Iteration 7, loss = 0.68795854
    Iteration 8, loss = 0.68602862
    Iteration 9, loss = 0.68612848
    Iteration 10, loss = 0.68414818
    Iteration 11, loss = 0.68431568
    Iteration 12, loss = 0.68425626
    Iteration 13, loss = 0.68309164
    Iteration 14, loss = 0.68320660
    Iteration 15, loss = 0.68251613
    Iteration 16, loss = 0.68189389
    Iteration 17, loss = 0.68180833
    Iteration 18, loss = 0.68044357
    Iteration 19, loss = 0.68115593
    Iteration 20, loss = 0.68013900
    Iteration 21, loss = 0.67959480
    Iteration 22, loss = 0.67956454
    Iteration 23, loss = 0.67925942
    Iteration 24, loss = 0.67809233
    Iteration 25, loss = 0.67968028
    Iteration 26, loss = 0.67814726
    Iteration 27, loss = 0.67681597
    Iteration 28, loss = 0.67656724
    Iteration 29, loss = 0.67624497
    Iteration 30, loss = 0.67643110
    Iteration 31, loss = 0.67647675
    Iteration 32, loss = 0.67541631
    Iteration 33, loss = 0.67586880
    Iteration 34, loss = 0.67428867
    Iteration 35, loss = 0.67472760
    Iteration 36, loss = 0.67472650
    Iteration 37, loss = 0.67364932
    Iteration 38, loss = 0.67320367
    Iteration 39, loss = 0.67330463
    Iteration 40, loss = 0.67351624
    Iteration 41, loss = 0.67468465
    Iteration 42, loss = 0.67235676
    Iteration 43, loss = 0.67243932
    Iteration 44, loss = 0.67195425
    Iteration 45, loss = 0.67186639
    Iteration 46, loss = 0.67044298
    Iteration 47, loss = 0.67077465
    Iteration 48, loss = 0.67157296
    Iteration 49, loss = 0.67013124
    Iteration 50, loss = 0.66983760
    Iteration 51, loss = 0.67082496
    Iteration 52, loss = 0.67013591
    Iteration 53, loss = 0.66947077
    Iteration 54, loss = 0.66965982
    Iteration 55, loss = 0.66915903
    Iteration 56, loss = 0.66912347
    Iteration 57, loss = 0.66802386
    Iteration 58, loss = 0.66849534
    Iteration 59, loss = 0.66866211
    Iteration 60, loss = 0.66790551
    Iteration 61, loss = 0.66807858
    Iteration 62, loss = 0.66799898
    Iteration 63, loss = 0.66710067
    Iteration 64, loss = 0.66730958
    Iteration 65, loss = 0.66684742
    Iteration 66, loss = 0.66980184
    Iteration 67, loss = 0.66761061
    Iteration 68, loss = 0.66676374
    Iteration 69, loss = 0.66794681
    Iteration 70, loss = 0.66596077
    Iteration 71, loss = 0.66607149
    Iteration 72, loss = 0.66658507
    Iteration 73, loss = 0.66651947
    Iteration 74, loss = 0.66602687
    Iteration 75, loss = 0.66513640
    Iteration 76, loss = 0.66664237
    Iteration 77, loss = 0.66557278
    Iteration 78, loss = 0.66735477
    Iteration 79, loss = 0.66572609
    Iteration 80, loss = 0.66558938
    Iteration 81, loss = 0.66517703
    Iteration 82, loss = 0.66552593
    Iteration 83, loss = 0.66527079
    Iteration 84, loss = 0.66581447
    Iteration 85, loss = 0.66461081
    Iteration 86, loss = 0.66415551
    Iteration 87, loss = 0.66511277
    Iteration 88, loss = 0.66405808
    Iteration 89, loss = 0.66596476
    Iteration 90, loss = 0.66397149
    Iteration 91, loss = 0.66466975
    Iteration 92, loss = 0.66369572
    Iteration 93, loss = 0.66390710
    Iteration 94, loss = 0.66449496
    Iteration 95, loss = 0.66468466
    Iteration 96, loss = 0.66378343
    Iteration 97, loss = 0.66347815
    Iteration 98, loss = 0.66444231
    Iteration 99, loss = 0.66457552
    Iteration 100, loss = 0.66329201
    Iteration 101, loss = 0.66384885
    Iteration 102, loss = 0.66411159
    Iteration 103, loss = 0.66498320
    Iteration 104, loss = 0.66403539
    Iteration 105, loss = 0.66330653
    Iteration 106, loss = 0.66412244
    Iteration 107, loss = 0.66333327
    Iteration 108, loss = 0.66382482
    Iteration 109, loss = 0.66386097
    Iteration 110, loss = 0.66299528
    Iteration 111, loss = 0.66315749
    Iteration 112, loss = 0.66363483
    Iteration 113, loss = 0.66353134
    Iteration 114, loss = 0.66377036
    Iteration 115, loss = 0.66354703
    Iteration 116, loss = 0.66374339
    Iteration 117, loss = 0.66304236
    Iteration 118, loss = 0.66260478
    Iteration 119, loss = 0.66369022
    Iteration 120, loss = 0.66346124
    Iteration 121, loss = 0.66474610
    Iteration 122, loss = 0.66346681
    Iteration 123, loss = 0.66237497
    Iteration 124, loss = 0.66282059
    Iteration 125, loss = 0.66295695
    Iteration 126, loss = 0.66258302
    Iteration 127, loss = 0.66294062
    Iteration 128, loss = 0.66356516
    Iteration 129, loss = 0.66348820
    Iteration 130, loss = 0.66267920
    Iteration 131, loss = 0.66358106
    Iteration 132, loss = 0.66248066
    Iteration 133, loss = 0.66337214
    Iteration 134, loss = 0.66238008
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=10, batch_size=auto, hidden_layer_sizes=(20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=True; total time= 2.1min


#### 9.3.8. Neural Network modes: Multilayer Perceptron Feed-Forward network - activation: 'relu', Grid Search
- increased tvec features to 10k
- max_feat: 50k
- activation: 'relu', 
- hidden_layer: [(20,20,20)]
- alpha: [10],
- max iterations: default (400)

Results:
- run time: 
- MSE: 77%


```python
# Create HSK5 binary variable
df_h5 = df_tracks_master_clean3_lyrics_genre_hsk_v2.copy()
df_h5['is_h5'] = 'NotHSK'
df_h5['is_h5'][df_tracks_master_clean3_lyrics_genre_hsk_v2.hsk5 > 0.9] = 'HSK5'
X = df_h5[['lyrics','is_h5'
]]
y = X.pop('is_h5')
X = X['lyrics']
df_h5['is_h5'].value_counts()
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    stratify=y,
                                                    test_size=0.2,
                                                    random_state=1)

from sklearn.model_selection import GridSearchCV

from sklearn.neural_network import MLPClassifier

# Tokenize lyrics
cvec = CountVectorizer(tokenizer=man_token_jie, max_features=50000)
cvec.fit(X_train)

# Transform training data
cvec_mat = cvec.transform(X_train)
X_train = cvec_mat
X_test = cvec.transform(X_test)
```


```python
params = {'solver':['adam'],
                'alpha':[10],
                'hidden_layer_sizes':[(20,20,20)],
                'activation':['relu'],
                'random_state':[42],
                'batch_size':['auto'],
                'max_iter':[500], 'verbose':[True],'warm_start':[True]}


skf = StratifiedKFold(n_splits=4)
skf.get_n_splits(X_train, y_train)



gs = GridSearchCV(MLPClassifier(), param_grid=params, cv=skf, n_jobs=-1, verbose=2)
gs.fit(X_train, y_train)

model = gs.best_estimator_
```

    Fitting 4 folds for each of 1 candidates, totalling 4 fits
    Iteration 1, loss = 1.35024914
    Iteration 2, loss = 0.88570812
    Iteration 3, loss = 0.75858544
    Iteration 4, loss = 0.72592676
    Iteration 5, loss = 0.71699222
    Iteration 6, loss = 0.71387496
    Iteration 7, loss = 0.71335430
    Iteration 8, loss = 0.71229031
    Iteration 9, loss = 0.71232893
    Iteration 10, loss = 0.71284070
    Iteration 11, loss = 0.71170232
    Iteration 12, loss = 0.71078681
    Iteration 13, loss = 0.71000950
    Iteration 14, loss = 0.71034506
    Iteration 15, loss = 0.71034724
    Iteration 16, loss = 0.70955133
    Iteration 17, loss = 0.70792978
    Iteration 1, loss = 1.44415131
    Iteration 2, loss = 0.98354003
    Iteration 3, loss = 0.82134583
    Iteration 4, loss = 0.75499884
    Iteration 5, loss = 0.72724757
    Iteration 6, loss = 0.71921240
    Iteration 7, loss = 0.71389767
    Iteration 8, loss = 0.71431159
    Iteration 9, loss = 0.70870511
    Iteration 10, loss = 0.70763664
    Iteration 11, loss = 0.70835559
    Iteration 12, loss = 0.70396569
    Iteration 13, loss = 0.70523846
    Iteration 14, loss = 0.70407632
    Iteration 15, loss = 0.70287095
    Iteration 16, loss = 0.70106011
    Iteration 17, loss = 0.70044266
    Iteration 18, loss = 0.69996420
    Iteration 19, loss = 0.69497643
    Iteration 20, loss = 0.69441703
    Iteration 21, loss = 0.69264011
    Iteration 22, loss = 0.69197881
    Iteration 23, loss = 0.69012754
    Iteration 24, loss = 0.68888503
    Iteration 25, loss = 0.68641773
    Iteration 26, loss = 0.68588677
    Iteration 27, loss = 0.68460954
    Iteration 28, loss = 0.68228692
    Iteration 29, loss = 0.68454845
    Iteration 30, loss = 0.68052235
    Iteration 31, loss = 0.67907625
    Iteration 32, loss = 0.67996620
    Iteration 33, loss = 0.67788612
    Iteration 34, loss = 0.67913359
    Iteration 35, loss = 0.67671539
    Iteration 36, loss = 0.67758046
    Iteration 37, loss = 0.67558500
    Iteration 38, loss = 0.67601671
    Iteration 39, loss = 0.67457365
    Iteration 40, loss = 0.67610600
    Iteration 41, loss = 0.67342892
    Iteration 42, loss = 0.67234727
    Iteration 43, loss = 0.67388269
    Iteration 44, loss = 0.67261033
    Iteration 45, loss = 0.67328312
    Iteration 46, loss = 0.67093953
    Iteration 47, loss = 0.67152607
    Iteration 48, loss = 0.67146865
    Iteration 49, loss = 0.66926957
    Iteration 50, loss = 0.66930409
    Iteration 51, loss = 0.67107782
    Iteration 52, loss = 0.67078556
    Iteration 53, loss = 0.66996786
    Iteration 54, loss = 0.67010160
    Iteration 55, loss = 0.66974765
    Iteration 56, loss = 0.66830428
    Iteration 57, loss = 0.66786043
    Iteration 58, loss = 0.66780467
    Iteration 59, loss = 0.66971204
    Iteration 60, loss = 0.66818454
    Iteration 61, loss = 0.66759732
    Iteration 62, loss = 0.66830676
    Iteration 63, loss = 0.66719742
    Iteration 64, loss = 0.66682339
    Iteration 65, loss = 0.66772079
    Iteration 66, loss = 0.66836973
    Iteration 67, loss = 0.66713589
    Iteration 68, loss = 0.66623267
    Iteration 69, loss = 0.66673505
    Iteration 70, loss = 0.66586139
    Iteration 71, loss = 0.66704794
    Iteration 72, loss = 0.66655758
    Iteration 73, loss = 0.66849319
    Iteration 74, loss = 0.66628490
    Iteration 75, loss = 0.66652696
    Iteration 76, loss = 0.66748088
    Iteration 77, loss = 0.66601512
    Iteration 78, loss = 0.66549650
    Iteration 79, loss = 0.66634112
    Iteration 80, loss = 0.66522201
    Iteration 81, loss = 0.66667783
    Iteration 82, loss = 0.66428947
    Iteration 83, loss = 0.66563956
    Iteration 84, loss = 0.66585394
    Iteration 85, loss = 0.66540104
    Iteration 86, loss = 0.66537203
    Iteration 87, loss = 0.66475146
    Iteration 88, loss = 0.66392869
    Iteration 89, loss = 0.66633811
    Iteration 90, loss = 0.66540518
    Iteration 91, loss = 0.66511652
    Iteration 92, loss = 0.66438413
    Iteration 93, loss = 0.66387855
    Iteration 94, loss = 0.66485494
    Iteration 95, loss = 0.66554033
    Iteration 96, loss = 0.66322447
    Iteration 97, loss = 0.66441418
    Iteration 98, loss = 0.66453277
    Iteration 99, loss = 0.66393072
    Iteration 100, loss = 0.66435148
    Iteration 101, loss = 0.66319788
    Iteration 102, loss = 0.66352446
    Iteration 103, loss = 0.66442405
    Iteration 104, loss = 0.66356267
    Iteration 105, loss = 0.66613681
    Iteration 106, loss = 0.66167147
    Iteration 107, loss = 0.66393837
    Iteration 108, loss = 0.66222251
    Iteration 109, loss = 0.66342310
    Iteration 110, loss = 0.66453870
    Iteration 111, loss = 0.66195230
    Iteration 112, loss = 0.66196508
    Iteration 113, loss = 0.66267252
    Iteration 114, loss = 0.66214761
    Iteration 115, loss = 0.66298436
    Iteration 116, loss = 0.66174402
    Iteration 117, loss = 0.66259729
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=10, batch_size=auto, hidden_layer_sizes=(20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=True; total time=11.2min
    Iteration 18, loss = 0.70782094
    Iteration 19, loss = 0.70597116
    Iteration 20, loss = 0.70602437
    Iteration 21, loss = 0.70589220
    Iteration 22, loss = 0.70462924
    Iteration 23, loss = 0.70415520
    Iteration 24, loss = 0.70241635
    Iteration 25, loss = 0.70219179
    Iteration 26, loss = 0.70061440
    Iteration 27, loss = 0.70038884
    Iteration 28, loss = 0.69812959
    Iteration 1, loss = 1.44593544
    Iteration 2, loss = 0.98608950
    Iteration 3, loss = 0.81876969
    Iteration 4, loss = 0.75138201
    Iteration 5, loss = 0.72716487
    Iteration 6, loss = 0.71749247
    Iteration 7, loss = 0.71293806
    Iteration 8, loss = 0.71284484
    Iteration 9, loss = 0.70723360
    Iteration 10, loss = 0.70632146
    Iteration 11, loss = 0.70649671
    Iteration 12, loss = 0.70174625
    Iteration 13, loss = 0.70253873
    Iteration 14, loss = 0.69950237
    Iteration 15, loss = 0.69885477
    Iteration 16, loss = 0.69728473
    Iteration 17, loss = 0.69490648
    Iteration 18, loss = 0.69516266
    Iteration 19, loss = 0.69137799
    Iteration 20, loss = 0.69251401
    Iteration 21, loss = 0.68887852
    Iteration 22, loss = 0.68905474
    Iteration 23, loss = 0.68807579
    Iteration 24, loss = 0.68724748
    Iteration 25, loss = 0.68602711
    Iteration 26, loss = 0.68534988
    Iteration 27, loss = 0.68261380
    Iteration 28, loss = 0.68281530
    Iteration 29, loss = 0.68200512
    Iteration 30, loss = 0.68124071
    Iteration 31, loss = 0.67962330
    Iteration 32, loss = 0.68000301
    Iteration 33, loss = 0.67827695
    Iteration 34, loss = 0.67713024
    Iteration 35, loss = 0.67834701
    Iteration 36, loss = 0.67665705
    Iteration 37, loss = 0.67641639
    Iteration 38, loss = 0.67580483
    Iteration 39, loss = 0.67503788
    Iteration 40, loss = 0.67637084
    Iteration 41, loss = 0.67346708
    Iteration 42, loss = 0.67276277
    Iteration 43, loss = 0.67598762
    Iteration 44, loss = 0.67254108
    Iteration 45, loss = 0.67277756
    Iteration 46, loss = 0.67187918
    Iteration 47, loss = 0.67185386
    Iteration 48, loss = 0.67170432
    Iteration 49, loss = 0.67056779
    Iteration 50, loss = 0.66988354
    Iteration 51, loss = 0.67181581
    Iteration 52, loss = 0.67192995
    Iteration 53, loss = 0.67050725
    Iteration 54, loss = 0.66914469
    Iteration 55, loss = 0.66983689
    Iteration 56, loss = 0.67038547
    Iteration 57, loss = 0.66812216
    Iteration 58, loss = 0.66962123
    Iteration 59, loss = 0.66974109
    Iteration 60, loss = 0.66782452
    Iteration 61, loss = 0.66890278
    Iteration 62, loss = 0.66833195
    Iteration 63, loss = 0.66783892
    Iteration 64, loss = 0.66818713
    Iteration 65, loss = 0.66902911
    Iteration 66, loss = 0.66723316
    Iteration 67, loss = 0.66699783
    Iteration 68, loss = 0.66803839
    Iteration 69, loss = 0.66738726
    Iteration 70, loss = 0.66652410
    Iteration 71, loss = 0.66650658
    Iteration 72, loss = 0.66658041
    Iteration 73, loss = 0.66629448
    Iteration 74, loss = 0.66622150
    Iteration 75, loss = 0.66660688
    Iteration 76, loss = 0.66628152
    Iteration 77, loss = 0.66547650
    Iteration 78, loss = 0.66631678
    Iteration 79, loss = 0.66645971
    Iteration 80, loss = 0.66610974
    Iteration 81, loss = 0.66564217
    Iteration 82, loss = 0.66587454
    Iteration 83, loss = 0.66478943
    Iteration 84, loss = 0.66598720
    Iteration 85, loss = 0.66577836
    Iteration 86, loss = 0.66489971
    Iteration 87, loss = 0.66383948
    Iteration 88, loss = 0.66385920
    Iteration 89, loss = 0.66708223
    Iteration 90, loss = 0.66695000
    Iteration 91, loss = 0.66394328
    Iteration 92, loss = 0.66326329
    Iteration 93, loss = 0.66372807
    Iteration 94, loss = 0.66317600
    Iteration 95, loss = 0.66486844
    Iteration 96, loss = 0.66314259
    Iteration 97, loss = 0.66409556
    Iteration 98, loss = 0.66306594
    Iteration 99, loss = 0.66486737
    Iteration 100, loss = 0.66280669
    Iteration 101, loss = 0.66324593
    Iteration 102, loss = 0.66415194
    Iteration 103, loss = 0.66235842
    Iteration 104, loss = 0.66348009
    Iteration 105, loss = 0.66124473
    Iteration 106, loss = 0.66171057
    Iteration 107, loss = 0.66275426
    Iteration 108, loss = 0.66210023
    Iteration 109, loss = 0.66161370
    Iteration 110, loss = 0.66291592
    Iteration 111, loss = 0.66092697
    Iteration 112, loss = 0.66179436
    Iteration 113, loss = 0.66189672
    Iteration 114, loss = 0.66256063
    Iteration 115, loss = 0.66240180
    Iteration 116, loss = 0.66097556
    Iteration 117, loss = 0.66151201
    Iteration 118, loss = 0.66182640
    Iteration 119, loss = 0.66199152
    Iteration 120, loss = 0.66173086
    Iteration 121, loss = 0.66204379
    Iteration 122, loss = 0.66175779
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=10, batch_size=auto, hidden_layer_sizes=(20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=True; total time=11.6min
    Iteration 29, loss = 0.69733527
    Iteration 30, loss = 0.69777064
    Iteration 31, loss = 0.69541914
    Iteration 32, loss = 0.69743891
    Iteration 33, loss = 0.69653160
    Iteration 34, loss = 0.69542744
    Iteration 35, loss = 0.69486550
    Iteration 36, loss = 0.69489337
    Iteration 37, loss = 0.69449421
    Iteration 38, loss = 0.69266193
    Iteration 39, loss = 0.69372821
    Iteration 40, loss = 0.69312790
    Iteration 41, loss = 0.69344944
    Iteration 42, loss = 0.69223218
    Iteration 43, loss = 0.69355911
    Iteration 44, loss = 0.69244126
    Iteration 45, loss = 0.69241712
    Iteration 46, loss = 0.69247078
    Iteration 47, loss = 0.69165601
    Iteration 48, loss = 0.69173240
    Iteration 49, loss = 0.69238435
    Iteration 50, loss = 0.68998832
    Iteration 51, loss = 0.68950573
    Iteration 52, loss = 0.69145219
    Iteration 53, loss = 0.69110339
    Iteration 54, loss = 0.69026099
    Iteration 55, loss = 0.68984224
    Iteration 56, loss = 0.68919030
    Iteration 57, loss = 0.69179332
    Iteration 58, loss = 0.68991976
    Iteration 59, loss = 0.68935137
    Iteration 60, loss = 0.68794273
    Iteration 61, loss = 0.68928418
    Iteration 62, loss = 0.69040395
    Iteration 63, loss = 0.68832591
    Iteration 64, loss = 0.68907511
    Iteration 65, loss = 0.68924864
    Iteration 66, loss = 0.68658911
    Iteration 67, loss = 0.68842401
    Iteration 68, loss = 0.68634958
    Iteration 69, loss = 0.68788694
    Iteration 70, loss = 0.68882833
    Iteration 71, loss = 0.68692247
    Iteration 72, loss = 0.68784498
    Iteration 73, loss = 0.68831448
    Iteration 74, loss = 0.68644271
    Iteration 75, loss = 0.68672507
    Iteration 76, loss = 0.68627063
    Iteration 77, loss = 0.68636801
    Iteration 78, loss = 0.68638924
    Iteration 79, loss = 0.68444852
    Iteration 80, loss = 0.68497609
    Iteration 81, loss = 0.68521769
    Iteration 82, loss = 0.68614050
    Iteration 83, loss = 0.68510451
    Iteration 84, loss = 0.68527867
    Iteration 85, loss = 0.68440636
    Iteration 86, loss = 0.68618894
    Iteration 1, loss = 1.44105623
    Iteration 2, loss = 0.98184597
    Iteration 3, loss = 0.81556608
    Iteration 4, loss = 0.75010334
    Iteration 5, loss = 0.72253675
    Iteration 6, loss = 0.71513487
    Iteration 7, loss = 0.70964674
    Iteration 8, loss = 0.70756737
    Iteration 9, loss = 0.70688439
    Iteration 10, loss = 0.70301302
    Iteration 11, loss = 0.70248234
    Iteration 12, loss = 0.70114356
    Iteration 13, loss = 0.70245263
    Iteration 14, loss = 0.70045708
    Iteration 15, loss = 0.70009483
    Iteration 16, loss = 0.69974891
    Iteration 17, loss = 0.70131765
    Iteration 18, loss = 0.69856489
    Iteration 19, loss = 0.69919132
    Iteration 20, loss = 0.69814810
    Iteration 21, loss = 0.69595635
    Iteration 22, loss = 0.69720472
    Iteration 23, loss = 0.69793175
    Iteration 24, loss = 0.69463526
    Iteration 25, loss = 0.69585925
    Iteration 26, loss = 0.69451668
    Iteration 27, loss = 0.69386901
    Iteration 28, loss = 0.69177623
    Iteration 29, loss = 0.69256208
    Iteration 30, loss = 0.68977387
    Iteration 31, loss = 0.68895802
    Iteration 32, loss = 0.68813546
    Iteration 33, loss = 0.68779412
    Iteration 34, loss = 0.68691302
    Iteration 35, loss = 0.68522052
    Iteration 36, loss = 0.68458971
    Iteration 37, loss = 0.68436739
    Iteration 38, loss = 0.68465908
    Iteration 39, loss = 0.68269514
    Iteration 40, loss = 0.68273290
    Iteration 41, loss = 0.68160273
    Iteration 42, loss = 0.68148386
    Iteration 43, loss = 0.67997143
    Iteration 44, loss = 0.68052872
    Iteration 45, loss = 0.68021760
    Iteration 46, loss = 0.67737299
    Iteration 47, loss = 0.67847477
    Iteration 48, loss = 0.67687715
    Iteration 49, loss = 0.67532043
    Iteration 50, loss = 0.67540480
    Iteration 51, loss = 0.67594495
    Iteration 52, loss = 0.67607502
    Iteration 53, loss = 0.67346745
    Iteration 54, loss = 0.67566429
    Iteration 55, loss = 0.67336800
    Iteration 56, loss = 0.67387659
    Iteration 57, loss = 0.67317763
    Iteration 58, loss = 0.67128238
    Iteration 59, loss = 0.67365842
    Iteration 60, loss = 0.67083336
    Iteration 61, loss = 0.67205863
    Iteration 62, loss = 0.67178425
    Iteration 63, loss = 0.67216593
    Iteration 64, loss = 0.66955118
    Iteration 65, loss = 0.67118020
    Iteration 66, loss = 0.67161615
    Iteration 67, loss = 0.67023487
    Iteration 68, loss = 0.66968707
    Iteration 69, loss = 0.66947208
    Iteration 70, loss = 0.66830298
    Iteration 71, loss = 0.66954819
    Iteration 72, loss = 0.66853823
    Iteration 73, loss = 0.66845905
    Iteration 74, loss = 0.66867703
    Iteration 75, loss = 0.66858438
    Iteration 76, loss = 0.66820341
    Iteration 77, loss = 0.66842801
    Iteration 78, loss = 0.66826882
    Iteration 79, loss = 0.66855928
    Iteration 80, loss = 0.66596893
    Iteration 81, loss = 0.66819259
    Iteration 82, loss = 0.66681247
    Iteration 83, loss = 0.66648983
    Iteration 84, loss = 0.66861913
    Iteration 85, loss = 0.66765082
    Iteration 86, loss = 0.66604245
    Iteration 87, loss = 0.66690427
    Iteration 88, loss = 0.66748456
    Iteration 89, loss = 0.66750924
    Iteration 90, loss = 0.66526223
    Iteration 91, loss = 0.66660950
    Iteration 92, loss = 0.66667231
    Iteration 93, loss = 0.66571176
    Iteration 94, loss = 0.66720674
    Iteration 95, loss = 0.66610002
    Iteration 96, loss = 0.66507123
    Iteration 97, loss = 0.66586935
    Iteration 98, loss = 0.66676174
    Iteration 99, loss = 0.66478436
    Iteration 100, loss = 0.66612674
    Iteration 101, loss = 0.66411640
    Iteration 102, loss = 0.66422385
    Iteration 103, loss = 0.66535419
    Iteration 104, loss = 0.66507820
    Iteration 105, loss = 0.66665072
    Iteration 106, loss = 0.66407507
    Iteration 107, loss = 0.66507754
    Iteration 108, loss = 0.66457012
    Iteration 109, loss = 0.66408193
    Iteration 110, loss = 0.66669674
    Iteration 111, loss = 0.66386062
    Iteration 112, loss = 0.66408490
    Iteration 113, loss = 0.66315402
    Iteration 114, loss = 0.66343311
    Iteration 115, loss = 0.66330249
    Iteration 116, loss = 0.66353814
    Iteration 117, loss = 0.66499324
    Iteration 118, loss = 0.66406791
    Iteration 119, loss = 0.66370709
    Iteration 120, loss = 0.66442061
    Iteration 121, loss = 0.66278581
    Iteration 122, loss = 0.66422192
    Iteration 123, loss = 0.66150674
    Iteration 124, loss = 0.66375451
    Iteration 125, loss = 0.66405087
    Iteration 126, loss = 0.66314443
    Iteration 127, loss = 0.66345414
    Iteration 128, loss = 0.66344171
    Iteration 129, loss = 0.66338298
    Iteration 130, loss = 0.66215458
    Iteration 131, loss = 0.66263164
    Iteration 132, loss = 0.66254838
    Iteration 133, loss = 0.66109373
    Iteration 134, loss = 0.66286577
    Iteration 135, loss = 0.66178572
    Iteration 136, loss = 0.66181755
    Iteration 137, loss = 0.66165405
    Iteration 138, loss = 0.66108026
    Iteration 139, loss = 0.66215488
    Iteration 140, loss = 0.66125167
    Iteration 141, loss = 0.66202648
    Iteration 142, loss = 0.66126993
    Iteration 143, loss = 0.66096934
    Iteration 144, loss = 0.66066721
    Iteration 145, loss = 0.66143519
    Iteration 146, loss = 0.66203816
    Iteration 147, loss = 0.65955032
    Iteration 148, loss = 0.65968767
    Iteration 149, loss = 0.65950578
    Iteration 150, loss = 0.65941476
    Iteration 151, loss = 0.66103518
    Iteration 152, loss = 0.66015900
    Iteration 153, loss = 0.66094198
    Iteration 154, loss = 0.66050492
    Iteration 155, loss = 0.65981133
    Iteration 156, loss = 0.65975871
    Iteration 157, loss = 0.65885516
    Iteration 158, loss = 0.66052243
    Iteration 159, loss = 0.66012174
    Iteration 160, loss = 0.65980259
    Iteration 161, loss = 0.65975597
    Iteration 162, loss = 0.66004560
    Iteration 163, loss = 0.66090228
    Iteration 164, loss = 0.66085466
    Iteration 165, loss = 0.65996337
    Iteration 166, loss = 0.65965175
    Iteration 167, loss = 0.66047790
    Iteration 168, loss = 0.65944627
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=10, batch_size=auto, hidden_layer_sizes=(20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=True; total time=14.0min
    Iteration 87, loss = 0.68449560
    Iteration 88, loss = 0.68423817
    Iteration 89, loss = 0.68655650
    Iteration 90, loss = 0.68418899
    Iteration 91, loss = 0.68500051
    Iteration 92, loss = 0.68475917
    Iteration 93, loss = 0.68375005
    Iteration 94, loss = 0.68443226
    Iteration 95, loss = 0.68480479
    Iteration 96, loss = 0.68347305
    Iteration 97, loss = 0.68514632
    Iteration 98, loss = 0.68364997
    Iteration 99, loss = 0.68394460
    Iteration 100, loss = 0.68218899
    Iteration 101, loss = 0.68308817
    Iteration 102, loss = 0.68286692
    Iteration 103, loss = 0.68366205
    Iteration 104, loss = 0.68287124
    Iteration 105, loss = 0.68421520
    Iteration 106, loss = 0.68303352
    Iteration 107, loss = 0.68286353
    Iteration 108, loss = 0.68230370
    Iteration 109, loss = 0.68207762
    Iteration 110, loss = 0.68239203
    Iteration 111, loss = 0.68270288
    Iteration 112, loss = 0.68258469
    Iteration 113, loss = 0.68221028
    Iteration 114, loss = 0.68257245
    Iteration 115, loss = 0.68150500
    Iteration 116, loss = 0.68261843
    Iteration 117, loss = 0.68168770
    Iteration 118, loss = 0.68162782
    Iteration 119, loss = 0.68137726
    Iteration 120, loss = 0.68155102
    Iteration 121, loss = 0.68235209
    Iteration 122, loss = 0.68238707
    Iteration 1, loss = 1.44190506
    Iteration 2, loss = 0.98349124
    Iteration 3, loss = 0.81468965
    Iteration 4, loss = 0.74912773
    Iteration 5, loss = 0.72521721
    Iteration 6, loss = 0.71560564
    Iteration 7, loss = 0.71317239
    Iteration 8, loss = 0.71207114
    Iteration 9, loss = 0.71061220
    Iteration 10, loss = 0.70955352
    Iteration 11, loss = 0.71022893
    Iteration 12, loss = 0.70798106
    Iteration 13, loss = 0.70882156
    Iteration 14, loss = 0.70742383
    Iteration 15, loss = 0.70749150
    Iteration 16, loss = 0.70555223
    Iteration 17, loss = 0.70511278
    Iteration 18, loss = 0.70423144
    Iteration 19, loss = 0.70464525
    Iteration 20, loss = 0.70429436
    Iteration 21, loss = 0.70384151
    Iteration 22, loss = 0.70319143
    Iteration 23, loss = 0.70254669
    Iteration 24, loss = 0.70268969
    Iteration 25, loss = 0.70410035
    Iteration 26, loss = 0.69968460
    Iteration 27, loss = 0.70142743
    Iteration 28, loss = 0.70001589
    Iteration 29, loss = 0.70031951
    Iteration 30, loss = 0.70084124
    Iteration 31, loss = 0.69895169
    Iteration 32, loss = 0.69987659
    Iteration 33, loss = 0.69827604
    Iteration 34, loss = 0.69782405
    Iteration 35, loss = 0.70075374
    Iteration 36, loss = 0.69858448
    Iteration 37, loss = 0.69901367
    Iteration 38, loss = 0.69993695
    Iteration 39, loss = 0.69840958
    Iteration 40, loss = 0.69684637
    Iteration 41, loss = 0.69617571
    Iteration 42, loss = 0.69595919
    Iteration 43, loss = 0.69843293
    Iteration 44, loss = 0.69488228
    Iteration 45, loss = 0.69597464
    Iteration 46, loss = 0.69621021
    Iteration 47, loss = 0.69413071
    Iteration 48, loss = 0.69521657
    Iteration 49, loss = 0.69561197
    Iteration 50, loss = 0.69448875
    Iteration 51, loss = 0.69373252
    Iteration 52, loss = 0.69383225
    Iteration 53, loss = 0.69207633
    Iteration 54, loss = 0.69266205
    Iteration 55, loss = 0.69073492
    Iteration 56, loss = 0.69050796
    Iteration 57, loss = 0.68924168
    Iteration 58, loss = 0.69104004
    Iteration 59, loss = 0.69017408
    Iteration 60, loss = 0.68895306
    Iteration 61, loss = 0.68918125
    Iteration 62, loss = 0.68798464
    Iteration 63, loss = 0.68805746
    Iteration 64, loss = 0.68856551
    Iteration 65, loss = 0.68742214
    Iteration 66, loss = 0.68678947
    Iteration 67, loss = 0.68622601
    Iteration 68, loss = 0.68890408
    Iteration 69, loss = 0.68474477
    Iteration 70, loss = 0.68565795
    Iteration 71, loss = 0.68624698
    Iteration 72, loss = 0.68541668
    Iteration 73, loss = 0.68576662
    Iteration 74, loss = 0.68491143
    Iteration 75, loss = 0.68484739
    Iteration 76, loss = 0.68443831
    Iteration 77, loss = 0.68536577
    Iteration 78, loss = 0.68443994
    Iteration 79, loss = 0.68406248
    Iteration 80, loss = 0.68459133
    Iteration 81, loss = 0.68286808
    Iteration 82, loss = 0.68326957
    Iteration 83, loss = 0.68361574
    Iteration 84, loss = 0.68163828
    Iteration 85, loss = 0.68211687
    Iteration 86, loss = 0.68213605
    Iteration 87, loss = 0.68211689
    Iteration 88, loss = 0.68085940
    Iteration 89, loss = 0.68323184
    Iteration 90, loss = 0.68257568
    Iteration 91, loss = 0.67949681
    Iteration 92, loss = 0.68167875
    Iteration 93, loss = 0.67962981
    Iteration 94, loss = 0.67829270
    Iteration 95, loss = 0.68035523
    Iteration 96, loss = 0.67915968
    Iteration 97, loss = 0.67888453
    Iteration 98, loss = 0.67854759
    Iteration 99, loss = 0.67842841
    Iteration 100, loss = 0.67713397
    Iteration 101, loss = 0.67737871
    Iteration 102, loss = 0.67753766
    Iteration 103, loss = 0.67664370
    Iteration 104, loss = 0.67689280
    Iteration 105, loss = 0.67601257
    Iteration 106, loss = 0.67600565
    Iteration 107, loss = 0.67508758
    Iteration 108, loss = 0.67490062
    Iteration 109, loss = 0.67815059
    Iteration 110, loss = 0.67442263
    Iteration 111, loss = 0.67404231
    Iteration 112, loss = 0.67375703
    Iteration 113, loss = 0.67350680
    Iteration 114, loss = 0.67373923
    Iteration 115, loss = 0.67328742
    Iteration 116, loss = 0.67179556
    Iteration 117, loss = 0.67300210
    Iteration 118, loss = 0.67223923
    Iteration 119, loss = 0.67291533
    Iteration 120, loss = 0.67201201
    Iteration 121, loss = 0.67108466
    Iteration 122, loss = 0.67096571
    Iteration 123, loss = 0.67207091
    Iteration 124, loss = 0.67081363
    Iteration 125, loss = 0.67092224
    Iteration 126, loss = 0.67053976
    Iteration 127, loss = 0.66991021
    Iteration 128, loss = 0.67131187
    Iteration 129, loss = 0.66994634
    Iteration 130, loss = 0.66872382
    Iteration 131, loss = 0.66984207
    Iteration 132, loss = 0.67002857
    Iteration 133, loss = 0.66948045
    Iteration 134, loss = 0.66947883
    Iteration 135, loss = 0.66921098
    Iteration 136, loss = 0.66971578
    Iteration 137, loss = 0.66934830
    Iteration 138, loss = 0.66961761
    Iteration 139, loss = 0.66875931
    Iteration 140, loss = 0.66739666
    Iteration 141, loss = 0.67189492
    Iteration 142, loss = 0.66846270
    Iteration 143, loss = 0.66941722
    Iteration 144, loss = 0.66710907
    Iteration 145, loss = 0.66911444
    Iteration 146, loss = 0.66779840
    Iteration 147, loss = 0.66792130
    Iteration 148, loss = 0.66734158
    Iteration 149, loss = 0.66826478
    Iteration 150, loss = 0.66746392
    Iteration 151, loss = 0.66689267
    Iteration 152, loss = 0.66792964
    Iteration 153, loss = 0.66740891
    Iteration 154, loss = 0.66753623
    Iteration 155, loss = 0.66714619
    Iteration 156, loss = 0.66656987
    Iteration 157, loss = 0.66638713
    Iteration 158, loss = 0.66591819
    Iteration 159, loss = 0.66592216
    Iteration 160, loss = 0.66727832
    Iteration 161, loss = 0.66555406
    Iteration 162, loss = 0.66667362
    Iteration 163, loss = 0.66582712
    Iteration 164, loss = 0.66540903
    Iteration 165, loss = 0.66562747
    Iteration 166, loss = 0.66708861
    Iteration 167, loss = 0.66536931
    Iteration 168, loss = 0.66560652
    Iteration 169, loss = 0.66646228
    Iteration 170, loss = 0.66648938
    Iteration 171, loss = 0.66667187
    Iteration 172, loss = 0.66498314
    Iteration 173, loss = 0.66481059
    Iteration 174, loss = 0.66512743
    Iteration 175, loss = 0.66491934
    Iteration 176, loss = 0.66574466
    Iteration 177, loss = 0.66524573
    Iteration 178, loss = 0.66431087
    Iteration 179, loss = 0.66427313
    Iteration 180, loss = 0.66499117
    Iteration 181, loss = 0.66490761
    Iteration 182, loss = 0.66377168
    Iteration 183, loss = 0.66428351
    Iteration 184, loss = 0.66379693
    Iteration 185, loss = 0.66517199
    Iteration 186, loss = 0.66347530
    Iteration 187, loss = 0.66423947
    Iteration 188, loss = 0.66342146
    Iteration 189, loss = 0.66346310
    Iteration 190, loss = 0.66402585
    Iteration 191, loss = 0.66342216
    Iteration 192, loss = 0.66351428
    Iteration 193, loss = 0.66317316
    Iteration 194, loss = 0.66444589
    Iteration 195, loss = 0.66364286
    Iteration 196, loss = 0.66328726
    Iteration 197, loss = 0.66312292
    Iteration 198, loss = 0.66328607
    Iteration 199, loss = 0.66309487
    Iteration 200, loss = 0.66375352
    Iteration 201, loss = 0.66324456
    Iteration 202, loss = 0.66237804
    Iteration 203, loss = 0.66218011
    Iteration 204, loss = 0.66420131
    Iteration 205, loss = 0.66309647
    Iteration 206, loss = 0.66350187
    Iteration 207, loss = 0.66348486
    Iteration 208, loss = 0.66286865
    Iteration 209, loss = 0.66219256
    Iteration 210, loss = 0.66253598
    Iteration 211, loss = 0.66339249
    Iteration 212, loss = 0.66434717
    Iteration 213, loss = 0.66353509
    Iteration 214, loss = 0.66217078
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=10, batch_size=auto, hidden_layer_sizes=(20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=True; total time=15.5min
    Iteration 123, loss = 0.68123717
    Iteration 124, loss = 0.68097950
    Iteration 125, loss = 0.68172889
    Iteration 126, loss = 0.68035611
    Iteration 127, loss = 0.68142587
    Iteration 128, loss = 0.68050031
    Iteration 129, loss = 0.68065783
    Iteration 130, loss = 0.68210202
    Iteration 131, loss = 0.68035978
    Iteration 132, loss = 0.67984985
    Iteration 133, loss = 0.68095309
    Iteration 134, loss = 0.67978183
    Iteration 135, loss = 0.68035586
    Iteration 136, loss = 0.68107316
    Iteration 137, loss = 0.67925299
    Iteration 138, loss = 0.68031730
    Iteration 139, loss = 0.68125423
    Iteration 140, loss = 0.68015618
    Iteration 141, loss = 0.67905392
    Iteration 142, loss = 0.67946569
    Iteration 143, loss = 0.67963863
    Iteration 144, loss = 0.67903906
    Iteration 145, loss = 0.67967449
    Iteration 146, loss = 0.67968267
    Iteration 147, loss = 0.67842681
    Iteration 148, loss = 0.67949922
    Iteration 149, loss = 0.67807213
    Iteration 150, loss = 0.67884751
    Iteration 151, loss = 0.67896410
    Iteration 152, loss = 0.67884457
    Iteration 153, loss = 0.67722157
    Iteration 154, loss = 0.67770104
    Iteration 155, loss = 0.67780656
    Iteration 156, loss = 0.67778764
    Iteration 157, loss = 0.67751276
    Iteration 158, loss = 0.67777243
    Iteration 159, loss = 0.67794496
    Iteration 160, loss = 0.67742499
    Iteration 161, loss = 0.67910301
    Iteration 162, loss = 0.67746237
    Iteration 163, loss = 0.67765558
    Iteration 164, loss = 0.67753824
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.



```python
print(gs.best_estimator_)
print('R2 training: ', gs.score(X_train, y_train))
print('R2 CV training: ', gs.best_score_)
print('R2 test: ', gs.score(X_test, y_test))
```

    MLPClassifier(alpha=10, hidden_layer_sizes=(20, 20, 20), max_iter=500,
                  random_state=42, verbose=True, warm_start=True)
    R2 training:  0.8366856904231625
    R2 CV training:  0.7674345768374164
    R2 test:  0.7585247042449548


#### 9.3.9. Neural Network modes: Multilayer Perceptron Feed-Forward network - activation: 'relu', Grid Search SNOW NLP
- increased tvec features to 10k
- max_feat: 50k
- activation: 'relu', 
- hidden_layer: [(20,20,20)]
- alpha: [10],
- max iterations: default (400)

Results:
![image.png](attachment:image.png)
![image-2.png](attachment:image-2.png)


```python

# Tokenizer using JieBa to account for Mandarin
def snowtok(x):
    return SnowNLP(x).words
```


```python
# import data file
df_tracks_master_clean3_lyrics_genre_hsk_v2 = pd.read_csv(
    '/Users/stuart/Desktop/Spoty-Linguist-Project/df_tracks_master_clean3_lyrics_genre_hsk_v2.csv',
    sep=',',
    index_col=0)
```


```python
# Create HSK5 binary variable
df_h5 = df_tracks_master_clean3_lyrics_genre_hsk_v2.copy()
df_h5['is_h5'] = 'NotHSK'
df_h5['is_h5'][df_tracks_master_clean3_lyrics_genre_hsk_v2.hsk5 > 0.9] = 'HSK5'
X = df_h5[['lyrics','is_h5'
]]
y = X.pop('is_h5')
X = X['lyrics']
df_h5['is_h5'].value_counts()
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    stratify=y,
                                                    test_size=0.2,
                                                    random_state=1)

from sklearn.model_selection import GridSearchCV

from sklearn.neural_network import MLPClassifier

# Tokenize lyrics
cvec = CountVectorizer(tokenizer=snowtok, max_features=50000)
cvec.fit(X_train)

# Transform training data
cvec_mat = cvec.transform(X_train)
X_train = cvec_mat
X_test = cvec.transform(X_test)
```

    Iteration 1, loss = 1.91325684
    Iteration 2, loss = 1.39304200
    Iteration 3, loss = 1.19232611
    Iteration 4, loss = 1.10263725
    Iteration 5, loss = 1.06149409
    Iteration 6, loss = 1.03916955
    Iteration 7, loss = 1.02889977
    Iteration 8, loss = 1.01976330
    Iteration 9, loss = 1.01456397
    Iteration 10, loss = 1.00789111
    Iteration 11, loss = 1.00541002
    Iteration 12, loss = 0.99992146
    Iteration 13, loss = 0.99761200
    Iteration 14, loss = 0.99479240
    Iteration 15, loss = 0.99181690
    Iteration 16, loss = 0.98946953
    Iteration 17, loss = 0.98813744
    Iteration 18, loss = 0.98466294
    Iteration 19, loss = 0.98106277
    Iteration 20, loss = 0.97904516
    Iteration 21, loss = 0.97811869
    Iteration 22, loss = 0.97467924
    Iteration 23, loss = 0.97266005
    Iteration 24, loss = 0.96918084
    Iteration 25, loss = 0.96999102
    Iteration 26, loss = 0.96539418
    Iteration 27, loss = 0.96374379
    Iteration 28, loss = 0.96327806
    Iteration 29, loss = 0.96080431
    Iteration 30, loss = 0.95861784
    Iteration 31, loss = 0.95722914
    Iteration 32, loss = 0.95677249
    Iteration 33, loss = 0.95427789
    Iteration 34, loss = 0.95271861
    Iteration 35, loss = 0.95076538
    Iteration 36, loss = 0.95053229
    Iteration 37, loss = 0.94795911
    Iteration 38, loss = 0.94660885
    Iteration 39, loss = 0.94661818
    Iteration 40, loss = 0.94365291
    Iteration 41, loss = 0.94335908
    Iteration 42, loss = 0.94238695
    Iteration 43, loss = 0.94052515
    Iteration 44, loss = 0.94142641
    Iteration 45, loss = 0.93930777
    Iteration 46, loss = 0.93909789
    Iteration 47, loss = 0.93690977
    Iteration 48, loss = 0.93777177
    Iteration 49, loss = 0.93646381
    Iteration 50, loss = 0.93742763
    Iteration 51, loss = 0.93522254
    Iteration 52, loss = 0.93584021
    Iteration 53, loss = 0.93467017
    Iteration 54, loss = 0.93483204
    Iteration 55, loss = 0.93391710
    Iteration 56, loss = 0.93360420
    Iteration 57, loss = 0.93289343
    Iteration 58, loss = 0.93367298
    Iteration 59, loss = 0.93231709
    Iteration 60, loss = 0.93343245
    Iteration 61, loss = 0.93337716
    Iteration 62, loss = 0.93215247
    Iteration 63, loss = 0.93252084
    Iteration 64, loss = 0.93117899
    Iteration 65, loss = 0.93200909
    Iteration 66, loss = 0.93185536
    Iteration 67, loss = 0.93034401
    Iteration 68, loss = 0.93153571
    Iteration 69, loss = 0.93129371
    Iteration 70, loss = 0.93029771
    Iteration 71, loss = 0.93127519
    Iteration 72, loss = 0.92932125
    Iteration 73, loss = 0.92951498
    Iteration 74, loss = 0.93138886
    Iteration 75, loss = 0.93049979
    Iteration 76, loss = 0.93006229
    Iteration 77, loss = 0.93074051
    Iteration 78, loss = 0.93061004
    Iteration 79, loss = 0.92988756
    Iteration 80, loss = 0.92900311
    Iteration 81, loss = 0.92921495
    Iteration 82, loss = 0.92953504
    Iteration 83, loss = 0.92971942
    Iteration 84, loss = 0.92902865
    Iteration 85, loss = 0.92885029
    Iteration 86, loss = 0.92890971
    Iteration 87, loss = 0.92902811
    Iteration 88, loss = 0.92879963
    Iteration 89, loss = 0.92927777
    Iteration 90, loss = 0.92804510
    Iteration 91, loss = 0.92743544
    Iteration 92, loss = 0.92940638
    Iteration 93, loss = 0.92857067
    Iteration 94, loss = 0.92821384
    Iteration 95, loss = 0.92801300
    Iteration 96, loss = 0.92857713
    Iteration 97, loss = 0.92701162
    Iteration 98, loss = 0.92797677
    Iteration 99, loss = 0.92805126
    Iteration 100, loss = 0.92803280
    Iteration 101, loss = 0.92748628
    Iteration 102, loss = 0.92780481
    Iteration 103, loss = 0.92558711
    Iteration 104, loss = 0.92644172
    Iteration 105, loss = 0.92717903
    Iteration 106, loss = 0.92578803
    Iteration 107, loss = 0.92658709
    Iteration 108, loss = 0.92579876
    Iteration 109, loss = 0.92577392
    Iteration 110, loss = 0.92533307
    Iteration 111, loss = 0.92529388
    Iteration 112, loss = 0.92477453
    Iteration 113, loss = 0.92531268
    Iteration 114, loss = 0.92400804
    Iteration 115, loss = 0.92432954
    Iteration 116, loss = 0.92396628
    Iteration 117, loss = 0.92284723
    Iteration 118, loss = 0.92278233
    Iteration 119, loss = 0.92217338
    Iteration 120, loss = 0.92237751
    Iteration 121, loss = 0.92028840
    Iteration 122, loss = 0.91955671
    Iteration 123, loss = 0.91877698
    Iteration 124, loss = 0.91870230
    Iteration 125, loss = 0.91781520
    Iteration 126, loss = 0.91689388
    Iteration 127, loss = 0.91716369
    Iteration 128, loss = 0.91641796
    Iteration 129, loss = 0.91613270
    Iteration 130, loss = 0.91583376
    Iteration 131, loss = 0.91435376
    Iteration 132, loss = 0.91408008
    Iteration 133, loss = 0.91431335
    Iteration 134, loss = 0.91361552
    Iteration 135, loss = 0.91249284
    Iteration 136, loss = 0.91242225
    Iteration 137, loss = 0.91172004
    Iteration 138, loss = 0.91245929
    Iteration 139, loss = 0.91487593
    Iteration 140, loss = 0.91077652
    Iteration 141, loss = 0.91026724
    Iteration 142, loss = 0.91127008
    Iteration 143, loss = 0.91028060
    Iteration 144, loss = 0.90962888
    Iteration 145, loss = 0.91018276
    Iteration 146, loss = 0.91012080
    Iteration 147, loss = 0.91002321
    Iteration 148, loss = 0.91059867
    Iteration 149, loss = 0.90906446
    Iteration 150, loss = 0.90951407
    Iteration 151, loss = 0.90926035
    Iteration 152, loss = 0.90865167
    Iteration 153, loss = 0.90979074
    Iteration 154, loss = 0.90947602
    Iteration 155, loss = 0.91004164
    Iteration 156, loss = 0.90869715
    Iteration 157, loss = 0.90879938
    Iteration 158, loss = 0.90867129
    Iteration 159, loss = 0.90809574
    Iteration 160, loss = 0.90927338
    Iteration 161, loss = 0.90872525
    Iteration 162, loss = 0.90901802
    Iteration 163, loss = 0.90913959
    Iteration 164, loss = 0.90864431
    Iteration 165, loss = 0.90837636
    Iteration 166, loss = 0.90842662
    Iteration 167, loss = 0.90933694
    Iteration 168, loss = 0.90827954
    Iteration 169, loss = 0.90875612
    Iteration 170, loss = 0.90797313
    Iteration 171, loss = 0.90849365
    Iteration 172, loss = 0.90962072
    Iteration 173, loss = 0.90774144
    Iteration 174, loss = 0.90971353
    Iteration 175, loss = 0.90847400
    Iteration 176, loss = 0.90896482
    Iteration 177, loss = 0.90801200
    Iteration 178, loss = 0.90978070
    Iteration 179, loss = 0.90838578
    Iteration 180, loss = 0.90842589
    Iteration 181, loss = 0.90803331
    Iteration 182, loss = 0.90812960
    Iteration 183, loss = 0.90788086
    Iteration 184, loss = 0.90839443
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=10, batch_size=auto, hidden_layer_sizes=(20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=False; total time=13.6min



```python
params = {'solver':['adam'],
                'alpha':[10],
                'hidden_layer_sizes':[(20,20,20)],
                'activation':['relu'],
                'random_state':[42],
                'batch_size':['auto'],
                'max_iter':[500], 'verbose':[True],'warm_start':[True]}


skf = StratifiedKFold(n_splits=4)
skf.get_n_splits(X_train, y_train)



gs = GridSearchCV(MLPClassifier(), param_grid=params, cv=skf, n_jobs=-1, verbose=2)
gs.fit(X_train, y_train)

model = gs.best_estimator_
```

    Fitting 4 folds for each of 1 candidates, totalling 4 fits
    Iteration 1, loss = 1.34570040
    Iteration 2, loss = 0.88768400
    Iteration 3, loss = 0.75830567
    Iteration 4, loss = 0.72072509
    Iteration 5, loss = 0.70902406
    Iteration 6, loss = 0.70411581
    Iteration 7, loss = 0.70288532
    Iteration 8, loss = 0.70022518
    Iteration 9, loss = 0.69849186
    Iteration 10, loss = 0.70018484
    Iteration 11, loss = 0.69704391
    Iteration 12, loss = 0.69592386
    Iteration 13, loss = 0.69542620
    Iteration 14, loss = 0.69506114
    Iteration 15, loss = 0.69505888
    Iteration 1, loss = 1.43178663
    Iteration 2, loss = 0.97106519
    Iteration 3, loss = 0.80632169
    Iteration 4, loss = 0.73937054
    Iteration 5, loss = 0.71260413
    Iteration 6, loss = 0.70507669
    Iteration 7, loss = 0.69902833
    Iteration 8, loss = 0.69831433
    Iteration 9, loss = 0.69917195
    Iteration 10, loss = 0.69361753
    Iteration 11, loss = 0.69300973
    Iteration 12, loss = 0.69261238
    Iteration 13, loss = 0.69314095
    Iteration 14, loss = 0.69225334
    Iteration 15, loss = 0.69142419
    Iteration 16, loss = 0.69038157
    Iteration 17, loss = 0.69169314
    Iteration 18, loss = 0.69008947
    Iteration 19, loss = 0.68914276
    Iteration 20, loss = 0.68863624
    Iteration 21, loss = 0.68481913
    Iteration 22, loss = 0.68615379
    Iteration 23, loss = 0.68679906
    Iteration 24, loss = 0.68341377
    Iteration 25, loss = 0.68452850
    Iteration 26, loss = 0.68221927
    Iteration 27, loss = 0.68183239
    Iteration 28, loss = 0.67834121
    Iteration 29, loss = 0.67936095
    Iteration 30, loss = 0.67695820
    Iteration 31, loss = 0.67620166
    Iteration 32, loss = 0.67492617
    Iteration 33, loss = 0.67514607
    Iteration 34, loss = 0.67403462
    Iteration 35, loss = 0.67195236
    Iteration 36, loss = 0.67084750
    Iteration 37, loss = 0.67036698
    Iteration 38, loss = 0.66997099
    Iteration 39, loss = 0.66786411
    Iteration 40, loss = 0.66819633
    Iteration 41, loss = 0.66561313
    Iteration 42, loss = 0.66640289
    Iteration 43, loss = 0.66462327
    Iteration 44, loss = 0.66472461
    Iteration 45, loss = 0.66526780
    Iteration 46, loss = 0.66182116
    Iteration 47, loss = 0.66234682
    Iteration 48, loss = 0.66180060
    Iteration 49, loss = 0.65917420
    Iteration 50, loss = 0.66017847
    Iteration 51, loss = 0.65958333
    Iteration 52, loss = 0.65999812
    Iteration 53, loss = 0.65743710
    Iteration 54, loss = 0.65941940
    Iteration 55, loss = 0.65724929
    Iteration 56, loss = 0.65819754
    Iteration 57, loss = 0.65718979
    Iteration 58, loss = 0.65516528
    Iteration 59, loss = 0.65744501
    Iteration 60, loss = 0.65418923
    Iteration 61, loss = 0.65712410
    Iteration 62, loss = 0.65576352
    Iteration 63, loss = 0.65565510
    Iteration 64, loss = 0.65284747
    Iteration 65, loss = 0.65543391
    Iteration 66, loss = 0.65519462
    Iteration 67, loss = 0.65460093
    Iteration 68, loss = 0.65396186
    Iteration 69, loss = 0.65380076
    Iteration 70, loss = 0.65274919
    Iteration 71, loss = 0.65411993
    Iteration 72, loss = 0.65304296
    Iteration 73, loss = 0.65309483
    Iteration 74, loss = 0.65312994
    Iteration 75, loss = 0.65280117
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=10, batch_size=auto, hidden_layer_sizes=(20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=True; total time= 7.2min
    Iteration 16, loss = 0.69466527
    Iteration 17, loss = 0.69213298
    Iteration 18, loss = 0.69342138
    Iteration 19, loss = 0.69076660
    Iteration 20, loss = 0.69163042
    Iteration 21, loss = 0.69120063
    Iteration 22, loss = 0.69125664
    Iteration 23, loss = 0.69056628
    Iteration 24, loss = 0.69027631
    Iteration 25, loss = 0.69029760
    Iteration 26, loss = 0.68933743
    Iteration 27, loss = 0.68917772
    Iteration 28, loss = 0.68718676
    Iteration 29, loss = 0.68791680
    Iteration 30, loss = 0.68830452
    Iteration 31, loss = 0.68613561
    Iteration 32, loss = 0.68861723
    Iteration 33, loss = 0.68717670
    Iteration 34, loss = 0.68697027
    Iteration 35, loss = 0.68573933
    Iteration 36, loss = 0.68641405
    Iteration 37, loss = 0.68558354
    Iteration 38, loss = 0.68480682
    Iteration 39, loss = 0.68483230
    Iteration 40, loss = 0.68219016
    Iteration 41, loss = 0.68313361
    Iteration 42, loss = 0.68111607
    Iteration 43, loss = 0.68229821
    Iteration 44, loss = 0.67981030
    Iteration 45, loss = 0.67871826
    Iteration 46, loss = 0.67865175
    Iteration 47, loss = 0.67746371
    Iteration 48, loss = 0.67681970
    Iteration 49, loss = 0.67667038
    Iteration 50, loss = 0.67501401
    Iteration 51, loss = 0.67292913
    Iteration 52, loss = 0.67522533
    Iteration 53, loss = 0.67406399
    Iteration 54, loss = 0.67383364
    Iteration 55, loss = 0.67273414
    Iteration 56, loss = 0.67233860
    Iteration 57, loss = 0.67229879
    Iteration 58, loss = 0.67294684
    Iteration 59, loss = 0.67130553
    Iteration 60, loss = 0.67116984
    Iteration 61, loss = 0.67172107
    Iteration 62, loss = 0.67192187
    Iteration 63, loss = 0.67061538
    Iteration 64, loss = 0.66924513
    Iteration 65, loss = 0.67193875
    Iteration 1, loss = 1.43272988
    Iteration 2, loss = 0.98007862
    Iteration 3, loss = 0.81035478
    Iteration 4, loss = 0.74426047
    Iteration 5, loss = 0.71985652
    Iteration 6, loss = 0.70680712
    Iteration 7, loss = 0.70417040
    Iteration 8, loss = 0.70301646
    Iteration 9, loss = 0.69953696
    Iteration 10, loss = 0.69847409
    Iteration 11, loss = 0.69907295
    Iteration 12, loss = 0.69714710
    Iteration 13, loss = 0.69734704
    Iteration 14, loss = 0.69703714
    Iteration 15, loss = 0.69559659
    Iteration 16, loss = 0.69408389
    Iteration 17, loss = 0.69182815
    Iteration 18, loss = 0.69177335
    Iteration 19, loss = 0.69137491
    Iteration 20, loss = 0.69038416
    Iteration 21, loss = 0.69014857
    Iteration 22, loss = 0.68951255
    Iteration 23, loss = 0.68744733
    Iteration 24, loss = 0.68792863
    Iteration 25, loss = 0.68807955
    Iteration 26, loss = 0.68464377
    Iteration 27, loss = 0.68472969
    Iteration 28, loss = 0.68373435
    Iteration 29, loss = 0.68401706
    Iteration 30, loss = 0.68403985
    Iteration 31, loss = 0.68171927
    Iteration 32, loss = 0.68175443
    Iteration 33, loss = 0.67909687
    Iteration 34, loss = 0.67855534
    Iteration 35, loss = 0.68096810
    Iteration 36, loss = 0.67741714
    Iteration 37, loss = 0.67632341
    Iteration 38, loss = 0.67755276
    Iteration 39, loss = 0.67606363
    Iteration 40, loss = 0.67366453
    Iteration 41, loss = 0.67223807
    Iteration 42, loss = 0.67180676
    Iteration 43, loss = 0.67307011
    Iteration 44, loss = 0.66933420
    Iteration 45, loss = 0.67004629
    Iteration 46, loss = 0.67110915
    Iteration 47, loss = 0.66823284
    Iteration 48, loss = 0.66817646
    Iteration 49, loss = 0.66729725
    Iteration 50, loss = 0.66643346
    Iteration 51, loss = 0.66767371
    Iteration 52, loss = 0.66618318
    Iteration 53, loss = 0.66443458
    Iteration 54, loss = 0.66492858
    Iteration 55, loss = 0.66341292
    Iteration 56, loss = 0.66310135
    Iteration 57, loss = 0.66165017
    Iteration 58, loss = 0.66410884
    Iteration 59, loss = 0.66207674
    Iteration 60, loss = 0.66141696
    Iteration 61, loss = 0.66304779
    Iteration 62, loss = 0.65997740
    Iteration 63, loss = 0.66023398
    Iteration 64, loss = 0.66015754
    Iteration 65, loss = 0.65973498
    Iteration 66, loss = 0.65868093
    Iteration 67, loss = 0.65904132
    Iteration 68, loss = 0.66146434
    Iteration 69, loss = 0.65705378
    Iteration 70, loss = 0.65813343
    Iteration 71, loss = 0.65803726
    Iteration 72, loss = 0.65798661
    Iteration 73, loss = 0.65777617
    Iteration 74, loss = 0.65617264
    Iteration 75, loss = 0.65699422
    Iteration 76, loss = 0.65586780
    Iteration 77, loss = 0.65620742
    Iteration 78, loss = 0.65727421
    Iteration 79, loss = 0.65682620
    Iteration 80, loss = 0.65665713
    Iteration 81, loss = 0.65538249
    Iteration 82, loss = 0.65697248
    Iteration 83, loss = 0.65712830
    Iteration 84, loss = 0.65477444
    Iteration 85, loss = 0.65591320
    Iteration 86, loss = 0.65694791
    Iteration 87, loss = 0.65521237
    Iteration 88, loss = 0.65523831
    Iteration 89, loss = 0.65623772
    Iteration 90, loss = 0.65618787
    Iteration 91, loss = 0.65461787
    Iteration 92, loss = 0.65672801
    Iteration 93, loss = 0.65424034
    Iteration 94, loss = 0.65270878
    Iteration 95, loss = 0.65457592
    Iteration 96, loss = 0.65367911
    Iteration 97, loss = 0.65425703
    Iteration 98, loss = 0.65341269
    Iteration 99, loss = 0.65492352
    Iteration 100, loss = 0.65360495
    Iteration 101, loss = 0.65464602
    Iteration 102, loss = 0.65417699
    Iteration 103, loss = 0.65354993
    Iteration 104, loss = 0.65382836
    Iteration 105, loss = 0.65270948
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=10, batch_size=auto, hidden_layer_sizes=(20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=True; total time= 9.3min
    Iteration 66, loss = 0.66858412
    Iteration 67, loss = 0.67049710
    Iteration 68, loss = 0.66888956
    Iteration 69, loss = 0.66880296
    Iteration 70, loss = 0.66918747
    Iteration 71, loss = 0.66793652
    Iteration 72, loss = 0.66828100
    Iteration 73, loss = 0.66945378
    Iteration 74, loss = 0.66713232
    Iteration 75, loss = 0.66763186
    Iteration 76, loss = 0.66701897
    Iteration 77, loss = 0.66783687
    Iteration 78, loss = 0.66721623
    Iteration 79, loss = 0.66604143
    Iteration 80, loss = 0.66661995
    Iteration 1, loss = 1.43159590
    Iteration 2, loss = 0.97384152
    Iteration 3, loss = 0.80594664
    Iteration 4, loss = 0.74174614
    Iteration 5, loss = 0.71359353
    Iteration 6, loss = 0.70630178
    Iteration 7, loss = 0.70057566
    Iteration 8, loss = 0.70232439
    Iteration 9, loss = 0.69502154
    Iteration 10, loss = 0.69487631
    Iteration 11, loss = 0.69516909
    Iteration 12, loss = 0.69171271
    Iteration 13, loss = 0.69422712
    Iteration 14, loss = 0.69304620
    Iteration 15, loss = 0.69174291
    Iteration 16, loss = 0.69027880
    Iteration 17, loss = 0.69035512
    Iteration 18, loss = 0.69020704
    Iteration 19, loss = 0.68541458
    Iteration 20, loss = 0.68694755
    Iteration 21, loss = 0.68497722
    Iteration 22, loss = 0.68485329
    Iteration 23, loss = 0.68377497
    Iteration 24, loss = 0.68477426
    Iteration 25, loss = 0.68108800
    Iteration 26, loss = 0.68133613
    Iteration 27, loss = 0.68106016
    Iteration 28, loss = 0.67946694
    Iteration 29, loss = 0.68152434
    Iteration 30, loss = 0.67762076
    Iteration 31, loss = 0.67587956
    Iteration 32, loss = 0.67603835
    Iteration 33, loss = 0.67462810
    Iteration 34, loss = 0.67569157
    Iteration 35, loss = 0.67224118
    Iteration 36, loss = 0.67253261
    Iteration 37, loss = 0.67077876
    Iteration 38, loss = 0.67124730
    Iteration 39, loss = 0.66801893
    Iteration 40, loss = 0.66902932
    Iteration 41, loss = 0.66648321
    Iteration 42, loss = 0.66457095
    Iteration 43, loss = 0.66598363
    Iteration 44, loss = 0.66442531
    Iteration 45, loss = 0.66565953
    Iteration 46, loss = 0.66141868
    Iteration 47, loss = 0.66206396
    Iteration 48, loss = 0.66229179
    Iteration 49, loss = 0.65978341
    Iteration 50, loss = 0.65950659
    Iteration 51, loss = 0.65988031
    Iteration 52, loss = 0.65880702
    Iteration 53, loss = 0.65835521
    Iteration 54, loss = 0.65869021
    Iteration 55, loss = 0.65828672
    Iteration 56, loss = 0.65597707
    Iteration 57, loss = 0.65561876
    Iteration 58, loss = 0.65526208
    Iteration 59, loss = 0.65653095
    Iteration 60, loss = 0.65521480
    Iteration 61, loss = 0.65498620
    Iteration 62, loss = 0.65527968
    Iteration 63, loss = 0.65369423
    Iteration 64, loss = 0.65226609
    Iteration 65, loss = 0.65383392
    Iteration 66, loss = 0.65392573
    Iteration 67, loss = 0.65341287
    Iteration 68, loss = 0.65197547
    Iteration 69, loss = 0.65220071
    Iteration 70, loss = 0.65202964
    Iteration 71, loss = 0.65199174
    Iteration 72, loss = 0.65230009
    Iteration 73, loss = 0.65382054
    Iteration 74, loss = 0.65132899
    Iteration 75, loss = 0.65082039
    Iteration 76, loss = 0.65232781
    Iteration 77, loss = 0.65035570
    Iteration 78, loss = 0.65168742
    Iteration 79, loss = 0.65134719
    Iteration 80, loss = 0.65143355
    Iteration 81, loss = 0.65085718
    Iteration 82, loss = 0.64980179
    Iteration 83, loss = 0.65038973
    Iteration 84, loss = 0.65104007
    Iteration 85, loss = 0.65008477
    Iteration 86, loss = 0.64993674
    Iteration 87, loss = 0.64940123
    Iteration 88, loss = 0.64847913
    Iteration 89, loss = 0.65089947
    Iteration 90, loss = 0.65016387
    Iteration 91, loss = 0.64982901
    Iteration 92, loss = 0.64846835
    Iteration 93, loss = 0.64880351
    Iteration 94, loss = 0.64880013
    Iteration 95, loss = 0.65048148
    Iteration 96, loss = 0.64807852
    Iteration 97, loss = 0.64883922
    Iteration 98, loss = 0.64886329
    Iteration 99, loss = 0.64837479
    Iteration 100, loss = 0.64872213
    Iteration 101, loss = 0.64829299
    Iteration 102, loss = 0.64827807
    Iteration 103, loss = 0.64863986
    Iteration 104, loss = 0.64830708
    Iteration 105, loss = 0.64969266
    Iteration 106, loss = 0.64659370
    Iteration 107, loss = 0.64843141
    Iteration 108, loss = 0.64742907
    Iteration 109, loss = 0.64989802
    Iteration 110, loss = 0.65009666
    Iteration 111, loss = 0.64686066
    Iteration 112, loss = 0.64751429
    Iteration 113, loss = 0.64706721
    Iteration 114, loss = 0.64792408
    Iteration 115, loss = 0.64749256
    Iteration 116, loss = 0.64761056
    Iteration 117, loss = 0.64768953
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=10, batch_size=auto, hidden_layer_sizes=(20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=True; total time= 9.9min
    Iteration 81, loss = 0.66578456
    Iteration 82, loss = 0.66746665
    Iteration 83, loss = 0.66614134
    Iteration 84, loss = 0.66677893
    Iteration 85, loss = 0.66542937
    Iteration 86, loss = 0.66677893
    Iteration 87, loss = 0.66604315
    Iteration 88, loss = 0.66498913
    Iteration 89, loss = 0.66646081
    Iteration 90, loss = 0.66524198
    Iteration 91, loss = 0.66835218
    Iteration 92, loss = 0.66672814
    Iteration 93, loss = 0.66515361
    Iteration 94, loss = 0.66429242
    Iteration 95, loss = 0.66626963
    Iteration 96, loss = 0.66476349
    Iteration 97, loss = 0.66628176
    Iteration 98, loss = 0.66482202
    Iteration 99, loss = 0.66499825
    Iteration 100, loss = 0.66354574
    Iteration 101, loss = 0.66587490
    Iteration 102, loss = 0.66461301
    Iteration 103, loss = 0.66506546
    Iteration 104, loss = 0.66398515
    Iteration 105, loss = 0.66498484
    Iteration 106, loss = 0.66377669
    Iteration 107, loss = 0.66372681
    Iteration 108, loss = 0.66340400
    Iteration 109, loss = 0.66348626
    Iteration 110, loss = 0.66433475
    Iteration 111, loss = 0.66324190
    Iteration 112, loss = 0.66292563
    Iteration 113, loss = 0.66320212
    Iteration 114, loss = 0.66352682
    Iteration 115, loss = 0.66245307
    Iteration 116, loss = 0.66403229
    Iteration 117, loss = 0.66246121
    Iteration 118, loss = 0.66217725
    Iteration 1, loss = 1.43330190
    Iteration 2, loss = 0.97977663
    Iteration 3, loss = 0.81484514
    Iteration 4, loss = 0.74640949
    Iteration 5, loss = 0.71883900
    Iteration 6, loss = 0.70777178
    Iteration 7, loss = 0.70244581
    Iteration 8, loss = 0.70372797
    Iteration 9, loss = 0.69678144
    Iteration 10, loss = 0.69627452
    Iteration 11, loss = 0.69641067
    Iteration 12, loss = 0.69366109
    Iteration 13, loss = 0.69462994
    Iteration 14, loss = 0.69184477
    Iteration 15, loss = 0.69228402
    Iteration 16, loss = 0.69131762
    Iteration 17, loss = 0.68898993
    Iteration 18, loss = 0.68985532
    Iteration 19, loss = 0.68639080
    Iteration 20, loss = 0.68981515
    Iteration 21, loss = 0.68618101
    Iteration 22, loss = 0.68663687
    Iteration 23, loss = 0.68616370
    Iteration 24, loss = 0.68711660
    Iteration 25, loss = 0.68320533
    Iteration 26, loss = 0.68344133
    Iteration 27, loss = 0.68295881
    Iteration 28, loss = 0.68135405
    Iteration 29, loss = 0.68205525
    Iteration 30, loss = 0.68178772
    Iteration 31, loss = 0.67859915
    Iteration 32, loss = 0.67999199
    Iteration 33, loss = 0.67687196
    Iteration 34, loss = 0.67605812
    Iteration 35, loss = 0.67743659
    Iteration 36, loss = 0.67487073
    Iteration 37, loss = 0.67404159
    Iteration 38, loss = 0.67375177
    Iteration 39, loss = 0.67182508
    Iteration 40, loss = 0.67193620
    Iteration 41, loss = 0.66939040
    Iteration 42, loss = 0.66730920
    Iteration 43, loss = 0.67029909
    Iteration 44, loss = 0.66695237
    Iteration 45, loss = 0.66621570
    Iteration 46, loss = 0.66437287
    Iteration 47, loss = 0.66460631
    Iteration 48, loss = 0.66325983
    Iteration 49, loss = 0.66211588
    Iteration 50, loss = 0.66198387
    Iteration 51, loss = 0.66212764
    Iteration 52, loss = 0.66175522
    Iteration 53, loss = 0.65961278
    Iteration 54, loss = 0.66020965
    Iteration 55, loss = 0.65934282
    Iteration 56, loss = 0.65953608
    Iteration 57, loss = 0.65765700
    Iteration 58, loss = 0.65689978
    Iteration 59, loss = 0.65729151
    Iteration 60, loss = 0.65565669
    Iteration 61, loss = 0.65827613
    Iteration 62, loss = 0.65665576
    Iteration 63, loss = 0.65645815
    Iteration 64, loss = 0.65527847
    Iteration 65, loss = 0.65761905
    Iteration 66, loss = 0.65516674
    Iteration 67, loss = 0.65516062
    Iteration 68, loss = 0.65607639
    Iteration 69, loss = 0.65440582
    Iteration 70, loss = 0.65431036
    Iteration 71, loss = 0.65354805
    Iteration 72, loss = 0.65464959
    Iteration 73, loss = 0.65374915
    Iteration 74, loss = 0.65378755
    Iteration 75, loss = 0.65292227
    Iteration 76, loss = 0.65290919
    Iteration 77, loss = 0.65290693
    Iteration 78, loss = 0.65383915
    Iteration 79, loss = 0.65494766
    Iteration 80, loss = 0.65311024
    Iteration 81, loss = 0.65230260
    Iteration 82, loss = 0.65364417
    Iteration 83, loss = 0.65091220
    Iteration 84, loss = 0.65250635
    Iteration 85, loss = 0.65212125
    Iteration 86, loss = 0.65138293
    Iteration 87, loss = 0.65050107
    Iteration 88, loss = 0.65047110
    Iteration 89, loss = 0.65268336
    Iteration 90, loss = 0.65285420
    Iteration 91, loss = 0.65089669
    Iteration 92, loss = 0.65056441
    Iteration 93, loss = 0.65090887
    Iteration 94, loss = 0.65035508
    Iteration 95, loss = 0.65145105
    Iteration 96, loss = 0.64919822
    Iteration 97, loss = 0.65089446
    Iteration 98, loss = 0.64970091
    Iteration 99, loss = 0.65042031
    Iteration 100, loss = 0.64954887
    Iteration 101, loss = 0.65058447
    Iteration 102, loss = 0.65027523
    Iteration 103, loss = 0.64949482
    Iteration 104, loss = 0.65036056
    Iteration 105, loss = 0.64838563
    Iteration 106, loss = 0.64899314
    Iteration 107, loss = 0.64991346
    Iteration 108, loss = 0.64968278
    Iteration 109, loss = 0.64888460
    Iteration 110, loss = 0.64976393
    Iteration 111, loss = 0.64863423
    Iteration 112, loss = 0.64842773
    Iteration 113, loss = 0.65017690
    Iteration 114, loss = 0.65058836
    Iteration 115, loss = 0.64827888
    Iteration 116, loss = 0.64859973
    Iteration 117, loss = 0.64918565
    Iteration 118, loss = 0.64874575
    Iteration 119, loss = 0.64779181
    Iteration 120, loss = 0.64823196
    Iteration 121, loss = 0.64802865
    Iteration 122, loss = 0.64803167
    Iteration 123, loss = 0.64742238
    Iteration 124, loss = 0.64937116
    Iteration 125, loss = 0.64703225
    Iteration 126, loss = 0.64821680
    Iteration 127, loss = 0.64830864
    Iteration 128, loss = 0.64605596
    Iteration 129, loss = 0.64868376
    Iteration 130, loss = 0.64733645
    Iteration 131, loss = 0.64715408
    Iteration 132, loss = 0.64768200
    Iteration 133, loss = 0.64802936
    Iteration 134, loss = 0.64804798
    Iteration 135, loss = 0.64698429
    Iteration 136, loss = 0.64712127
    Iteration 137, loss = 0.64615397
    Iteration 138, loss = 0.64530520
    Iteration 139, loss = 0.64633388
    Iteration 140, loss = 0.64515513
    Iteration 141, loss = 0.64659865
    Iteration 142, loss = 0.64733641
    Iteration 143, loss = 0.64798312
    Iteration 144, loss = 0.64514759
    Iteration 145, loss = 0.64726004
    Iteration 146, loss = 0.64466214
    Iteration 147, loss = 0.64535736
    Iteration 148, loss = 0.64421798
    Iteration 149, loss = 0.64617857
    Iteration 150, loss = 0.64408953
    Iteration 151, loss = 0.64474475
    Iteration 152, loss = 0.64523057
    Iteration 153, loss = 0.64838541
    Iteration 154, loss = 0.64456476
    Iteration 155, loss = 0.64493280
    Iteration 156, loss = 0.64370808
    Iteration 157, loss = 0.64581924
    Iteration 158, loss = 0.64369123
    Iteration 159, loss = 0.64507559
    Iteration 160, loss = 0.64858207
    Iteration 161, loss = 0.64534641
    Iteration 162, loss = 0.64488027
    Iteration 163, loss = 0.64483529
    Iteration 164, loss = 0.64458510
    Iteration 165, loss = 0.64399610
    Iteration 166, loss = 0.64581984
    Iteration 167, loss = 0.64367246
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=10, batch_size=auto, hidden_layer_sizes=(20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=True; total time=11.6min
    Iteration 119, loss = 0.66222842
    Iteration 120, loss = 0.66220772
    Iteration 121, loss = 0.66391333
    Iteration 122, loss = 0.66313093
    Iteration 123, loss = 0.66241597
    Iteration 124, loss = 0.66215528
    Iteration 125, loss = 0.66265079
    Iteration 126, loss = 0.66123624
    Iteration 127, loss = 0.66243707
    Iteration 128, loss = 0.66095057
    Iteration 129, loss = 0.66143119
    Iteration 130, loss = 0.66214589
    Iteration 131, loss = 0.66172917
    Iteration 132, loss = 0.66103711
    Iteration 133, loss = 0.66220781
    Iteration 134, loss = 0.66126672
    Iteration 135, loss = 0.66174324
    Iteration 136, loss = 0.66210151
    Iteration 137, loss = 0.66052308
    Iteration 138, loss = 0.66095829
    Iteration 139, loss = 0.66275412
    Iteration 140, loss = 0.66186452
    Iteration 141, loss = 0.66067267
    Iteration 142, loss = 0.66108905
    Iteration 143, loss = 0.66139412
    Iteration 144, loss = 0.66092684
    Iteration 145, loss = 0.66245711
    Iteration 146, loss = 0.66155445
    Iteration 147, loss = 0.66102122
    Iteration 148, loss = 0.66247150
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.



```python
print(gs.best_estimator_)
print('R2 training: ', gs.score(X_train, y_train))
print('R2 CV training: ', gs.best_score_)
print('R2 test: ', gs.score(X_test, y_test))
```

    MLPClassifier(alpha=10, hidden_layer_sizes=(20, 20, 20), max_iter=500,
                  random_state=42, verbose=True, warm_start=True)
    R2 training:  0.837451280623608
    R2 CV training:  0.7782224387527841
    R2 test:  0.7684064022268615



```python
logist_plots(gs,
             X_train=X_train,
             y_train=y_train,
             X_test=X_test,
             y_test=y_test)
```

    y_train classification report:
                  precision    recall  f1-score   support
    
            HSK5       0.88      0.80      0.84     15114
          NotHSK       0.80      0.88      0.84     13622
    
        accuracy                           0.84     28736
       macro avg       0.84      0.84      0.84     28736
    weighted avg       0.84      0.84      0.84     28736
    
    y_test classification report:
                  precision    recall  f1-score   support
    
            HSK5       0.82      0.72      0.77      3779
          NotHSK       0.73      0.82      0.77      3406
    
        accuracy                           0.77      7185
       macro avg       0.77      0.77      0.77      7185
    weighted avg       0.77      0.77      0.77      7185
    



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_255_1.png)
    


    ROC Curves:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_255_3.png)
    


    Precision Recall Curve:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_255_5.png)
    


### 9.4. Multi Classification of HSK Based on Lyrics

#### 9.4.1. Try All Models

#### Fastest/Best Model:
- Training: 
- BernoulliNB(alpha=0.01)
- train time: 0.022s
- test time:  0.008s
- accuracy:   0.685
- dimensionality: 50000
- density: 1.000000
- top 10 keywords per class:
- 3: 一天 一起 世界 没 不会 走 不要 知道 没有 爱
- 4: 爱情 不要 不会 没 心 世界 知道 走 没有 爱
- 5: 梦 不会 太 走 混音 没 心 世界 没有 爱
- 6: 走 出 心 监制 录音 声 世界 吉他 爱 混音

- classification report:
confusion matrix:
- [[  98  118    9    1]
- [ 104 1628  680   16]
- [  48  921 3040  167]
- [  20   18  158  159]]
![image.png](attachment:image.png)




```python
# import data file
df_tracks_master_clean3_lyrics_genre_hsk_v2 = pd.read_csv(
    '/Users/stuart/Desktop/Spoty-Linguist-Project/df_tracks_master_clean3_lyrics_genre_hsk_v2.csv',
    sep=',',
    index_col=0)
```


```python
# Create X and y
df_HSK_classification = df_tracks_master_clean3_lyrics_genre_hsk_v2.copy()

X = df_HSK_classification[['lyrics','hsk_level'
]]

y = X.pop('hsk_level')
X=X['lyrics']

```


```python
def hsk3_group(x):
    if x <= 3:
        return 3
    else:
        return x
```


```python
y = y.apply(hsk3_group)
```


```python
y.value_counts()
```




    5    20877
    4    12139
    6     1773
    3     1132
    Name: hsk_level, dtype: int64




```python
# Tokenize lyrics
cvec = CountVectorizer(tokenizer=man_token_jie)
cvec.fit(X)
```




    CountVectorizer(tokenizer=<function man_token_jie at 0x7fdb0b4cc940>)




```python
from time import time
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import RidgeClassifier
from sklearn.pipeline import Pipeline
from sklearn.svm import LinearSVC
from sklearn.linear_model import SGDClassifier
from sklearn.linear_model import Perceptron
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.naive_bayes import BernoulliNB, MultinomialNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors import NearestCentroid
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils.extmath import density
from sklearn.feature_selection import SelectFromModel
from sklearn import metrics

# Get Train test split from sample data
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    stratify=y,
                                                    test_size=0.2,
                                                    random_state=1)

w = {3: np.round(1/1132), 4: np.round(1/12139), 5: np.round(1/20877), 6: np.round(1/1773) }

cvec = CountVectorizer(stop_words=updated_stopwords,
max_features=50000,
tokenizer=man_token_jie)





cvec.fit(X_train)
X_train = cvec.transform(X_train)
X_test = cvec.transform(X_test)


feature_names = np.array(cvec.get_feature_names())


def trim(s):
    """Trim string to fit on terminal (assuming 80-column display)"""
    return s if len(s) <= 80 else s[:77] + "..."

###############################################################################
# Benchmark classifiers


def benchmark(clf):
    print(('_' * 80))
    print("Training: ")
    print(clf)
    t0 = time()
    clf.fit(X_train, y_train)
    train_time = time() - t0
    print(("train time: %0.3fs" % train_time))

    t0 = time()
    pred = clf.predict(X_test)
    test_time = time() - t0
    print(("test time:  %0.3fs" % test_time))

    score = metrics.accuracy_score(y_test, pred)
    print(("accuracy:   %0.3f" % score))

    if hasattr(clf, 'coef_'):
        print(("dimensionality: %d" % clf.coef_.shape[1]))
        print(("density: %f" % density(clf.coef_)))
        try:
            if feature_names is not None:
                print("top 10 keywords per class:")
                for i, category in enumerate(set(y_train.values)):
                    top10 = np.argsort(clf.coef_[i])[-10:]
                    print((trim("%s: %s"
                                % (category, " ".join(feature_names[top10])))))
            print()
        except:
            pass
    print("classification report:")
    try:
        print((metrics.classification_report(y_test, pred,
                                         target_names=set(y_train.values))))
    except:
        pass
    try:
        print("confusion matrix:")
        print((metrics.confusion_matrix(y_test, pred)))

        print()
    except: 
        pass
    try:
        clf_descr = str(clf).split('(')[0]
    except:
        pass
    return clf_descr, score, train_time, test_time


results = []
for clf, name in ((Perceptron(max_iter=1000, tol=1e-3, class_weight=w, n_jobs=-1), "Perceptron"),(PassiveAggressiveClassifier(max_iter=1000, tol=1e-3, class_weight=w,n_jobs=-1), "Passive-Aggressive"),(KNeighborsClassifier(n_neighbors=10, n_jobs=-1), "kNN"),(RandomForestClassifier(n_estimators=100, class_weight=w, n_jobs=-1), "Random forest")):
    print(('=' * 80))
    print(name)
    results.append(benchmark(clf))

for penalty in ["l2", "l1"]:
    print(('=' * 80))
    print(("%s penalty" % penalty.upper()))
    # Train Liblinear model
    results.append(benchmark(LinearSVC(loss='squared_hinge', penalty=penalty,
                                            dual=False)))

    # Train SGD model
    results.append(benchmark(SGDClassifier(alpha=.0001,
                                           penalty=penalty,
                                           max_iter=1000,
                                           tol=1e-3)))

# Train SGD with Elastic Net penalty
print(('=' * 80))
print("Elastic-Net penalty")
results.append(benchmark(SGDClassifier(alpha=.0001,
                                       penalty="elasticnet",
                                       max_iter=1000,
                                       tol=1e-3)))

# Train NearestCentroid without threshold
print(('=' * 80))
print("NearestCentroid (aka Rocchio classifier)")
results.append(benchmark(NearestCentroid()))

# Train sparse Naive Bayes classifiers
print(('=' * 80))
print("Naive Bayes")
results.append(benchmark(MultinomialNB(alpha=.01)))
results.append(benchmark(BernoulliNB(alpha=.01)))

print(('=' * 80))
print("LinearSVC with L1-based feature selection")
# The smaller C, the stronger the regularization.
# The more regularization, the more sparsity.
results.append(benchmark(Pipeline([
    ('feature_selection', SelectFromModel(LinearSVC(penalty="l1", dual=False))),
    ('classification', LinearSVC(penalty="l2"))])))
# make some plots

indices = np.arange(len(results))

results = [[x[i] for x in results] for i in range(4)]

clf_names, score, training_time, test_time = results
training_time = np.array(training_time) / np.max(training_time)
test_time = np.array(test_time) / np.max(test_time)

plt.figure(figsize=(12, 8))
plt.title("Score")
plt.barh(indices, score, .2, label="score", color='r')
plt.barh(indices + .3, training_time, .2, label="training time", color='g')
plt.barh(indices + .6, test_time, .2, label="test time", color='b')
plt.yticks((), fontsize=14)
plt.xticks(fontsize=14)
plt.legend(loc='best')
plt.subplots_adjust(left=.25)
plt.subplots_adjust(top=.95)
plt.subplots_adjust(bottom=.05)

for i, c in zip(indices, clf_names):
    plt.text(-.3, i, c, fontsize=14)

plt.show()
```

    ================================================================================
    Perceptron
    ________________________________________________________________________________
    Training: 
    Perceptron(class_weight={3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0}, n_jobs=-1)
    train time: 0.058s
    test time:  0.004s
    accuracy:   0.319
    dimensionality: 50000
    density: 0.001060
    top 10 keywords per class:
    3: 小猫 小猫咪 小猫猫 小王 小王子 小玩意 小球 小生 小气 龟裂
    4: 小狗 小猪 小猫 小猫咪 小猫猫 小王 小王子 小玩意 小步 龟裂
    5: 小爱 小牛 小狗 小猪 小猫 小猫咪 小猫猫 小王 小桥流水 龟裂
    6: 小猪 小猫 小猫咪 小猫猫 小王 小王子 小玩意 小球 小毛病 龟裂
    
    classification report:
    confusion matrix:
    [[  74   83   59   10]
     [ 516  661 1037  214]
     [1163 1088 1540  385]
     [ 204   85   49   17]]
    
    ================================================================================
    Passive-Aggressive
    ________________________________________________________________________________
    Training: 
    PassiveAggressiveClassifier(class_weight={3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0},
                                n_jobs=-1)
    train time: 0.053s
    test time:  0.004s
    accuracy:   0.450
    dimensionality: 50000
    density: 0.220495
    top 10 keywords per class:
    3: 心向 心和心 心咒 心喜 心喲 心因 心困 心图 心同 龟裂
    4: 微醺 微雨 微雨燕 微震 微風 微风吹拂 德国 德成善 微酸 龟裂
    5: 归西 归还 归途 归队 归隐 归零 当一 当上 归航 龟裂
    6: 往里 往里面 往高雄 征兆 征婚启事 征战 征衣 征讨 往返 龟裂
    
    classification report:
    confusion matrix:
    [[  75   83   58   10]
     [ 537  981  794  116]
     [ 721 1132 2072  251]
     [  56   46  151  102]]
    
    ================================================================================
    kNN
    ________________________________________________________________________________
    Training: 
    KNeighborsClassifier(n_jobs=-1, n_neighbors=10)
    train time: 0.004s
    test time:  6.568s
    accuracy:   0.273
    classification report:
    confusion matrix:
    [[  79   59   20   68]
     [ 815  731  289  593]
     [1026  916  933 1301]
     [  26   34   74  221]]
    
    ================================================================================
    Random forest
    ________________________________________________________________________________
    Training: 
    RandomForestClassifier(class_weight={3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0}, n_jobs=-1)
    train time: 0.179s
    test time:  0.041s
    accuracy:   0.031
    classification report:
    confusion matrix:
    [[ 226    0    0    0]
     [2428    0    0    0]
     [4176    0    0    0]
     [ 355    0    0    0]]
    
    ================================================================================
    L2 penalty
    ________________________________________________________________________________
    Training: 
    LinearSVC(dual=False)
    train time: 72.220s
    test time:  0.003s
    accuracy:   0.681
    dimensionality: 50000
    density: 1.000000
    top 10 keywords per class:
    3: 筷子 快进 空缺 不能自己 得来不易 点名 巡演 山丘 算起 三五个
    4: 关爱 干渴 装著 西行 代谢 无表情 著雨 榕 内脏 没法子
    5: 融融 久别 唱词 转来 天灰 版本 最短 竖琴 峇 搞砸
    6: 芸 监制 吟唱 好时光 焼 红酥手 失眠症 処 进行曲 編曲
    
    classification report:
    confusion matrix:
    [[  69  124   33    0]
     [  89 1535  791   13]
     [  49  855 3179   93]
     [   8   44  190  113]]
    
    ________________________________________________________________________________
    Training: 
    SGDClassifier()
    train time: 1.119s
    test time:  0.004s
    accuracy:   0.697
    dimensionality: 50000
    density: 0.730925
    top 10 keywords per class:
    3: 半个 七天 客官 男该 利亚 算了吧 歌曲 漂着 同胞 好久不见
    4: 亿万年 太多别 这破 低着头 无表情 干渴 两边 阿姆斯壮 面红 没法子
    5: 证实 催 紧绷 浮游 愤怒 世代 纷纷扰扰 野蛮 堆积 笨拙
    6: 腐朽 伸出 波澜 吔 見 南无 啰 呼嘿 叻 噻
    
    classification report:
    confusion matrix:
    [[  51  150   25    0]
     [  74 1572  770   12]
     [  23  806 3271   76]
     [   1   43  196  115]]
    
    ================================================================================
    L1 penalty
    ________________________________________________________________________________
    Training: 
    LinearSVC(dual=False, penalty='l1')
    train time: 33.254s
    test time:  0.003s
    accuracy:   0.689
    dimensionality: 50000
    density: 0.156030
    top 10 keywords per class:
    3: 点名 木村 快进 开导 钱财 好几年 得来不易 幅 反问 努力争取
    4: 又荡来 长短 多要 夺目 没法子 点心 诉出 同期 烟硝 哭哭笑笑
    5: 书桌上 报以 浮游 不合时宜 远地 一触 蓝图 云霞 应是 强盗
    6: 哇哇 芸 詞曲 布谷鸟 曜 時 葉 見 編曲 柯智棠
    
    classification report:
    confusion matrix:
    [[  59  144   23    0]
     [  84 1571  762   11]
     [  37  815 3221  103]
     [   7   41  204  103]]
    
    ________________________________________________________________________________
    Training: 
    SGDClassifier(penalty='l1')
    train time: 26.222s
    test time:  0.003s
    accuracy:   0.673
    dimensionality: 50000
    density: 0.137745
    top 10 keywords per class:
    3: 表白 有何不可 美国 剪短 红毯 讨人厌 甜蜜蜜 从头来 加糖 静下来
    4: 颜 邦戈 甘蔗 冰冰 桃花江 淡 妞 夏威夷 钉子户 蚂蚁
    5: 玛莉 刘秉义 潘必正 拆掉 啾 苏三唱 噗通 谋 王金龙 拄
    6: 应怜 嫂子 嘎啦 気 嘻嘻 揭谛 惊过 欺 长醉 我庄
    
    classification report:
    confusion matrix:
    [[  47  151   28    0]
     [  83 1461  859   25]
     [  45  774 3205  152]
     [   8   29  197  121]]
    
    ================================================================================
    Elastic-Net penalty
    ________________________________________________________________________________
    Training: 
    SGDClassifier(penalty='elasticnet')
    train time: 3.452s
    test time:  0.003s
    accuracy:   0.701
    dimensionality: 50000
    density: 0.277470
    top 10 keywords per class:
    3: 当爱 大衣 山上 漂着 半个 同胞 得来不易 客官 好久不见 利亚
    4: 之书 亿万年 干渴 我共渡 两边 无表情 没法子 面红 这破 阿姆斯壮
    5: 居然 催 志气 挫败 时尚 堆积 世代 野蛮 浮游 笨拙
    6: 嬢 吔 見 啰 诃 娑婆 南无 叻 噻 彌
    
    classification report:
    confusion matrix:
    [[  48  150   28    0]
     [  66 1592  758   12]
     [  23  777 3292   84]
     [   2   42  206  105]]
    
    ================================================================================
    NearestCentroid (aka Rocchio classifier)
    ________________________________________________________________________________
    Training: 
    NearestCentroid()
    train time: 0.029s
    test time:  0.006s
    accuracy:   0.372
    classification report:
    confusion matrix:
    [[  70   51   21   84]
     [ 298  947  556  627]
     [ 190  951 1353 1682]
     [   5   13   37  300]]
    
    ================================================================================
    Naive Bayes
    ________________________________________________________________________________
    Training: 
    MultinomialNB(alpha=0.01)
    train time: 0.013s
    test time:  0.002s
    accuracy:   0.690
    dimensionality: 50000
    density: 1.000000
    top 10 keywords per class:
    3: 世界 一起 快乐 没 不会 不要 走 知道 没有 爱
    4: 不会 没 世界 心 爱情 知道 走 不要 没有 爱
    5: 不会 爱情 太 走 不要 没 心 世界 没有 爱
    6: 笑 阮 声 走 梦 心 不要 混音 世界 爱
    
    classification report:
    confusion matrix:
    [[  43  151   32    0]
     [  32 1514  865   17]
     [   4  810 3256  106]
     [   0   10  197  148]]
    
    ________________________________________________________________________________
    Training: 
    BernoulliNB(alpha=0.01)
    train time: 0.022s
    test time:  0.008s
    accuracy:   0.685
    dimensionality: 50000
    density: 1.000000
    top 10 keywords per class:
    3: 一天 一起 世界 没 不会 走 不要 知道 没有 爱
    4: 爱情 不要 不会 没 心 世界 知道 走 没有 爱
    5: 梦 不会 太 走 混音 没 心 世界 没有 爱
    6: 走 出 心 监制 录音 声 世界 吉他 爱 混音
    
    classification report:
    confusion matrix:
    [[  98  118    9    1]
     [ 104 1628  680   16]
     [  48  921 3040  167]
     [  20   18  158  159]]
    
    ================================================================================
    LinearSVC with L1-based feature selection
    ________________________________________________________________________________
    Training: 
    Pipeline(steps=[('feature_selection',
                     SelectFromModel(estimator=LinearSVC(dual=False,
                                                         penalty='l1'))),
                    ('classification', LinearSVC())])
    train time: 40.130s
    test time:  0.008s
    accuracy:   0.683
    classification report:
    confusion matrix:
    [[  64  132   30    0]
     [  79 1546  784   19]
     [  42  846 3190   98]
     [   7   44  197  107]]
    



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_264_1.png)
    


#### 9.4.2. Grid Search on BernouliNB
![image.png](attachment:image.png)
![image-2.png](attachment:image-2.png)


```python
# import data file
df_tracks_master_clean3_lyrics_genre_hsk_v2 = pd.read_csv(
    '/Users/stuart/Desktop/Spoty-Linguist-Project/df_tracks_master_clean3_lyrics_genre_hsk_v2.csv',
    sep=',',
    index_col=0)
```


```python
# Create X and y
df_HSK_classification = df_tracks_master_clean3_lyrics_genre_hsk_v2.copy()

X = df_HSK_classification[['lyrics','hsk_level'
]]

y = X.pop('hsk_level')
X = X['lyrics']

```


```python
def hsk3_group(x):
    if x <= 3:
        return 3
    elif x == 4:
        return 4
    elif x == 5:
        return 5
    elif x == 6:
        return 6
```


```python
y = y.apply(hsk3_group)
```


```python
y.value_counts()
```




    5    20877
    4    12139
    6     1773
    3     1132
    Name: hsk_level, dtype: int64




```python
# Tokenize lyrics
cvec = CountVectorizer(tokenizer=man_token_jie)
cvec.fit(X)

# Get Train test split from sample data
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    stratify=y,
                                                    test_size=0.2,
                                                    random_state=1)

w = {3: np.round(1/1132), 4: np.round(1/12139), 5: np.round(1/20877), 6: np.round(1/1773) }

cvec = CountVectorizer(stop_words=updated_stopwords,
max_features=50000,
tokenizer=man_token_jie)





cvec.fit(X_train)
X_train = cvec.transform(X_train)
X_test = cvec.transform(X_test)

```


```python
from sklearn.model_selection import GridSearchCV
# Get Train test split from sample data


params = {'alpha': [0.01, 0.1, 0.5, 1.0, 10.0],
         }

skf = StratifiedKFold(n_splits=4)
skf.get_n_splits(X_train, y_train)

model = GridSearchCV(BernoulliNB(), param_grid=params, n_jobs=-1, cv=skf, verbose=5)

model.fit(X_train, y_train)


print(Bold + 'Base line score:/n' + Normal,
      np.round(y_train.value_counts(normalize=True), 3))
print(Bold + 'Training Score:' + Normal, np.round(model.score(X_train, y_train),
                                                  3))
print(Bold + 'Test Score:' + Normal, np.round(model.score(X_test, y_test), 3))
print(Bold + "Cross-validated training scores:" + Normal,
      np.round(lr_scores, 4))
print(Bold + "Mean cross-validated training score:" + Normal,
      np.round(lr_scores.mean(), 3), '\n')

logist_plots(model,
             X_train=X_train,
             y_train=y_train,
             X_test=X_test,
             y_test=y_test)

```

    Fitting 4 folds for each of 5 candidates, totalling 20 fits
    [1mBase line score:/n[0m 5    0.581
    4    0.338
    6    0.049
    3    0.032
    Name: hsk_level, dtype: float64
    [1mTraining Score:[0m 0.859
    [1mTest Score:[0m 0.685
    [1mCross-validated training scores:[0m [0.7121 0.6988 0.7052 0.7113 0.6896]
    [1mMean cross-validated training score:[0m 0.703 
    
    y_train classification report:
                  precision    recall  f1-score   support
    
               3       0.62      1.00      0.76       906
               4       0.78      0.91      0.84      9711
               5       0.96      0.82      0.88     16701
               6       0.76      0.92      0.83      1418
    
        accuracy                           0.86     28736
       macro avg       0.78      0.91      0.83     28736
    weighted avg       0.88      0.86      0.86     28736
    
    y_test classification report:
                  precision    recall  f1-score   support
    
               3       0.36      0.43      0.40       226
               4       0.61      0.67      0.64      2428
               5       0.78      0.73      0.75      4176
               6       0.46      0.45      0.46       355
    
        accuracy                           0.69      7185
       macro avg       0.55      0.57      0.56      7185
    weighted avg       0.69      0.69      0.69      7185
    



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_272_1.png)
    


    ROC Curves:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_272_3.png)
    


    Precision Recall Curve:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_272_5.png)
    


#### 9.4.4. TO RUN Neural Network modes: Multilayer Perceptron Feed-Forward network - activation: 'relu', Grid Search
![image.png](attachment:image.png)


```python
from sklearn.model_selection import GridSearchCV
# Get Train test split from sample data

params = {'solver':['adam'],
                'alpha':[10],
                'hidden_layer_sizes':[(20,20,20)],
                'activation':['relu'],
                'random_state':[42],
                'batch_size':['auto'],
                'max_iter':[500], 'verbose':[True],'warm_start':[False]}


skf = StratifiedKFold(n_splits=4)
skf.get_n_splits(X_train, y_train)



gs = GridSearchCV(MLPClassifier(), param_grid=params, cv=skf, n_jobs=-1, verbose=2)
gs.fit(X_train, y_train)

model = gs.best_estimator_
```

    Fitting 4 folds for each of 1 candidates, totalling 4 fits
    Iteration 1, loss = 1.81194088
    Iteration 2, loss = 1.27949234
    Iteration 3, loss = 1.11469619
    Iteration 4, loss = 1.05756040
    Iteration 5, loss = 1.03557702
    Iteration 6, loss = 1.02255827
    Iteration 7, loss = 1.01600807
    Iteration 8, loss = 1.01077285
    Iteration 9, loss = 1.00526487
    Iteration 10, loss = 1.00170086
    Iteration 11, loss = 0.99674681
    Iteration 12, loss = 0.99510047
    Iteration 13, loss = 0.99100381
    Iteration 14, loss = 0.98838621
    Iteration 15, loss = 0.98602989
    Iteration 16, loss = 0.98329116
    Iteration 17, loss = 0.98010685
    Iteration 18, loss = 0.97903095
    Iteration 19, loss = 0.97830609
    Iteration 20, loss = 0.97580157
    Iteration 21, loss = 0.97328774
    Iteration 22, loss = 0.97179357
    Iteration 23, loss = 0.97001263
    Iteration 24, loss = 0.96991397
    Iteration 25, loss = 0.96715826
    Iteration 26, loss = 0.96469610
    Iteration 27, loss = 0.96346226
    Iteration 28, loss = 0.96300918
    Iteration 29, loss = 0.96043473
    Iteration 30, loss = 0.95876683
    Iteration 31, loss = 0.95700847
    Iteration 32, loss = 0.95713756
    Iteration 1, loss = 1.91402688
    Iteration 2, loss = 1.39508932
    Iteration 3, loss = 1.19370462
    Iteration 4, loss = 1.10372250
    Iteration 5, loss = 1.06194609
    Iteration 6, loss = 1.03959443
    Iteration 7, loss = 1.02806717
    Iteration 8, loss = 1.01949401
    Iteration 9, loss = 1.01294094
    Iteration 10, loss = 1.00697650
    Iteration 11, loss = 1.00586353
    Iteration 12, loss = 0.99873040
    Iteration 13, loss = 0.99611176
    Iteration 14, loss = 0.99361214
    Iteration 15, loss = 0.99061438
    Iteration 16, loss = 0.98694555
    Iteration 17, loss = 0.98574494
    Iteration 18, loss = 0.98287966
    Iteration 19, loss = 0.97971368
    Iteration 20, loss = 0.97812642
    Iteration 21, loss = 0.97620466
    Iteration 22, loss = 0.97460952
    Iteration 23, loss = 0.97151645
    Iteration 24, loss = 0.96877812
    Iteration 25, loss = 0.96837793
    Iteration 26, loss = 0.96558798
    Iteration 27, loss = 0.96321737
    Iteration 28, loss = 0.96261547
    Iteration 29, loss = 0.96014473
    Iteration 30, loss = 0.95776889
    Iteration 31, loss = 0.95606617
    Iteration 32, loss = 0.95552146
    Iteration 33, loss = 0.95199984
    Iteration 34, loss = 0.94936943
    Iteration 35, loss = 0.94958799
    Iteration 36, loss = 0.94636049
    Iteration 37, loss = 0.94482224
    Iteration 38, loss = 0.94126293
    Iteration 39, loss = 0.94146421
    Iteration 40, loss = 0.93968355
    Iteration 41, loss = 0.93664066
    Iteration 42, loss = 0.93757971
    Iteration 43, loss = 0.93545729
    Iteration 44, loss = 0.93557117
    Iteration 45, loss = 0.93268019
    Iteration 46, loss = 0.93305048
    Iteration 47, loss = 0.93085859
    Iteration 48, loss = 0.93046517
    Iteration 49, loss = 0.92943009
    Iteration 50, loss = 0.92876391
    Iteration 51, loss = 0.92620343
    Iteration 52, loss = 0.92674480
    Iteration 53, loss = 0.92534428
    Iteration 54, loss = 0.92533062
    Iteration 55, loss = 0.92254677
    Iteration 56, loss = 0.92279103
    Iteration 57, loss = 0.92188740
    Iteration 58, loss = 0.92082157
    Iteration 59, loss = 0.91975047
    Iteration 60, loss = 0.91966767
    Iteration 61, loss = 0.91956089
    Iteration 62, loss = 0.91776846
    Iteration 63, loss = 0.91785223
    Iteration 64, loss = 0.91756615
    Iteration 65, loss = 0.91544073
    Iteration 66, loss = 0.91646873
    Iteration 67, loss = 0.91482355
    Iteration 68, loss = 0.91564562
    Iteration 69, loss = 0.91326206
    Iteration 70, loss = 0.91374688
    Iteration 71, loss = 0.91276518
    Iteration 72, loss = 0.91225203
    Iteration 73, loss = 0.91186851
    Iteration 74, loss = 0.91251704
    Iteration 75, loss = 0.91204498
    Iteration 76, loss = 0.91036147
    Iteration 77, loss = 0.91107076
    Iteration 78, loss = 0.91073825
    Iteration 79, loss = 0.91090764
    Iteration 80, loss = 0.90971245
    Iteration 81, loss = 0.90986016
    Iteration 82, loss = 0.90953139
    Iteration 83, loss = 0.90922042
    Iteration 84, loss = 0.90957744
    Iteration 85, loss = 0.90937942
    Iteration 86, loss = 0.90969698
    Iteration 87, loss = 0.90915675
    Iteration 88, loss = 0.90941498
    Iteration 89, loss = 0.90936956
    Iteration 90, loss = 0.90901477
    Iteration 91, loss = 0.90784196
    Iteration 92, loss = 0.90813883
    Iteration 93, loss = 0.90847854
    Iteration 94, loss = 0.90940569
    Iteration 95, loss = 0.90857956
    Iteration 96, loss = 0.90888983
    Iteration 97, loss = 0.90827795
    Iteration 98, loss = 0.90775067
    Iteration 99, loss = 0.90893547
    Iteration 100, loss = 0.90819447
    Iteration 101, loss = 0.90899663
    Iteration 102, loss = 0.90824780
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=10, batch_size=auto, hidden_layer_sizes=(20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=False; total time= 9.9min
    Iteration 33, loss = 0.95492751
    Iteration 34, loss = 0.95466296
    Iteration 35, loss = 0.95283192
    Iteration 36, loss = 0.95079087
    Iteration 37, loss = 0.95211822
    Iteration 38, loss = 0.94898119
    Iteration 39, loss = 0.94864671
    Iteration 40, loss = 0.94780427
    Iteration 41, loss = 0.94704263
    Iteration 42, loss = 0.94651388
    Iteration 43, loss = 0.94498893
    Iteration 1, loss = 1.91218321
    Iteration 2, loss = 1.39207645
    Iteration 3, loss = 1.19201889
    Iteration 4, loss = 1.10182826
    Iteration 5, loss = 1.05989521
    Iteration 6, loss = 1.03979518
    Iteration 7, loss = 1.02707649
    Iteration 8, loss = 1.02014491
    Iteration 9, loss = 1.01400274
    Iteration 10, loss = 1.00732721
    Iteration 11, loss = 1.00518810
    Iteration 12, loss = 0.99943002
    Iteration 13, loss = 0.99816481
    Iteration 14, loss = 0.99295209
    Iteration 15, loss = 0.99232081
    Iteration 16, loss = 0.98828866
    Iteration 17, loss = 0.98620392
    Iteration 18, loss = 0.98398394
    Iteration 19, loss = 0.98090930
    Iteration 20, loss = 0.97872043
    Iteration 21, loss = 0.97907520
    Iteration 22, loss = 0.97409441
    Iteration 23, loss = 0.97211835
    Iteration 24, loss = 0.96990673
    Iteration 25, loss = 0.96976097
    Iteration 26, loss = 0.96731037
    Iteration 27, loss = 0.96717119
    Iteration 28, loss = 0.96630547
    Iteration 29, loss = 0.96330605
    Iteration 30, loss = 0.96267017
    Iteration 31, loss = 0.96119588
    Iteration 32, loss = 0.96221550
    Iteration 33, loss = 0.96102035
    Iteration 34, loss = 0.95903231
    Iteration 35, loss = 0.95875992
    Iteration 36, loss = 0.95946832
    Iteration 37, loss = 0.95663548
    Iteration 38, loss = 0.95610822
    Iteration 39, loss = 0.95595598
    Iteration 40, loss = 0.95410717
    Iteration 41, loss = 0.95432392
    Iteration 42, loss = 0.95271205
    Iteration 43, loss = 0.95257037
    Iteration 44, loss = 0.95108658
    Iteration 45, loss = 0.95022444
    Iteration 46, loss = 0.94746080
    Iteration 47, loss = 0.94603179
    Iteration 48, loss = 0.94572660
    Iteration 49, loss = 0.94398647
    Iteration 50, loss = 0.94501746
    Iteration 51, loss = 0.94029667
    Iteration 52, loss = 0.94138730
    Iteration 53, loss = 0.93813984
    Iteration 54, loss = 0.93741775
    Iteration 55, loss = 0.93560760
    Iteration 56, loss = 0.93562894
    Iteration 57, loss = 0.93354589
    Iteration 58, loss = 0.93353490
    Iteration 59, loss = 0.93182713
    Iteration 60, loss = 0.93042990
    Iteration 61, loss = 0.93086462
    Iteration 62, loss = 0.92880635
    Iteration 63, loss = 0.92824262
    Iteration 64, loss = 0.92666812
    Iteration 65, loss = 0.92487817
    Iteration 66, loss = 0.92587003
    Iteration 67, loss = 0.92325642
    Iteration 68, loss = 0.92335554
    Iteration 69, loss = 0.92300303
    Iteration 70, loss = 0.92080149
    Iteration 71, loss = 0.92214877
    Iteration 72, loss = 0.91826273
    Iteration 73, loss = 0.91804800
    Iteration 74, loss = 0.91791068
    Iteration 75, loss = 0.91775141
    Iteration 76, loss = 0.91673756
    Iteration 77, loss = 0.91693508
    Iteration 78, loss = 0.91584630
    Iteration 79, loss = 0.91631324
    Iteration 80, loss = 0.91389207
    Iteration 81, loss = 0.91461913
    Iteration 82, loss = 0.91381044
    Iteration 83, loss = 0.91348721
    Iteration 84, loss = 0.91354434
    Iteration 85, loss = 0.91234276
    Iteration 86, loss = 0.91198503
    Iteration 87, loss = 0.91228426
    Iteration 88, loss = 0.91221039
    Iteration 89, loss = 0.91192593
    Iteration 90, loss = 0.91155305
    Iteration 91, loss = 0.91115195
    Iteration 92, loss = 0.91143056
    Iteration 93, loss = 0.91186825
    Iteration 94, loss = 0.91125838
    Iteration 95, loss = 0.91110076
    Iteration 96, loss = 0.91113465
    Iteration 97, loss = 0.90940408
    Iteration 98, loss = 0.91015138
    Iteration 99, loss = 0.91101116
    Iteration 100, loss = 0.91055120
    Iteration 101, loss = 0.91023339
    Iteration 102, loss = 0.91046109
    Iteration 103, loss = 0.91023685
    Iteration 104, loss = 0.90993468
    Iteration 105, loss = 0.91113975
    Iteration 106, loss = 0.90971368
    Iteration 107, loss = 0.91026669
    Iteration 108, loss = 0.90989471
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=10, batch_size=auto, hidden_layer_sizes=(20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=False; total time=10.3min
    Iteration 44, loss = 0.94558108
    Iteration 45, loss = 0.94414210
    Iteration 46, loss = 0.94342960
    Iteration 47, loss = 0.94271115
    Iteration 48, loss = 0.94281238
    Iteration 49, loss = 0.94208799
    Iteration 50, loss = 0.94255537
    Iteration 51, loss = 0.94147257
    Iteration 52, loss = 0.94064880
    Iteration 53, loss = 0.94117626
    Iteration 54, loss = 0.93999480
    Iteration 55, loss = 0.93966000
    Iteration 56, loss = 0.93901041
    Iteration 57, loss = 0.93880618
    Iteration 58, loss = 0.93840795
    Iteration 59, loss = 0.93771382
    Iteration 60, loss = 0.93815665
    Iteration 61, loss = 0.93746840
    Iteration 62, loss = 0.93600344
    Iteration 63, loss = 0.93578555
    Iteration 64, loss = 0.93551200
    Iteration 65, loss = 0.93412268
    Iteration 66, loss = 0.93455331
    Iteration 67, loss = 0.93462790
    Iteration 68, loss = 0.93291034
    Iteration 69, loss = 0.93267396
    Iteration 70, loss = 0.93251089
    Iteration 71, loss = 0.93120060
    Iteration 72, loss = 0.93172322
    Iteration 73, loss = 0.93024313
    Iteration 74, loss = 0.93033915
    Iteration 75, loss = 0.92965793
    Iteration 76, loss = 0.92914885
    Iteration 77, loss = 0.92839241
    Iteration 78, loss = 0.92864897
    Iteration 79, loss = 0.92683206
    Iteration 80, loss = 0.92834835
    Iteration 81, loss = 0.92760417
    Iteration 82, loss = 0.92723679
    Iteration 83, loss = 0.92702269
    Iteration 84, loss = 0.92700487
    Iteration 85, loss = 0.92687541
    Iteration 86, loss = 0.92601855
    Iteration 87, loss = 0.92578595
    Iteration 88, loss = 0.92642296
    Iteration 89, loss = 0.92530700
    Iteration 90, loss = 0.92628654
    Iteration 91, loss = 0.92740486
    Iteration 92, loss = 0.92536209
    Iteration 93, loss = 0.92649476
    Iteration 1, loss = 1.91340850
    Iteration 2, loss = 1.39099474
    Iteration 3, loss = 1.18977036
    Iteration 4, loss = 1.10092350
    Iteration 5, loss = 1.05737588
    Iteration 6, loss = 1.03524747
    Iteration 7, loss = 1.02575341
    Iteration 8, loss = 1.01732060
    Iteration 9, loss = 1.00992305
    Iteration 10, loss = 1.00526944
    Iteration 11, loss = 1.00357995
    Iteration 12, loss = 0.99855776
    Iteration 13, loss = 0.99480511
    Iteration 14, loss = 0.99201995
    Iteration 15, loss = 0.98996878
    Iteration 16, loss = 0.98850410
    Iteration 17, loss = 0.98570828
    Iteration 18, loss = 0.98455277
    Iteration 19, loss = 0.98125058
    Iteration 20, loss = 0.97831372
    Iteration 21, loss = 0.97842754
    Iteration 22, loss = 0.97596667
    Iteration 23, loss = 0.97250287
    Iteration 24, loss = 0.96972727
    Iteration 25, loss = 0.96989510
    Iteration 26, loss = 0.96532752
    Iteration 27, loss = 0.96233537
    Iteration 28, loss = 0.96135381
    Iteration 29, loss = 0.95781173
    Iteration 30, loss = 0.95337961
    Iteration 31, loss = 0.95157860
    Iteration 32, loss = 0.94900074
    Iteration 33, loss = 0.94721988
    Iteration 34, loss = 0.94629083
    Iteration 35, loss = 0.94373585
    Iteration 36, loss = 0.94187292
    Iteration 37, loss = 0.94027676
    Iteration 38, loss = 0.93913676
    Iteration 39, loss = 0.93856153
    Iteration 40, loss = 0.93729746
    Iteration 41, loss = 0.93791854
    Iteration 42, loss = 0.93640859
    Iteration 43, loss = 0.93501615
    Iteration 44, loss = 0.93483524
    Iteration 45, loss = 0.93348878
    Iteration 46, loss = 0.93423750
    Iteration 47, loss = 0.93233784
    Iteration 48, loss = 0.93223485
    Iteration 49, loss = 0.93287080
    Iteration 50, loss = 0.93243035
    Iteration 51, loss = 0.93152389
    Iteration 52, loss = 0.93090407
    Iteration 53, loss = 0.93106633
    Iteration 54, loss = 0.93038236
    Iteration 55, loss = 0.93182103
    Iteration 56, loss = 0.92903813
    Iteration 57, loss = 0.92913497
    Iteration 58, loss = 0.92917149
    Iteration 59, loss = 0.92887116
    Iteration 60, loss = 0.92973891
    Iteration 61, loss = 0.92832557
    Iteration 62, loss = 0.92746784
    Iteration 63, loss = 0.92896009
    Iteration 64, loss = 0.92811881
    Iteration 65, loss = 0.92773905
    Iteration 66, loss = 0.92808670
    Iteration 67, loss = 0.92699559
    Iteration 68, loss = 0.92840988
    Iteration 69, loss = 0.92653402
    Iteration 70, loss = 0.92747790
    Iteration 71, loss = 0.92791468
    Iteration 72, loss = 0.92721967
    Iteration 73, loss = 0.92620026
    Iteration 74, loss = 0.92667523
    Iteration 75, loss = 0.92885031
    Iteration 76, loss = 0.92589438
    Iteration 77, loss = 0.92566583
    Iteration 78, loss = 0.92697535
    Iteration 79, loss = 0.92593567
    Iteration 80, loss = 0.92443529
    Iteration 81, loss = 0.92582990
    Iteration 82, loss = 0.92487063
    Iteration 83, loss = 0.92535399
    Iteration 84, loss = 0.92638916
    Iteration 85, loss = 0.92480187
    Iteration 86, loss = 0.92505861
    Iteration 87, loss = 0.92356064
    Iteration 88, loss = 0.92430262
    Iteration 89, loss = 0.92411324
    Iteration 90, loss = 0.92404959
    Iteration 91, loss = 0.92238741
    Iteration 92, loss = 0.92292338
    Iteration 93, loss = 0.92209771
    Iteration 94, loss = 0.92316353
    Iteration 95, loss = 0.92138539
    Iteration 96, loss = 0.92156879
    Iteration 97, loss = 0.92127572
    Iteration 98, loss = 0.92107044
    Iteration 99, loss = 0.92110520
    Iteration 100, loss = 0.91992308
    Iteration 101, loss = 0.91954939
    Iteration 102, loss = 0.91881331
    Iteration 103, loss = 0.91837344
    Iteration 104, loss = 0.91797372
    Iteration 105, loss = 0.91659998
    Iteration 106, loss = 0.91568734
    Iteration 107, loss = 0.91621372
    Iteration 108, loss = 0.91575450
    Iteration 109, loss = 0.91505836
    Iteration 110, loss = 0.91417891
    Iteration 111, loss = 0.91330887
    Iteration 112, loss = 0.91273072
    Iteration 113, loss = 0.91342638
    Iteration 114, loss = 0.91106790
    Iteration 115, loss = 0.91200199
    Iteration 116, loss = 0.91065837
    Iteration 117, loss = 0.91039235
    Iteration 118, loss = 0.91000686
    Iteration 119, loss = 0.90971164
    Iteration 120, loss = 0.90964566
    Iteration 121, loss = 0.91011114
    Iteration 122, loss = 0.90971702
    Iteration 123, loss = 0.91031051
    Iteration 124, loss = 0.90883721
    Iteration 125, loss = 0.90799300
    Iteration 126, loss = 0.90830008
    Iteration 127, loss = 0.90776522
    Iteration 128, loss = 0.90784625
    Iteration 129, loss = 0.90754602
    Iteration 130, loss = 0.90954054
    Iteration 131, loss = 0.90718217
    Iteration 132, loss = 0.90750234
    Iteration 133, loss = 0.90841593
    Iteration 134, loss = 0.90752934
    Iteration 135, loss = 0.90698558
    Iteration 136, loss = 0.90791155
    Iteration 137, loss = 0.90748554
    Iteration 138, loss = 0.90597350
    Iteration 139, loss = 0.90706353
    Iteration 140, loss = 0.90651742
    Iteration 141, loss = 0.90658267
    Iteration 142, loss = 0.90644003
    Iteration 143, loss = 0.90725374
    Iteration 144, loss = 0.90759051
    Iteration 145, loss = 0.90648298
    Iteration 146, loss = 0.90662178
    Iteration 147, loss = 0.90714182
    Iteration 148, loss = 0.90658937
    Iteration 149, loss = 0.90688559
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
    [CV] END activation=relu, alpha=10, batch_size=auto, hidden_layer_sizes=(20, 20, 20), max_iter=500, random_state=42, solver=adam, verbose=True, warm_start=False; total time=12.5min
    Iteration 94, loss = 0.92571478
    Iteration 95, loss = 0.92667988
    Iteration 96, loss = 0.92568517
    Iteration 97, loss = 0.92566179
    Iteration 98, loss = 0.92527286
    Iteration 99, loss = 0.92574659
    Iteration 100, loss = 0.92757585
    Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.



```python
print(gs.best_estimator_)
print('R2 training: ', gs.score(X_train, y_train))
print('R2 CV training: ', gs.best_score_)
print('R2 test: ', gs.score(X_test, y_test))
```

    MLPClassifier(alpha=10, hidden_layer_sizes=(20, 20, 20), max_iter=500,
                  random_state=42, verbose=True)
    R2 training:  0.7729329064587973
    R2 CV training:  0.7094585189309577
    R2 test:  0.7032707028531663


### 9.3 Classification of Genre Based on Lyrics
#### Summary

Is it possible to classify genres based on Spotify lyrics?

I ran a train of classification models to try and identify the 5 largest genres based on song lyrics: (i) Cantopop (27%), (ii) Classic Mandopop (20%), (iii) Taiwan Pop (13%), and (iv) Mandopop (11%). To handle class imbalance I applied inverse class weights which proved to be effective. I used JieBa to tokenize the lyrics and TfidfVectorizer. I experimented with using updated stop words but the impact was neglible. The best score was from the BernoulliNB with a test score of 62.4% vs. a base lin of 36%. This was also one of the quickest models to run. 

![image.png](attachment:image.png)


#### 9.3.1. Four Genres, TfidfVectorizer, Stop Words, Logistic Regression

Start off with top 5 genres:




```python
df_genres = df_tracks_master_clean3_lyrics_genre_hsk_v2.copy()
```


```python
df_genres.Genre2.value_counts(normalize=True)
```




    cantopop             0.274605
    classic mandopop     0.200139
    taiwan pop           0.133954
    mandopop             0.107493
    chinese indie        0.035930
                           ...   
    kayokyoku            0.000033
    anime rock           0.000033
    blues rock           0.000033
    taiwan electronic    0.000033
    xinyao               0.000033
    Name: Genre2, Length: 64, dtype: float64




```python
X = df_genres.lyrics[(df_genres.Genre2 == 'cantopop') | (df_genres.Genre2 == 'classic mandopop') | (df_genres.Genre2 == 'taiwan pop') | (df_genres.Genre2 == 'mandopop') | (df_genres.Genre2 == 'chinese indie')]
y = df_genres.Genre2[(df_genres.Genre2 == 'cantopop') | (df_genres.Genre2 == 'classic mandopop') | (df_genres.Genre2 == 'taiwan pop') | (df_genres.Genre2 == 'mandopop') | (df_genres.Genre2 == 'chinese indie')]
```


```python
X.shape, y.shape
```




    ((22796,), (22796,))




```python
X.head()
```




    0    英雄词曲人生不是一个人的游戏一起奋斗一起超越一起杀吧兄弟好战好胜战胜逆命管他天赋够不够我们都...
    1    双截棍词曲岩烧店的烟味弥漫店里面的妈妈桑教拳脚武术的老板练铁沙掌硬底子功夫最擅长还会金钟罩铁...
    2    开不了口词曲才离开没多久就开始担心今天的你过得好不好整个画面是你嘴嘟嘟那可爱的模样还有在你身...
    3    床边故事词曲从前从前有只猫头鹰它站在屋顶屋顶后面一遍森林森林很安静安静的钢琴在大厅阁楼里仔细...
    4    夜曲夜曲词曲窃爱词曲一群嗜血的蚂蚁被腐肉所吸引我面无表情看孤独的风景失去你失去你当鸽子不再象...
    Name: lyrics, dtype: object




```python
type(X)
```




    pandas.core.series.Series




```python
y.head()
```




    0    mandopop
    1    mandopop
    2    mandopop
    3    mandopop
    4    mandopop
    Name: Genre2, dtype: object




```python
y.value_counts()
```




    cantopop            8323
    classic mandopop    6066
    taiwan pop          4060
    mandopop            3258
    chinese indie       1089
    Name: Genre2, dtype: int64




```python

# Get Train test split from sample data
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    stratify=y,
                                                    test_size=0.2,
                                                    random_state=1)
```


```python
# Create a customer tokenizer using JieBa to account for Mandarin
def man_token_jie(x):
    return jieba.lcut(x)
```


```python
y_train.value_counts(normalize=True)
```




    cantopop            0.365102
    classic mandopop    0.266122
    taiwan pop          0.178109
    mandopop            0.142904
    chinese indie       0.047763
    Name: Genre2, dtype: float64




```python
w = {'cantopop': 1/0.36, 'classic mandopop': 1/0.27,'taiwan pop': 1/0.18, 'mandopop': 1/0.14,'chinese indie': 1/0.05  }
model = make_pipeline(TfidfVectorizer(stop_words=stopwords(["zh"]),
                                      # sublinear_tf=True,
                                      max_df=0.3,
                                      max_features=1000,
                                      tokenizer=man_token_jie,
                                      norm='l2'),
                      LogisticRegression(solver='lbfgs', multi_class='ovr', class_weight=w),
                      )



model.fit(X_train, y_train)
```




    Pipeline(steps=[('tfidfvectorizer',
                     TfidfVectorizer(max_df=0.3, max_features=1000,
                                     stop_words={'、', '。', '〈', '〉', '《', '》', '一',
                                                 '一个', '一些', '一何', '一切', '一则',
                                                 '一方面', '一旦', '一来', '一样', '一种',
                                                 '一般', '一转眼', '七', '万一', '三', '上',
                                                 '上下', '下', '不', '不仅', '不但', '不光',
                                                 '不单', ...},
                                     tokenizer=<function man_token_jie at 0x7fdb0b4cc940>)),
                    ('logisticregression',
                     LogisticRegression(class_weight={'cantopop': 2.7777777777777777,
                                                      'chinese indie': 20.0,
                                                      'classic mandopop': 3.7037037037037033,
                                                      'mandopop': 7.142857142857142,
                                                      'taiwan pop': 5.555555555555555},
                                        multi_class='ovr'))])




```python
Bold = '\033[1m'
Normal = '\033[0m'

skf = StratifiedKFold(n_splits=5)
skf.get_n_splits(X_train, y_train)
lr_scores = cross_val_score(model, X_train, y_train, cv=skf, n_jobs=6)

print(Bold + 'Base line score:' + Normal,
      np.round(y_train.value_counts(normalize=True)[0], 3))
print(Bold + 'Training Score:' + Normal, np.round(model.score(X_train, y_train),
                                                  3))
print(Bold + 'Test Score:' + Normal, np.round(model.score(X_test, y_test), 3))
print(Bold + "Cross-validated training scores:" + Normal,
      np.round(lr_scores, 4))
print(Bold + "Mean cross-validated training score:" + Normal,
      np.round(lr_scores.mean(), 3), '\n')
```

    Building prefix dict from the default dictionary ...
    Building prefix dict from the default dictionary ...
    Building prefix dict from the default dictionary ...
    Building prefix dict from the default dictionary ...
    Building prefix dict from the default dictionary ...
    Dumping model to file cache /var/folders/87/4nvcbwyx25s9fz86v2nq_xmw0000gq/T/jieba.cache
    Dumping model to file cache /var/folders/87/4nvcbwyx25s9fz86v2nq_xmw0000gq/T/jieba.cache
    Dumping model to file cache /var/folders/87/4nvcbwyx25s9fz86v2nq_xmw0000gq/T/jieba.cache
    Dumping model to file cache /var/folders/87/4nvcbwyx25s9fz86v2nq_xmw0000gq/T/jieba.cache
    Dumping model to file cache /var/folders/87/4nvcbwyx25s9fz86v2nq_xmw0000gq/T/jieba.cache
    Loading model cost 0.892 seconds.
    Prefix dict has been built successfully.
    Loading model cost 0.896 seconds.
    Prefix dict has been built successfully.
    Loading model cost 0.895 seconds.
    Prefix dict has been built successfully.
    Loading model cost 0.902 seconds.
    Prefix dict has been built successfully.
    Loading model cost 0.906 seconds.
    Prefix dict has been built successfully.


    [1mBase line score:[0m 0.365


    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['借傥', '兼', '前', '唷', '啪', '啷', '喔', '天', '始', '漫', '然', '特', '竟', '莫', '见', '设', '达'] not in stop_words.
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
    
    Increase the number of iterations (max_iter) or scale the data as shown in:
        https://scikit-learn.org/stable/modules/preprocessing.html
    Please also refer to the documentation for alternative solver options:
        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
      n_iter_i = _check_optimize_result(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['借傥', '兼', '前', '唷', '啪', '啷', '喔', '天', '始', '漫', '然', '特', '竟', '莫', '见', '设', '达'] not in stop_words.
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
    
    Increase the number of iterations (max_iter) or scale the data as shown in:
        https://scikit-learn.org/stable/modules/preprocessing.html
    Please also refer to the documentation for alternative solver options:
        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
      n_iter_i = _check_optimize_result(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
    
    Increase the number of iterations (max_iter) or scale the data as shown in:
        https://scikit-learn.org/stable/modules/preprocessing.html
    Please also refer to the documentation for alternative solver options:
        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
      n_iter_i = _check_optimize_result(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
    
    Increase the number of iterations (max_iter) or scale the data as shown in:
        https://scikit-learn.org/stable/modules/preprocessing.html
    Please also refer to the documentation for alternative solver options:
        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
      n_iter_i = _check_optimize_result(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
    
    Increase the number of iterations (max_iter) or scale the data as shown in:
        https://scikit-learn.org/stable/modules/preprocessing.html
    Please also refer to the documentation for alternative solver options:
        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
      n_iter_i = _check_optimize_result(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
    
    Increase the number of iterations (max_iter) or scale the data as shown in:
        https://scikit-learn.org/stable/modules/preprocessing.html
    Please also refer to the documentation for alternative solver options:
        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
      n_iter_i = _check_optimize_result(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['借傥', '兼', '前', '唷', '啪', '啷', '喔', '天', '始', '漫', '然', '特', '竟', '莫', '见', '设', '达'] not in stop_words.
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
    
    Increase the number of iterations (max_iter) or scale the data as shown in:
        https://scikit-learn.org/stable/modules/preprocessing.html
    Please also refer to the documentation for alternative solver options:
        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
      n_iter_i = _check_optimize_result(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['借傥', '兼', '前', '唷', '啪', '啷', '喔', '天', '始', '漫', '然', '特', '竟', '莫', '见', '设', '达'] not in stop_words.
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
    
    Increase the number of iterations (max_iter) or scale the data as shown in:
        https://scikit-learn.org/stable/modules/preprocessing.html
    Please also refer to the documentation for alternative solver options:
        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
      n_iter_i = _check_optimize_result(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
    
    Increase the number of iterations (max_iter) or scale the data as shown in:
        https://scikit-learn.org/stable/modules/preprocessing.html
    Please also refer to the documentation for alternative solver options:
        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
      n_iter_i = _check_optimize_result(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['借傥', '兼', '前', '唷', '啪', '啷', '喔', '天', '始', '漫', '然', '特', '竟', '莫', '见', '设', '达'] not in stop_words.
      warnings.warn(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
    
    Increase the number of iterations (max_iter) or scale the data as shown in:
        https://scikit-learn.org/stable/modules/preprocessing.html
    Please also refer to the documentation for alternative solver options:
        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
      n_iter_i = _check_optimize_result(
    /Users/stuart/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
    
    Increase the number of iterations (max_iter) or scale the data as shown in:
        https://scikit-learn.org/stable/modules/preprocessing.html
    Please also refer to the documentation for alternative solver options:
        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
      n_iter_i = _check_optimize_result(



    ---------------------------------------------------------------------------

    KeyboardInterrupt                         Traceback (most recent call last)

    /var/folders/87/4nvcbwyx25s9fz86v2nq_xmw0000gq/T/ipykernel_13777/2959730751.py in <module>
          8 print(Bold + 'Base line score:' + Normal,
          9       np.round(y_train.value_counts(normalize=True)[0], 3))
    ---> 10 print(Bold + 'Training Score:' + Normal, np.round(model.score(X_train, y_train),
         11                                                   3))
         12 print(Bold + 'Test Score:' + Normal, np.round(model.score(X_test, y_test), 3))


    ~/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/utils/metaestimators.py in <lambda>(*args, **kwargs)
        111 
        112             # lambda, but not partial, allows help() to work with update_wrapper
    --> 113             out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
        114         else:
        115 


    ~/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/pipeline.py in score(self, X, y, sample_weight)
        705         Xt = X
        706         for _, name, transform in self._iter(with_final=False):
    --> 707             Xt = transform.transform(Xt)
        708         score_params = {}
        709         if sample_weight is not None:


    ~/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py in transform(self, raw_documents)
       2099         check_is_fitted(self, msg="The TF-IDF vectorizer is not fitted")
       2100 
    -> 2101         X = super().transform(raw_documents)
       2102         return self._tfidf.transform(X, copy=False)
       2103 


    ~/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py in transform(self, raw_documents)
       1377 
       1378         # use the same matrix-building strategy as fit_transform
    -> 1379         _, X = self._count_vocab(raw_documents, fixed_vocab=True)
       1380         if self.binary:
       1381             X.data.fill(1)


    ~/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py in _count_vocab(self, raw_documents, fixed_vocab)
       1199         for doc in raw_documents:
       1200             feature_counter = {}
    -> 1201             for feature in analyze(doc):
       1202                 try:
       1203                     feature_idx = vocabulary[feature]


    ~/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py in _analyze(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)
        113             doc = preprocessor(doc)
        114         if tokenizer is not None:
    --> 115             doc = tokenizer(doc)
        116         if ngrams is not None:
        117             if stop_words is not None:


    /var/folders/87/4nvcbwyx25s9fz86v2nq_xmw0000gq/T/ipykernel_13777/4041829911.py in man_token_jie(x)
          1 # Create a customer tokenizer using JieBa to account for Mandarin
          2 def man_token_jie(x):
    ----> 3     return jieba.lcut(x)
    

    ~/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/jieba/__init__.py in lcut(self, *args, **kwargs)
        355 
        356     def lcut(self, *args, **kwargs):
    --> 357         return list(self.cut(*args, **kwargs))
        358 
        359     def lcut_for_search(self, *args, **kwargs):


    ~/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/jieba/__init__.py in cut(self, sentence, cut_all, HMM, use_paddle)
        323                 continue
        324             if re_han.match(blk):
    --> 325                 for word in cut_block(blk):
        326                     yield word
        327             else:


    ~/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/jieba/__init__.py in __cut_DAG(self, sentence)
        267                         if not self.FREQ.get(buf):
        268                             recognized = finalseg.cut(buf)
    --> 269                             for t in recognized:
        270                                 yield t
        271                         else:


    ~/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/jieba/finalseg/__init__.py in cut(sentence)
         88     for blk in blocks:
         89         if re_han.match(blk):
    ---> 90             for word in __cut(blk):
         91                 if word not in Force_Split_Words:
         92                     yield word


    ~/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/jieba/finalseg/__init__.py in __cut(sentence)
         59 def __cut(sentence):
         60     global emit_P
    ---> 61     prob, pos_list = viterbi(sentence, 'BMES', start_P, trans_P, emit_P)
         62     begin, nexti = 0, 0
         63     # print pos_list, sentence


    ~/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/jieba/finalseg/__init__.py in viterbi(obs, states, start_p, trans_p, emit_p)
         47             em_p = emit_p[y].get(obs[t], MIN_FLOAT)
         48             (prob, state) = max(
    ---> 49                 [(V[t - 1][y0] + trans_p[y0].get(y, MIN_FLOAT) + em_p, y0) for y0 in PrevStatus[y]])
         50             V[t][y] = prob
         51             newpath[y] = path[state] + [y]


    ~/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/jieba/finalseg/__init__.py in <listcomp>(.0)
         47             em_p = emit_p[y].get(obs[t], MIN_FLOAT)
         48             (prob, state) = max(
    ---> 49                 [(V[t - 1][y0] + trans_p[y0].get(y, MIN_FLOAT) + em_p, y0) for y0 in PrevStatus[y]])
         50             V[t][y] = prob
         51             newpath[y] = path[state] + [y]


    KeyboardInterrupt: 



```python
logist_plots(model,
             X_train=X_train,
             y_train=y_train,
             X_test=X_test,
             y_test=y_test)
```

    y_train classification report:
                      precision    recall  f1-score   support
    
            cantopop       0.86      0.69      0.77      6658
       chinese indie       0.22      0.60      0.32       871
    classic mandopop       0.65      0.55      0.59      4853
            mandopop       0.43      0.51      0.47      2606
          taiwan pop       0.51      0.52      0.51      3248
    
            accuracy                           0.59     18236
           macro avg       0.53      0.57      0.53     18236
        weighted avg       0.65      0.59      0.61     18236
    
    y_test classification report:
                      precision    recall  f1-score   support
    
            cantopop       0.80      0.65      0.72      1665
       chinese indie       0.12      0.31      0.18       218
    classic mandopop       0.55      0.49      0.51      1213
            mandopop       0.31      0.38      0.34       652
          taiwan pop       0.41      0.39      0.40       812
    
            accuracy                           0.50      4560
           macro avg       0.44      0.44      0.43      4560
        weighted avg       0.56      0.50      0.53      4560
    



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_292_1.png)
    


    ROC Curves:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_292_3.png)
    


    Precision Recall Curve:



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_292_5.png)
    


#### 9.3.2. Four Genres, TfidfVectorizer, Stop Words, All Models


```python
# Import Jieba to tokenize lyrics
import jieba.posseg as pseg
import jieba
```


```python
# Create a customer tokenizer using JieBa to account for Mandarin
def man_token_jie(x):
    return jieba.lcut(x)
```


```python
lyrics = df_tracks_master_clean3_lyrics_genre_hsk_v2.lyrics

# Tokenize lyrics
cvec = CountVectorizer(tokenizer=man_token_jie)
cvec.fit(lyrics)
cvec.get_feature_names()[:5]
```

    Building prefix dict from the default dictionary ...
    Dumping model to file cache /var/folders/87/4nvcbwyx25s9fz86v2nq_xmw0000gq/T/jieba.cache
    Loading model cost 0.732 seconds.
    Prefix dict has been built successfully.





    ['一', '一一', '一一二', '一一记', '一丁']




```python
from time import time
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import RidgeClassifier
from sklearn.pipeline import Pipeline
from sklearn.svm import LinearSVC
from sklearn.linear_model import SGDClassifier
from sklearn.linear_model import Perceptron
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.naive_bayes import BernoulliNB, MultinomialNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors import NearestCentroid
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils.extmath import density
from sklearn.feature_selection import SelectFromModel
from sklearn import metrics

X = df_genres.lyrics[(df_genres.Genre2 == 'cantopop') | (df_genres.Genre2 == 'classic mandopop') | (df_genres.Genre2 == 'taiwan pop') | (df_genres.Genre2 == 'mandopop') | (df_genres.Genre2 == 'chinese indie')]
y = df_genres.Genre2[(df_genres.Genre2 == 'cantopop') | (df_genres.Genre2 == 'classic mandopop') | (df_genres.Genre2 == 'taiwan pop') | (df_genres.Genre2 == 'mandopop') | (df_genres.Genre2 == 'chinese indie')]

# Get Train test split from sample data
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    stratify=y,
                                                    test_size=0.2,
                                                    random_state=1)

w = {'cantopop': np.round(1/0.36,2), 'classic mandopop': np.round(1/0.27),'taiwan pop': np.round(1/0.18), 'mandopop': np.round(1/0.14),'chinese indie': np.round(1/0.05)  }

tvec = TfidfVectorizer(stop_words=stopwords(["zh"]),
                                      max_df=0.3,
                                      max_features=50000,
                                      tokenizer=man_token_jie,
                                      norm='l2')




tvec.fit(X_train)
X_train = tvec.transform(X_train)
X_test = tvec.transform(X_test)


feature_names = np.array(tvec.get_feature_names())


def trim(s):
    """Trim string to fit on terminal (assuming 80-column display)"""
    return s if len(s) <= 80 else s[:77] + "..."

###############################################################################
# Benchmark classifiers


def benchmark(clf):
    print(('_' * 80))
    print("Training: ")
    print(clf)
    t0 = time()
    clf.fit(X_train, y_train)
    train_time = time() - t0
    print(("train time: %0.3fs" % train_time))

    t0 = time()
    pred = clf.predict(X_test)
    test_time = time() - t0
    print(("test time:  %0.3fs" % test_time))

    score = metrics.accuracy_score(y_test, pred)
    print(("accuracy:   %0.3f" % score))

    if hasattr(clf, 'coef_'):
        print(("dimensionality: %d" % clf.coef_.shape[1]))
        print(("density: %f" % density(clf.coef_)))

        if feature_names is not None:
            print("top 10 keywords per class:")
            for i, category in enumerate(set(y_train.values)):
                top10 = np.argsort(clf.coef_[i])[-10:]
                print((trim("%s: %s"
                            % (category, " ".join(feature_names[top10])))))
        print()

    print("classification report:")
    print((metrics.classification_report(y_test, pred,
                                         target_names=set(y_train.values))))

    print("confusion matrix:")
    print((metrics.confusion_matrix(y_test, pred)))

    print()
    clf_descr = str(clf).split('(')[0]
    return clf_descr, score, train_time, test_time


results = []
for clf, name in (
#         (RidgeClassifier(tol=1e-2, solver="sag", class_weight=w), "Ridge Classifier"),
        (Perceptron(max_iter=1000, tol=1e-3, class_weight=w, n_jobs=-1), "Perceptron"),
        (PassiveAggressiveClassifier(max_iter=1000, tol=1e-3, class_weight=w,n_jobs=-1), "Passive-Aggressive"),
        (KNeighborsClassifier(n_neighbors=10, n_jobs=-1), "kNN"),
        (RandomForestClassifier(n_estimators=100, class_weight=w, n_jobs=-1), "Random forest")):
    print(('=' * 80))
    print(name)
    results.append(benchmark(clf))

for penalty in ["l2", "l1"]:
    print(('=' * 80))
    print(("%s penalty" % penalty.upper()))
    # Train Liblinear model
    results.append(benchmark(LinearSVC(loss='squared_hinge', penalty=penalty,
                                            dual=False)))

    # Train SGD model
    results.append(benchmark(SGDClassifier(alpha=.0001,
                                           penalty=penalty,
                                           max_iter=1000,
                                           tol=1e-3)))

# Train SGD with Elastic Net penalty
print(('=' * 80))
print("Elastic-Net penalty")
results.append(benchmark(SGDClassifier(alpha=.0001,
                                       penalty="elasticnet",
                                       max_iter=1000,
                                       tol=1e-3)))

# Train NearestCentroid without threshold
print(('=' * 80))
print("NearestCentroid (aka Rocchio classifier)")
results.append(benchmark(NearestCentroid()))

# Train sparse Naive Bayes classifiers
print(('=' * 80))
print("Naive Bayes")
results.append(benchmark(MultinomialNB(alpha=.01)))
results.append(benchmark(BernoulliNB(alpha=.01)))

print(('=' * 80))
print("LinearSVC with L1-based feature selection")
# The smaller C, the stronger the regularization.
# The more regularization, the more sparsity.
results.append(benchmark(Pipeline([
    ('feature_selection', SelectFromModel(LinearSVC(penalty="l1", dual=False))),
    ('classification', LinearSVC(penalty="l2"))])))
# make some plots

indices = np.arange(len(results))

results = [[x[i] for x in results] for i in range(4)]

clf_names, score, training_time, test_time = results
training_time = np.array(training_time) / np.max(training_time)
test_time = np.array(test_time) / np.max(test_time)

plt.figure(figsize=(12, 8))
plt.title("Score")
plt.barh(indices, score, .2, label="score", color='r')
plt.barh(indices + .3, training_time, .2, label="training time", color='g')
plt.barh(indices + .6, test_time, .2, label="test time", color='b')
plt.yticks((), fontsize=14)
plt.xticks(fontsize=14)
plt.legend(loc='best')
plt.subplots_adjust(left=.25)
plt.subplots_adjust(top=.95)
plt.subplots_adjust(bottom=.05)

for i, c in zip(indices, clf_names):
    plt.text(-.3, i, c, fontsize=14)

plt.show()
```

    ================================================================================
    Perceptron
    ________________________________________________________________________________
    Training: 
    Perceptron(class_weight={'cantopop': 2.78, 'chinese indie': 20.0,
                             'classic mandopop': 4.0, 'mandopop': 7.0,
                             'taiwan pop': 6.0},
               n_jobs=-1)
    train time: 0.122s
    test time:  0.003s
    accuracy:   0.554
    dimensionality: 50000
    density: 0.815148
    top 10 keywords per class:
    cantopop: 一片空白 风花雪月 终止 借着 谭 脑海中 一双眼 雨里 令 监制
    taiwan pop: 承载 沉默不语 合音 真理 随心所欲 难以形容 出走 飘来飘去 曰 工程
    classic mandopop: 向北飞 心田 山盟海誓 流成 成河 瓢泼 变黑 统筹 落寞 不由自主
    mandopop: 装乖 受伤害 最大 早点 一号 不管怎样 男当 日暮 之上 共看
    chinese indie: 心若倦 纠结 寓言 放晴 所迷 前方 悬着 胡闹 一步步 大提琴
    
    classification report:
                      precision    recall  f1-score   support
    
            cantopop       0.74      0.72      0.73      1665
          taiwan pop       0.27      0.27      0.27       218
    classic mandopop       0.56      0.54      0.55      1213
            mandopop       0.34      0.40      0.37       652
       chinese indie       0.45      0.43      0.44       812
    
            accuracy                           0.55      4560
           macro avg       0.47      0.47      0.47      4560
        weighted avg       0.56      0.55      0.56      4560
    
    confusion matrix:
    [[1204   50  200  115   96]
     [  66   58   42   34   18]
     [ 182   45  651  178  157]
     [  88   35  111  261  157]
     [  86   28  161  186  351]]
    
    ================================================================================
    Passive-Aggressive
    ________________________________________________________________________________
    Training: 
    PassiveAggressiveClassifier(class_weight={'cantopop': 2.78,
                                              'chinese indie': 20.0,
                                              'classic mandopop': 4.0,
                                              'mandopop': 7.0, 'taiwan pop': 6.0},
                                n_jobs=-1)
    train time: 0.145s
    test time:  0.003s
    accuracy:   0.562
    dimensionality: 50000
    density: 0.909168
    top 10 keywords per class:
    cantopop: 一双眼 哪需 说法 脑海中 下落不明 谭 令 借着 雨里 监制
    taiwan pop: 蓝 沉默不语 两端 宴席 属相 大米 工程 出走 两天 曰
    classic mandopop: 瞑 产生 耶耶爱 心田 山盟海誓 落寞 词 泪珠 不由自主 统筹
    mandopop: 录音棚 早点 感伤 共看 牢牢 最大 痛痛 猜管 不管怎样 日暮
    chinese indie: 悬着 主 都还没 穿越 投降 走火入魔 寓言 放晴 得来不易 下定决心
    
    classification report:
                      precision    recall  f1-score   support
    
            cantopop       0.71      0.74      0.73      1665
          taiwan pop       0.33      0.25      0.29       218
    classic mandopop       0.55      0.57      0.56      1213
            mandopop       0.37      0.35      0.36       652
       chinese indie       0.45      0.43      0.44       812
    
            accuracy                           0.56      4560
           macro avg       0.48      0.47      0.47      4560
        weighted avg       0.56      0.56      0.56      4560
    
    confusion matrix:
    [[1235   39  212   86   93]
     [  72   55   48   26   17]
     [ 209   27  696  129  152]
     [ 111   23  127  226  165]
     [ 110   23  179  147  353]]
    
    ================================================================================
    kNN
    ________________________________________________________________________________
    Training: 
    KNeighborsClassifier(n_jobs=-1, n_neighbors=10)
    train time: 0.023s
    test time:  2.446s
    accuracy:   0.432
    classification report:
                      precision    recall  f1-score   support
    
            cantopop       0.61      0.55      0.58      1665
          taiwan pop       0.36      0.06      0.10       218
    classic mandopop       0.41      0.52      0.46      1213
            mandopop       0.26      0.14      0.18       652
       chinese indie       0.28      0.37      0.32       812
    
            accuracy                           0.43      4560
           macro avg       0.38      0.33      0.33      4560
        weighted avg       0.43      0.43      0.42      4560
    
    confusion matrix:
    [[924   5 386  75 275]
     [ 73  13  82  12  38]
     [237   5 636  87 248]
     [123   8 207  93 221]
     [161   5 246  97 303]]
    
    ================================================================================
    Random forest
    ________________________________________________________________________________
    Training: 
    RandomForestClassifier(class_weight={'cantopop': 2.78, 'chinese indie': 20.0,
                                         'classic mandopop': 4.0, 'mandopop': 7.0,
                                         'taiwan pop': 6.0},
                           n_jobs=-1)
    train time: 4.440s
    test time:  0.059s
    accuracy:   0.582
    classification report:
                      precision    recall  f1-score   support
    
            cantopop       0.72      0.78      0.75      1665
          taiwan pop       0.85      0.13      0.22       218
    classic mandopop       0.48      0.74      0.58      1213
            mandopop       0.64      0.14      0.23       652
       chinese indie       0.48      0.42      0.45       812
    
            accuracy                           0.58      4560
           macro avg       0.63      0.44      0.45      4560
        weighted avg       0.61      0.58      0.55      4560
    
    confusion matrix:
    [[1298    3  283   11   70]
     [  83   28   83    8   16]
     [ 189    1  895   11  117]
     [ 122    1  273   94  162]
     [ 121    0  327   24  340]]
    
    ================================================================================
    L2 penalty
    ________________________________________________________________________________
    Training: 
    LinearSVC(dual=False)
    train time: 1.164s
    test time:  0.002s
    accuracy:   0.604
    dimensionality: 50000
    density: 1.000000
    top 10 keywords per class:
    cantopop: 下落不明 只得 谭 极 当天 便 几多 似 令 监制
    taiwan pop: 真理 公园 飘来飘去 公司 沉默不语 出走 两天 合音 工程 曰
    classic mandopop: 表示 涙 落寞 山盟海誓 不由自主 寒冷 恰恰 目屎 心田 统筹
    mandopop: 一号 无关 写下 确定 地平线 日暮 感伤 制作 录音 解
    chinese indie: 寓言 置 前方 放晴 投降 得来不易 师 穿越 一步步 编写
    
    classification report:
                      precision    recall  f1-score   support
    
            cantopop       0.75      0.78      0.77      1665
          taiwan pop       0.59      0.24      0.34       218
    classic mandopop       0.56      0.65      0.60      1213
            mandopop       0.43      0.34      0.38       652
       chinese indie       0.47      0.48      0.47       812
    
            accuracy                           0.60      4560
           macro avg       0.56      0.50      0.51      4560
        weighted avg       0.60      0.60      0.60      4560
    
    confusion matrix:
    [[1299   11  206   63   86]
     [  65   52   61   21   19]
     [ 183    5  791   90  144]
     [  90   12  145  224  181]
     [  88    8  202  128  386]]
    
    ________________________________________________________________________________
    Training: 
    SGDClassifier()
    train time: 0.244s
    test time:  0.002s
    accuracy:   0.596
    dimensionality: 50000
    density: 0.798848
    top 10 keywords per class:
    cantopop: 不可 仍然 极 没法 几多 未 监制 便 令 似
    taiwan pop: 出走 轻盈 公司 悄然 两天 一吹 飘来飘去 合音 工程 曰
    classic mandopop: 我俩 分离 昨夜 伊 相思 今夜 心头 涙 阮 真情
    mandopop: 七天 解 地平线 谱曲 人声 写下 卢广仲 边界 录音 编辑
    chinese indie: 玛丽 星球 室 告白 穿越 已读 录制 师 编曲 编写
    
    classification report:
                      precision    recall  f1-score   support
    
            cantopop       0.71      0.80      0.76      1665
          taiwan pop       0.55      0.12      0.20       218
    classic mandopop       0.53      0.71      0.61      1213
            mandopop       0.47      0.23      0.31       652
       chinese indie       0.49      0.42      0.45       812
    
            accuracy                           0.60      4560
           macro avg       0.55      0.46      0.47      4560
        weighted avg       0.58      0.60      0.57      4560
    
    confusion matrix:
    [[1340    2  227   27   69]
     [  84   27   77   13   17]
     [ 198    2  862   42  109]
     [ 120   13  207  148  164]
     [ 136    5  246   83  342]]
    
    ================================================================================
    L1 penalty
    ________________________________________________________________________________
    Training: 
    LinearSVC(dual=False, penalty='l1')
    train time: 3.177s
    test time:  0.002s
    accuracy:   0.585
    dimensionality: 50000
    density: 0.072104
    top 10 keywords per class:
    cantopop: 做证 剪辑 欠缺 这生 谭 似 红日 令 编监 监制
    taiwan pop: 亮起 子夜 碰巧 看近 工程 种籽 合音 娉婷 曰 互比
    classic mandopop: 通话时间 执行 风雨无阻 情丝 一会儿 鞭 张震岳 恰恰 统筹 目屎
    mandopop: 不如说 咿耶咿耶 解救 童话世界 变形 不愿醒 两分钟 卢广仲 无限期 透进
    chinese indie: 精准 师 当梦 毋知 编写 阁 严爵 落笔 才刚 心若倦
    
    classification report:
                      precision    recall  f1-score   support
    
            cantopop       0.74      0.77      0.76      1665
          taiwan pop       0.46      0.13      0.21       218
    classic mandopop       0.54      0.65      0.59      1213
            mandopop       0.40      0.31      0.35       652
       chinese indie       0.45      0.44      0.45       812
    
            accuracy                           0.59      4560
           macro avg       0.52      0.46      0.47      4560
        weighted avg       0.57      0.59      0.57      4560
    
    confusion matrix:
    [[1287    7  226   59   86]
     [  69   29   66   30   24]
     [ 190    8  790   87  138]
     [  97   10  156  203  186]
     [  99    9  213  131  360]]
    
    ________________________________________________________________________________
    Training: 
    SGDClassifier(penalty='l1')
    train time: 0.262s
    test time:  0.002s
    accuracy:   0.527
    dimensionality: 50000
    density: 0.007596
    top 10 keywords per class:
    cantopop: 当天 只得 未 极 没法 几多 便 监制 令 似
    taiwan pop: 岛愿 岛有 岩 岩浆 岗上 工程 阳光 两天 归去来兮 曰
    classic mandopop: 伊 小河 山盟海誓 相逢 相思 昨夜 心头 涙 阮 真情
    mandopop: 整个 双截棍 亲爱 勇敢 杜杜杜 拆掉 一周年 三十而立 痛痛 不管怎样
    chinese indie: 评评理 那安 师 录制 迎战 与狼共舞 回不来 阿嬷 天诛地灭 编写
    
    classification report:
                      precision    recall  f1-score   support
    
            cantopop       0.62      0.81      0.71      1665
          taiwan pop       0.50      0.03      0.06       218
    classic mandopop       0.47      0.67      0.55      1213
            mandopop       0.30      0.15      0.20       652
       chinese indie       0.44      0.17      0.24       812
    
            accuracy                           0.53      4560
           macro avg       0.47      0.37      0.35      4560
        weighted avg       0.50      0.53      0.48      4560
    
    confusion matrix:
    [[1354    1  244   37   29]
     [  96    7   92   17    6]
     [ 295    3  807   58   50]
     [ 198    3  261  101   89]
     [ 232    0  316  128  136]]
    
    ================================================================================
    Elastic-Net penalty
    ________________________________________________________________________________
    Training: 
    SGDClassifier(penalty='elasticnet')
    train time: 0.592s
    test time:  0.002s
    accuracy:   0.574
    dimensionality: 50000
    density: 0.151468
    top 10 keywords per class:
    cantopop: 当天 仍然 极 没法 几多 未 监制 便 令 似
    taiwan pop: 追得 雨下个 看近 监唱 脱脂 合音 工程 一吹 飘来飘去 曰
    classic mandopop: 轻轻地 相逢 昨夜 涙 伊 相思 心头 今夜 真情 阮
    mandopop: 高尚情操 这么一来 没差 双截棍 录音 盖世英雄 边界 编辑 谱曲 卢广仲
    chinese indie: 简讯 回不来 编曲 玛丽 告白 室 已读 录制 师 编写
    
    classification report:
                      precision    recall  f1-score   support
    
            cantopop       0.67      0.81      0.73      1665
          taiwan pop       0.41      0.04      0.08       218
    classic mandopop       0.52      0.71      0.60      1213
            mandopop       0.45      0.15      0.22       652
       chinese indie       0.47      0.37      0.41       812
    
            accuracy                           0.57      4560
           macro avg       0.50      0.42      0.41      4560
        weighted avg       0.55      0.57      0.54      4560
    
    confusion matrix:
    [[1353    3  226   20   63]
     [  96    9   85   11   17]
     [ 236    1  863   33   80]
     [ 160    5  218   96  173]
     [ 180    4  276   54  298]]
    
    ================================================================================
    NearestCentroid (aka Rocchio classifier)
    ________________________________________________________________________________
    Training: 
    NearestCentroid()
    train time: 0.024s
    test time:  0.003s
    accuracy:   0.564
    classification report:
                      precision    recall  f1-score   support
    
            cantopop       0.78      0.73      0.75      1665
          taiwan pop       0.21      0.23      0.22       218
    classic mandopop       0.57      0.57      0.57      1213
            mandopop       0.35      0.40      0.38       652
       chinese indie       0.44      0.43      0.44       812
    
            accuracy                           0.56      4560
           macro avg       0.47      0.47      0.47      4560
        weighted avg       0.58      0.56      0.57      4560
    
    confusion matrix:
    [[1212   45  195  107  106]
     [  60   51   56   29   22]
     [ 154   76  696  142  145]
     [  59   36  116  264  177]
     [  61   40  149  211  351]]
    
    ================================================================================
    Naive Bayes
    ________________________________________________________________________________
    Training: 
    MultinomialNB(alpha=0.01)
    train time: 0.059s
    test time:  0.002s
    accuracy:   0.623
    dimensionality: 50000
    density: 1.000000
    top 10 keywords per class:
    cantopop: 走 太 未 今天 一生 没 里 心 没有 想
    taiwan pop: 不会 心 不要 不能 一天 走 里 世界 想 没有
    classic mandopop: 寂寞 永远 梦 走 知道 不要 爱情 心 没有 想
    mandopop: 心 不会 太 走 知道 爱情 不要 世界 没有 想
    chinese indie: 心 幸福 一起 走 不要 没 爱情 世界 没有 想
    
    classification report:
                      precision    recall  f1-score   support
    
            cantopop       0.81      0.79      0.80      1665
          taiwan pop       0.77      0.20      0.31       218
    classic mandopop       0.57      0.70      0.63      1213
            mandopop       0.43      0.31      0.36       652
       chinese indie       0.47      0.54      0.50       812
    
            accuracy                           0.62      4560
           macro avg       0.61      0.51      0.52      4560
        weighted avg       0.63      0.62      0.61      4560
    
    confusion matrix:
    [[1310    5  197   60   93]
     [  62   43   73   17   23]
     [ 131    2  848   80  152]
     [  54    4  164  203  227]
     [  65    2  199  111  435]]
    
    ________________________________________________________________________________
    Training: 
    BernoulliNB(alpha=0.01)
    train time: 0.066s
    test time:  0.007s
    accuracy:   0.623
    dimensionality: 50000
    density: 1.000000
    top 10 keywords per class:
    cantopop: 未 走 世界 太 里 心 没 编曲 没有 想
    taiwan pop: 一天 知道 不会 心 编曲 走 里 世界 想 没有
    classic mandopop: 世界 梦 寂寞 知道 走 爱情 编曲 心 没有 想
    mandopop: 知道 心 爱情 没 里 走 世界 没有 编曲 想
    chinese indie: 太 不会 爱情 心 走 没 世界 没有 想 编曲
    
    classification report:
                      precision    recall  f1-score   support
    
            cantopop       0.87      0.75      0.81      1665
          taiwan pop       0.67      0.24      0.35       218
    classic mandopop       0.57      0.71      0.63      1213
            mandopop       0.40      0.37      0.39       652
       chinese indie       0.48      0.53      0.50       812
    
            accuracy                           0.62      4560
           macro avg       0.60      0.52      0.54      4560
        weighted avg       0.64      0.62      0.62      4560
    
    confusion matrix:
    [[1255    9  225   83   93]
     [  57   52   69   17   23]
     [  83    5  863  114  148]
     [  24    6  172  244  206]
     [  24    6  196  157  429]]
    
    ================================================================================
    LinearSVC with L1-based feature selection
    ________________________________________________________________________________
    Training: 
    Pipeline(steps=[('feature_selection',
                     SelectFromModel(estimator=LinearSVC(dual=False,
                                                         penalty='l1'))),
                    ('classification', LinearSVC())])
    train time: 3.610s
    test time:  0.006s
    accuracy:   0.587
    classification report:
                      precision    recall  f1-score   support
    
            cantopop       0.74      0.77      0.76      1665
          taiwan pop       0.57      0.18      0.27       218
    classic mandopop       0.54      0.64      0.59      1213
            mandopop       0.40      0.32      0.36       652
       chinese indie       0.46      0.44      0.45       812
    
            accuracy                           0.59      4560
           macro avg       0.54      0.47      0.48      4560
        weighted avg       0.58      0.59      0.58      4560
    
    confusion matrix:
    [[1288    7  227   60   83]
     [  67   39   62   26   24]
     [ 195    7  780   92  139]
     [  88   12  156  211  185]
     [ 100    4  208  140  360]]
    



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_297_1.png)
    


#### 9.3.3. Four Genres, TfidfVectorizer, Stop Words Updated, All Models


```python
# Import Jieba to tokenize lyrics
import jieba.posseg as pseg
import jieba
```


```python
# Create a customer tokenizer using JieBa to account for Mandarin
def man_token_jie(x):
    return jieba.lcut(x)
```


```python
lyrics = df_tracks_master_clean3_lyrics_genre_hsk_v2.lyrics

# Tokenize lyrics
cvec = CountVectorizer(tokenizer=man_token_jie)
cvec.fit(lyrics)
cvec.get_feature_names()[:5]
```




    ['一', '一一', '一一二', '一一记', '一丁']




```python
from time import time
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import RidgeClassifier
from sklearn.pipeline import Pipeline
from sklearn.svm import LinearSVC
from sklearn.linear_model import SGDClassifier
from sklearn.linear_model import Perceptron
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.naive_bayes import BernoulliNB, MultinomialNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors import NearestCentroid
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils.extmath import density
from sklearn.feature_selection import SelectFromModel
from sklearn import metrics

X = df_genres.lyrics[(df_genres.Genre2 == 'cantopop') | (df_genres.Genre2 == 'classic mandopop') | (df_genres.Genre2 == 'taiwan pop') | (df_genres.Genre2 == 'mandopop') | (df_genres.Genre2 == 'chinese indie')]
y = df_genres.Genre2[(df_genres.Genre2 == 'cantopop') | (df_genres.Genre2 == 'classic mandopop') | (df_genres.Genre2 == 'taiwan pop') | (df_genres.Genre2 == 'mandopop') | (df_genres.Genre2 == 'chinese indie')]

# Get Train test split from sample data
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    stratify=y,
                                                    test_size=0.2,
                                                    random_state=1)

w = {'cantopop': np.round(1/0.36,2), 'classic mandopop': np.round(1/0.27),'taiwan pop': np.round(1/0.18), 'mandopop': np.round(1/0.14),'chinese indie': np.round(1/0.05)  }

tvec = TfidfVectorizer(stop_words=updated_stopwords,
max_df=0.3,
max_features=50000,
tokenizer=man_token_jie, norm='l2')





tvec.fit(X_train)
X_train = tvec.transform(X_train)
X_test = tvec.transform(X_test)


feature_names = np.array(tvec.get_feature_names())


def trim(s):
    """Trim string to fit on terminal (assuming 80-column display)"""
    return s if len(s) <= 80 else s[:77] + "..."

###############################################################################
# Benchmark classifiers


def benchmark(clf):
    print(('_' * 80))
    print("Training: ")
    print(clf)
    t0 = time()
    clf.fit(X_train, y_train)
    train_time = time() - t0
    print(("train time: %0.3fs" % train_time))

    t0 = time()
    pred = clf.predict(X_test)
    test_time = time() - t0
    print(("test time:  %0.3fs" % test_time))

    score = metrics.accuracy_score(y_test, pred)
    print(("accuracy:   %0.3f" % score))

    if hasattr(clf, 'coef_'):
        print(("dimensionality: %d" % clf.coef_.shape[1]))
        print(("density: %f" % density(clf.coef_)))

        if feature_names is not None:
            print("top 10 keywords per class:")
            for i, category in enumerate(set(y_train.values)):
                top10 = np.argsort(clf.coef_[i])[-10:]
                print((trim("%s: %s"
                            % (category, " ".join(feature_names[top10])))))
        print()

    print("classification report:")
    print((metrics.classification_report(y_test, pred,
                                         target_names=set(y_train.values))))

    print("confusion matrix:")
    print((metrics.confusion_matrix(y_test, pred)))

    print()
    clf_descr = str(clf).split('(')[0]
    return clf_descr, score, train_time, test_time


results = []
for clf, name in ((Perceptron(max_iter=1000, tol=1e-3, class_weight=w, n_jobs=-1), "Perceptron"),(PassiveAggressiveClassifier(max_iter=1000, tol=1e-3, class_weight=w,n_jobs=-1), "Passive-Aggressive"),(KNeighborsClassifier(n_neighbors=10, n_jobs=-1), "kNN"),(RandomForestClassifier(n_estimators=100, class_weight=w, n_jobs=-1), "Random forest")):
    print(('=' * 80))
    print(name)
    results.append(benchmark(clf))

for penalty in ["l2", "l1"]:
    print(('=' * 80))
    print(("%s penalty" % penalty.upper()))
    # Train Liblinear model
    results.append(benchmark(LinearSVC(loss='squared_hinge', penalty=penalty,
                                            dual=False)))

    # Train SGD model
    results.append(benchmark(SGDClassifier(alpha=.0001,
                                           penalty=penalty,
                                           max_iter=1000,
                                           tol=1e-3)))

# Train SGD with Elastic Net penalty
print(('=' * 80))
print("Elastic-Net penalty")
results.append(benchmark(SGDClassifier(alpha=.0001,
                                       penalty="elasticnet",
                                       max_iter=1000,
                                       tol=1e-3)))

# Train NearestCentroid without threshold
print(('=' * 80))
print("NearestCentroid (aka Rocchio classifier)")
results.append(benchmark(NearestCentroid()))

# Train sparse Naive Bayes classifiers
print(('=' * 80))
print("Naive Bayes")
results.append(benchmark(MultinomialNB(alpha=.01)))
results.append(benchmark(BernoulliNB(alpha=.01)))

print(('=' * 80))
print("LinearSVC with L1-based feature selection")
# The smaller C, the stronger the regularization.
# The more regularization, the more sparsity.
results.append(benchmark(Pipeline([
    ('feature_selection', SelectFromModel(LinearSVC(penalty="l1", dual=False))),
    ('classification', LinearSVC(penalty="l2"))])))
# make some plots

indices = np.arange(len(results))

results = [[x[i] for x in results] for i in range(4)]

clf_names, score, training_time, test_time = results
training_time = np.array(training_time) / np.max(training_time)
test_time = np.array(test_time) / np.max(test_time)

plt.figure(figsize=(12, 8))
plt.title("Score")
plt.barh(indices, score, .2, label="score", color='r')
plt.barh(indices + .3, training_time, .2, label="training time", color='g')
plt.barh(indices + .6, test_time, .2, label="test time", color='b')
plt.yticks((), fontsize=14)
plt.xticks(fontsize=14)
plt.legend(loc='best')
plt.subplots_adjust(left=.25)
plt.subplots_adjust(top=.95)
plt.subplots_adjust(bottom=.05)

for i, c in zip(indices, clf_names):
    plt.text(-.3, i, c, fontsize=14)

plt.show()
```

    ================================================================================
    Perceptron
    ________________________________________________________________________________
    Training: 
    Perceptron(class_weight={'cantopop': 2.78, 'chinese indie': 20.0,
                             'classic mandopop': 4.0, 'mandopop': 7.0,
                             'taiwan pop': 6.0},
               n_jobs=-1)
    train time: 0.184s
    test time:  0.005s
    accuracy:   0.557
    dimensionality: 50000
    density: 0.815796
    top 10 keywords per class:
    chinese indie: 路过 俩 最快 假想 置身事外 借着 仿似 令 雨里 监制
    classic mandopop: 悄然 大米 出走 沉默不语 老鼠 曰 合音 鞋子 飘来飘去 工程
    mandopop: 表达 心田 成河 恰恰 泪珠 捉迷藏 统筹 多心 不由自主 落寞
    taiwan pop: 可救 无关 理想 感伤 男当 装乖 不管怎样 最大 变形 共看
    cantopop: 前方 相框 置 惊天动地 得来不易 寓言 搜寻 主 下定决心 悬着
    
    classification report:
                      precision    recall  f1-score   support
    
       chinese indie       0.74      0.72      0.73      1665
    classic mandopop       0.28      0.29      0.29       218
            mandopop       0.56      0.57      0.57      1213
          taiwan pop       0.35      0.37      0.36       652
            cantopop       0.43      0.41      0.42       812
    
            accuracy                           0.56      4560
           macro avg       0.47      0.47      0.47      4560
        weighted avg       0.56      0.56      0.56      4560
    
    confusion matrix:
    [[1201   51  218  107   88]
     [  61   64   44   28   21]
     [ 184   44  696  130  159]
     [  93   37  106  242  174]
     [  94   30  177  176  335]]
    
    ================================================================================
    Passive-Aggressive
    ________________________________________________________________________________
    Training: 
    PassiveAggressiveClassifier(class_weight={'cantopop': 2.78,
                                              'chinese indie': 20.0,
                                              'classic mandopop': 4.0,
                                              'mandopop': 7.0, 'taiwan pop': 6.0},
                                n_jobs=-1)
    train time: 0.191s
    test time:  0.003s
    accuracy:   0.563
    dimensionality: 50000
    density: 0.909240
    top 10 keywords per class:
    chinese indie: 迷迷糊糊 下落不明 有生之年 脑海中 借着 说法 哪需 令 雨里 监制
    classic mandopop: 公司 竇 独住 真理 微醺 出走 公园 飘来飘去 工程 曰
    mandopop: 成河 瞑 表示 词 落寞 统筹 耶耶爱 不由自主 山盟海誓 泪珠
    taiwan pop: 变形 配唱 牢牢 猜管 共看 剥落 不管怎样 无名英雄 之上 感伤
    cantopop: 相框 那句 悬着 穿越 放晴 主 心若倦 寓言 得来不易 下定决心
    
    classification report:
                      precision    recall  f1-score   support
    
       chinese indie       0.71      0.75      0.73      1665
    classic mandopop       0.34      0.24      0.28       218
            mandopop       0.55      0.56      0.56      1213
          taiwan pop       0.36      0.36      0.36       652
            cantopop       0.46      0.43      0.45       812
    
            accuracy                           0.56      4560
           macro avg       0.48      0.47      0.48      4560
        weighted avg       0.56      0.56      0.56      4560
    
    confusion matrix:
    [[1242   28  216   91   88]
     [  71   53   53   28   13]
     [ 216   30  685  134  148]
     [ 113   23  120  234  162]
     [ 106   22  174  157  353]]
    
    ================================================================================
    kNN
    ________________________________________________________________________________
    Training: 
    KNeighborsClassifier(n_jobs=-1, n_neighbors=10)
    train time: 0.026s
    test time:  2.824s
    accuracy:   0.428
    classification report:
                      precision    recall  f1-score   support
    
       chinese indie       0.64      0.49      0.55      1665
    classic mandopop       0.43      0.06      0.10       218
            mandopop       0.37      0.66      0.47      1213
          taiwan pop       0.28      0.11      0.16       652
            cantopop       0.31      0.32      0.31       812
    
            accuracy                           0.43      4560
           macro avg       0.40      0.33      0.32      4560
        weighted avg       0.45      0.43      0.41      4560
    
    confusion matrix:
    [[812   6 582  52 213]
     [ 61  13 108  10  26]
     [193   2 800  54 164]
     [ 93   7 317  71 164]
     [119   2 363  71 257]]
    
    ================================================================================
    Random forest
    ________________________________________________________________________________
    Training: 
    RandomForestClassifier(class_weight={'cantopop': 2.78, 'chinese indie': 20.0,
                                         'classic mandopop': 4.0, 'mandopop': 7.0,
                                         'taiwan pop': 6.0},
                           n_jobs=-1)
    train time: 5.406s
    test time:  0.081s
    accuracy:   0.588
    classification report:
                      precision    recall  f1-score   support
    
       chinese indie       0.73      0.77      0.75      1665
    classic mandopop       0.80      0.15      0.25       218
            mandopop       0.48      0.76      0.59      1213
          taiwan pop       0.60      0.16      0.25       652
            cantopop       0.52      0.42      0.46       812
    
            accuracy                           0.59      4560
           macro avg       0.63      0.45      0.46      4560
        weighted avg       0.61      0.59      0.56      4560
    
    confusion matrix:
    [[1280    4  297   13   71]
     [  73   33   86   10   16]
     [ 180    3  924   11   95]
     [ 113    1  301  103  134]
     [ 115    0  323   34  340]]
    
    ================================================================================
    L2 penalty
    ________________________________________________________________________________
    Training: 
    LinearSVC(dual=False)
    train time: 1.355s
    test time:  0.002s
    accuracy:   0.602
    dimensionality: 50000
    density: 1.000000
    top 10 keywords per class:
    chinese indie: 下落不明 只得 谭 极 当天 便 几多 似 令 监制
    classic mandopop: 悄然 公园 公司 飘来飘去 沉默不语 出走 两天 合音 工程 曰
    mandopop: 成河 山盟海誓 落寞 不由自主 涙 寒冷 恰恰 目屎 心田 统筹
    taiwan pop: 痛痛 写下 变形 无关 地平线 感伤 日暮 解 配唱 录音
    cantopop: 置 寓言 前方 放晴 投降 得来不易 师 穿越 一步步 编写
    
    classification report:
                      precision    recall  f1-score   support
    
       chinese indie       0.75      0.78      0.76      1665
    classic mandopop       0.59      0.24      0.34       218
            mandopop       0.56      0.65      0.61      1213
          taiwan pop       0.42      0.34      0.38       652
            cantopop       0.48      0.47      0.48       812
    
            accuracy                           0.60      4560
           macro avg       0.56      0.50      0.51      4560
        weighted avg       0.60      0.60      0.59      4560
    
    confusion matrix:
    [[1295   11  208   65   86]
     [  64   52   59   22   21]
     [ 186    5  793   91  138]
     [  92   13  147  222  178]
     [  89    7  200  131  385]]
    
    ________________________________________________________________________________
    Training: 
    SGDClassifier()
    train time: 0.277s
    test time:  0.003s
    accuracy:   0.597
    dimensionality: 50000
    density: 0.797596
    top 10 keywords per class:
    chinese indie: 不可 仍然 极 没法 几多 未 监制 便 令 似
    classic mandopop: 轻盈 出走 公司 悄然 两天 一吹 合音 飘来飘去 工程 曰
    mandopop: 我俩 分离 相思 昨夜 伊 心头 涙 今夜 阮 真情
    taiwan pop: 七天 地平线 写下 谱曲 解 人声 卢广仲 边界 编辑 录音
    cantopop: 玛丽 回不来 星球 室 告白 穿越 已读 录制 师 编写
    
    classification report:
                      precision    recall  f1-score   support
    
       chinese indie       0.71      0.80      0.75      1665
    classic mandopop       0.60      0.13      0.22       218
            mandopop       0.54      0.71      0.61      1213
          taiwan pop       0.47      0.23      0.31       652
            cantopop       0.48      0.42      0.45       812
    
            accuracy                           0.60      4560
           macro avg       0.56      0.46      0.47      4560
        weighted avg       0.58      0.60      0.57      4560
    
    confusion matrix:
    [[1337    3  220   31   74]
     [  83   29   78   10   18]
     [ 198    1  863   43  108]
     [ 122   12  201  149  168]
     [ 140    3  243   83  343]]
    
    ================================================================================
    L1 penalty
    ________________________________________________________________________________
    Training: 
    LinearSVC(dual=False, penalty='l1')
    train time: 4.383s
    test time:  0.002s
    accuracy:   0.585
    dimensionality: 50000
    density: 0.072232
    top 10 keywords per class:
    chinese indie: 发梦 欠缺 这生 万语千言 谭 似 红日 令 编监 监制
    classic mandopop: 看近 子夜 亮起 碰巧 工程 种籽 合音 娉婷 曰 互比
    mandopop: 通话时间 携带 情丝 风雨无阻 一会儿 鞭 张震岳 恰恰 统筹 目屎
    taiwan pop: 解救 咿耶咿耶 不如说 童话世界 变形 不愿醒 卢广仲 两分钟 无限期 透进
    cantopop: 精准 当梦 师 毋知 落笔 阁 编写 严爵 才刚 心若倦
    
    classification report:
                      precision    recall  f1-score   support
    
       chinese indie       0.74      0.77      0.76      1665
    classic mandopop       0.43      0.14      0.21       218
            mandopop       0.54      0.65      0.59      1213
          taiwan pop       0.39      0.30      0.34       652
            cantopop       0.45      0.45      0.45       812
    
            accuracy                           0.58      4560
           macro avg       0.51      0.46      0.47      4560
        weighted avg       0.57      0.58      0.57      4560
    
    confusion matrix:
    [[1289    7  225   60   84]
     [  68   30   67   29   24]
     [ 190   10  787   87  139]
     [  98   13  156  197  188]
     [  98    9  212  130  363]]
    
    ________________________________________________________________________________
    Training: 
    SGDClassifier(penalty='l1')
    train time: 0.265s
    test time:  0.002s
    accuracy:   0.526
    dimensionality: 50000
    density: 0.007728
    top 10 keywords per class:
    chinese indie: 仿似 当天 未 极 几多 没法 监制 便 令 似
    classic mandopop: 已醒 已重 已闷 已难 已静 已过 龟壳 雨下个 主义 曰
    mandopop: 姑娘 相逢 涙 相思 昨夜 心田 蔓莉 阮 心头 真情
    taiwan pop: 桥边 罗斯福 买单 后知后觉 天黑黑 没差 卢广仲 欧拉 痛痛 气球
    cantopop: 拉链 玛丽 编写 假假 录制 一下下 满洲里 已读 回不来 保护色
    
    classification report:
                      precision    recall  f1-score   support
    
       chinese indie       0.61      0.82      0.70      1665
    classic mandopop       0.15      0.04      0.06       218
            mandopop       0.48      0.65      0.56      1213
          taiwan pop       0.30      0.17      0.22       652
            cantopop       0.48      0.14      0.21       812
    
            accuracy                           0.53      4560
           macro avg       0.40      0.36      0.35      4560
        weighted avg       0.49      0.53      0.48      4560
    
    confusion matrix:
    [[1371    3  232   40   19]
     [  96    8   85   24    5]
     [ 307   14  794   66   32]
     [ 218   15  241  113   65]
     [ 257   14  295  135  111]]
    
    ================================================================================
    Elastic-Net penalty
    ________________________________________________________________________________
    Training: 
    SGDClassifier(penalty='elasticnet')
    train time: 0.637s
    test time:  0.002s
    accuracy:   0.575
    dimensionality: 50000
    density: 0.150872
    top 10 keywords per class:
    chinese indie: 当天 仍然 极 没法 监制 几多 未 便 令 似
    classic mandopop: 追得 竇 看近 监唱 脱脂 合音 一吹 工程 飘来飘去 曰
    mandopop: 轻轻地 相逢 昨夜 涙 伊 相思 心头 今夜 真情 阮
    taiwan pop: 这么一来 没差 双截棍 自然而然 盖世英雄 边界 谱曲 录音 编辑 卢广仲
    cantopop: 简讯 穿越 回不来 玛丽 室 告白 录制 师 已读 编写
    
    classification report:
                      precision    recall  f1-score   support
    
       chinese indie       0.67      0.81      0.74      1665
    classic mandopop       0.44      0.06      0.10       218
            mandopop       0.52      0.72      0.60      1213
          taiwan pop       0.46      0.14      0.21       652
            cantopop       0.46      0.36      0.40       812
    
            accuracy                           0.57      4560
           macro avg       0.51      0.42      0.41      4560
        weighted avg       0.55      0.57      0.54      4560
    
    confusion matrix:
    [[1355    3  226   14   67]
     [  94   12   86   10   16]
     [ 232    2  869   26   84]
     [ 159    5  219   91  178]
     [ 182    5  273   59  293]]
    
    ================================================================================
    NearestCentroid (aka Rocchio classifier)
    ________________________________________________________________________________
    Training: 
    NearestCentroid()
    train time: 0.027s
    test time:  0.005s
    accuracy:   0.564
    classification report:
                      precision    recall  f1-score   support
    
       chinese indie       0.78      0.73      0.75      1665
    classic mandopop       0.20      0.23      0.22       218
            mandopop       0.58      0.57      0.57      1213
          taiwan pop       0.35      0.41      0.38       652
            cantopop       0.44      0.44      0.44       812
    
            accuracy                           0.56      4560
           macro avg       0.47      0.47      0.47      4560
        weighted avg       0.58      0.56      0.57      4560
    
    confusion matrix:
    [[1208   43  189  115  110]
     [  61   50   57   29   21]
     [ 151   77  692  145  148]
     [  58   37  110  266  181]
     [  62   38  149  207  356]]
    
    ================================================================================
    Naive Bayes
    ________________________________________________________________________________
    Training: 
    MultinomialNB(alpha=0.01)
    train time: 0.062s
    test time:  0.002s
    accuracy:   0.622
    dimensionality: 50000
    density: 1.000000
    top 10 keywords per class:
    chinese indie: 世界 似 走 太 未 今天 一生 没 心 没有
    classic mandopop: 也许 曰 不会 心 不要 不能 一天 走 世界 没有
    mandopop: 不能 寂寞 永远 梦 走 知道 不要 爱情 心 没有
    taiwan pop: 时间 心 不会 太 走 知道 爱情 不要 世界 没有
    cantopop: 不能 心 幸福 一起 走 不要 没 爱情 世界 没有
    
    classification report:
                      precision    recall  f1-score   support
    
       chinese indie       0.81      0.79      0.80      1665
    classic mandopop       0.78      0.20      0.32       218
            mandopop       0.57      0.70      0.63      1213
          taiwan pop       0.43      0.31      0.36       652
            cantopop       0.47      0.53      0.50       812
    
            accuracy                           0.62      4560
           macro avg       0.61      0.51      0.52      4560
        weighted avg       0.63      0.62      0.61      4560
    
    confusion matrix:
    [[1311    4  196   59   95]
     [  62   43   76   16   21]
     [ 132    2  851   79  149]
     [  56    4  162  201  229]
     [  64    2  203  112  431]]
    
    ________________________________________________________________________________
    Training: 
    BernoulliNB(alpha=0.01)
    train time: 0.067s
    test time:  0.006s
    accuracy:   0.624
    dimensionality: 50000
    density: 1.000000
    top 10 keywords per class:
    chinese indie: 一生 似 今天 未 走 世界 太 心 没 没有
    classic mandopop: 感觉 太 没 一天 知道 不会 心 走 世界 没有
    mandopop: 不要 永远 世界 梦 寂寞 知道 走 爱情 心 没有
    taiwan pop: 不会 时间 太 知道 心 爱情 没 走 世界 没有
    cantopop: 时间 不要 太 不会 爱情 心 走 没 世界 没有
    
    classification report:
                      precision    recall  f1-score   support
    
       chinese indie       0.87      0.75      0.81      1665
    classic mandopop       0.71      0.25      0.37       218
            mandopop       0.56      0.71      0.63      1213
          taiwan pop       0.40      0.37      0.38       652
            cantopop       0.48      0.53      0.50       812
    
            accuracy                           0.62      4560
           macro avg       0.60      0.52      0.54      4560
        weighted avg       0.64      0.62      0.62      4560
    
    confusion matrix:
    [[1255    8  226   83   93]
     [  54   54   71   18   21]
     [  84    4  861  113  151]
     [  25    5  179  243  200]
     [  22    5  197  157  431]]
    
    ================================================================================
    LinearSVC with L1-based feature selection
    ________________________________________________________________________________
    Training: 
    Pipeline(steps=[('feature_selection',
                     SelectFromModel(estimator=LinearSVC(dual=False,
                                                         penalty='l1'))),
                    ('classification', LinearSVC())])
    train time: 4.259s
    test time:  0.007s
    accuracy:   0.586
    classification report:
                      precision    recall  f1-score   support
    
       chinese indie       0.74      0.77      0.75      1665
    classic mandopop       0.57      0.17      0.27       218
            mandopop       0.54      0.64      0.59      1213
          taiwan pop       0.40      0.33      0.36       652
            cantopop       0.46      0.45      0.45       812
    
            accuracy                           0.59      4560
           macro avg       0.54      0.47      0.48      4560
        weighted avg       0.58      0.59      0.58      4560
    
    confusion matrix:
    [[1289    6  224   63   83]
     [  68   38   64   25   23]
     [ 205    7  772   91  138]
     [  87   11  160  213  181]
     [ 101    5  204  140  362]]
    



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_302_1.png)
    


#### 9.3.4. Four Genres, TfidfVectorizer, Stop Words Updated (Again), All Models

#### 9.3.5. Word Vectorizer Models


```python
from gensim.models import Word2Vec
from gensim.models import KeyedVectors

# Facebook's Chinese word vectors.
fb_model = KeyedVectors.load_word2vec_format('/Users/stuart/Desktop/Spoty-Linguist-Project/wiki.zh/wiki.zh.vec')
```


```python
fb_model.most_similar(positive=[u'高'])
```




    [('低', 0.9805826544761658),
     ('，', 0.9782601594924927),
     ('和', 0.9773808121681213),
     ('也', 0.9772681593894958),
     ('的', 0.9756231904029846),
     ('。', 0.9755971431732178),
     ('以', 0.9750012159347534),
     ('而', 0.9744661450386047),
     ('有', 0.9743781685829163),
     ('其', 0.9742969274520874)]




```python
# Import Jieba to tokenize lyrics
import jieba.posseg as pseg
import jieba
```


```python
# Create a customer tokenizer using JieBa to account for Mandarin
def man_token_jie(x):
    return jieba.lcut(x)
```


```python
import time
#Import the DecisionTreeeClassifier
from sklearn.tree import DecisionTreeClassifier
# Load from the filename
word2vec_df = fb_model
#Initialize the model
clf_decision_word2vec = DecisionTreeClassifier()

start_time = time.time()
# Fit the model
clf_decision_word2vec.fit(word2vec_df, y_train)
print("Time taken to fit the model with word2vec vectors: " + str(time.time() - start_time))
```


    ---------------------------------------------------------------------------

    TypeError                                 Traceback (most recent call last)

    TypeError: float() argument must be a string or a number, not 'KeyedVectors'

    
    The above exception was the direct cause of the following exception:


    ValueError                                Traceback (most recent call last)

    /var/folders/87/4nvcbwyx25s9fz86v2nq_xmw0000gq/T/ipykernel_13777/3623533321.py in <module>
          9 start_time = time.time()
         10 # Fit the model
    ---> 11 clf_decision_word2vec.fit(word2vec_df, y_train)
         12 print("Time taken to fit the model with word2vec vectors: " + str(time.time() - start_time))


    ~/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/tree/_classes.py in fit(self, X, y, sample_weight, check_input, X_idx_sorted)
        935         """
        936 
    --> 937         super().fit(
        938             X,
        939             y,


    ~/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/tree/_classes.py in fit(self, X, y, sample_weight, check_input, X_idx_sorted)
        163             check_X_params = dict(dtype=DTYPE, accept_sparse="csc")
        164             check_y_params = dict(ensure_2d=False, dtype=None)
    --> 165             X, y = self._validate_data(
        166                 X, y, validate_separately=(check_X_params, check_y_params)
        167             )


    ~/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/base.py in _validate_data(self, X, y, reset, validate_separately, **check_params)
        571                 # :(
        572                 check_X_params, check_y_params = validate_separately
    --> 573                 X = check_array(X, **check_X_params)
        574                 y = check_array(y, **check_y_params)
        575             else:


    ~/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)
        736                     array = array.astype(dtype, casting="unsafe", copy=False)
        737                 else:
    --> 738                     array = np.asarray(array, order=order, dtype=dtype)
        739             except ComplexWarning as complex_warning:
        740                 raise ValueError(


    ValueError: setting an array element with a sequence.



```python

```


```python
from time import time
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import RidgeClassifier
from sklearn.pipeline import Pipeline
from sklearn.svm import LinearSVC
from sklearn.linear_model import SGDClassifier
from sklearn.linear_model import Perceptron
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.naive_bayes import BernoulliNB, MultinomialNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors import NearestCentroid
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils.extmath import density
from sklearn.feature_selection import SelectFromModel
from sklearn import metrics

X = df_genres.lyrics[(df_genres.Genre2 == 'cantopop') | (df_genres.Genre2 == 'classic mandopop') | (df_genres.Genre2 == 'taiwan pop') | (df_genres.Genre2 == 'mandopop') | (df_genres.Genre2 == 'chinese indie')]
y = df_genres.Genre2[(df_genres.Genre2 == 'cantopop') | (df_genres.Genre2 == 'classic mandopop') | (df_genres.Genre2 == 'taiwan pop') | (df_genres.Genre2 == 'mandopop') | (df_genres.Genre2 == 'chinese indie')]

# Get Train test split from sample data
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    stratify=y,
                                                    test_size=0.2,
                                                    random_state=1)

w = {'cantopop': np.round(1/0.36,2), 'classic mandopop': np.round(1/0.27),'taiwan pop': np.round(1/0.18), 'mandopop': np.round(1/0.14),'chinese indie': np.round(1/0.05)  }

tvec = TfidfVectorizer(stop_words=updated_stopwords_2,
max_df=0.3,
max_features=50000,
tokenizer=man_token_jie, norm='l2')





tvec.fit(X_train)
X_train = tvec.transform(X_train)
X_test = tvec.transform(X_test)


feature_names = np.array(tvec.get_feature_names())


def trim(s):
    """Trim string to fit on terminal (assuming 80-column display)"""
    return s if len(s) <= 80 else s[:77] + "..."

###############################################################################
# Benchmark classifiers


def benchmark(clf):
    print(('_' * 80))
    print("Training: ")
    print(clf)
    t0 = time()
    clf.fit(X_train, y_train)
    train_time = time() - t0
    print(("train time: %0.3fs" % train_time))

    t0 = time()
    pred = clf.predict(X_test)
    test_time = time() - t0
    print(("test time:  %0.3fs" % test_time))

    score = metrics.accuracy_score(y_test, pred)
    print(("accuracy:   %0.3f" % score))

    if hasattr(clf, 'coef_'):
        print(("dimensionality: %d" % clf.coef_.shape[1]))
        print(("density: %f" % density(clf.coef_)))

        if feature_names is not None:
            print("top 10 keywords per class:")
            for i, category in enumerate(set(y_train.values)):
                top10 = np.argsort(clf.coef_[i])[-10:]
                print((trim("%s: %s"
                            % (category, " ".join(feature_names[top10])))))
        print()

    print("classification report:")
    print((metrics.classification_report(y_test, pred,
                                         target_names=set(y_train.values))))

    print("confusion matrix:")
    print((metrics.confusion_matrix(y_test, pred)))

    print()
    clf_descr = str(clf).split('(')[0]
    return clf_descr, score, train_time, test_time


results = []
for clf, name in ((Perceptron(max_iter=1000, tol=1e-3, class_weight=w, n_jobs=-1), "Perceptron"),(PassiveAggressiveClassifier(max_iter=1000, tol=1e-3, class_weight=w,n_jobs=-1), "Passive-Aggressive"),(KNeighborsClassifier(n_neighbors=10, n_jobs=-1), "kNN"),(RandomForestClassifier(n_estimators=100, class_weight=w, n_jobs=-1), "Random forest")):
    print(('=' * 80))
    print(name)
    results.append(benchmark(clf))

for penalty in ["l2", "l1"]:
    print(('=' * 80))
    print(("%s penalty" % penalty.upper()))
    # Train Liblinear model
    results.append(benchmark(LinearSVC(loss='squared_hinge', penalty=penalty,
                                            dual=False)))

    # Train SGD model
    results.append(benchmark(SGDClassifier(alpha=.0001,
                                           penalty=penalty,
                                           max_iter=1000,
                                           tol=1e-3)))

# Train SGD with Elastic Net penalty
print(('=' * 80))
print("Elastic-Net penalty")
results.append(benchmark(SGDClassifier(alpha=.0001,
                                       penalty="elasticnet",
                                       max_iter=1000,
                                       tol=1e-3)))

# Train NearestCentroid without threshold
print(('=' * 80))
print("NearestCentroid (aka Rocchio classifier)")
results.append(benchmark(NearestCentroid()))

# Train sparse Naive Bayes classifiers
print(('=' * 80))
print("Naive Bayes")
results.append(benchmark(MultinomialNB(alpha=.01)))
results.append(benchmark(BernoulliNB(alpha=.01)))

print(('=' * 80))
print("LinearSVC with L1-based feature selection")
# The smaller C, the stronger the regularization.
# The more regularization, the more sparsity.
results.append(benchmark(Pipeline([
    ('feature_selection', SelectFromModel(LinearSVC(penalty="l1", dual=False))),
    ('classification', LinearSVC(penalty="l2"))])))
# make some plots

indices = np.arange(len(results))

results = [[x[i] for x in results] for i in range(4)]

clf_names, score, training_time, test_time = results
training_time = np.array(training_time) / np.max(training_time)
test_time = np.array(test_time) / np.max(test_time)

plt.figure(figsize=(12, 8))
plt.title("Score")
plt.barh(indices, score, .2, label="score", color='r')
plt.barh(indices + .3, training_time, .2, label="training time", color='g')
plt.barh(indices + .6, test_time, .2, label="test time", color='b')
plt.yticks((), fontsize=14)
plt.xticks(fontsize=14)
plt.legend(loc='best')
plt.subplots_adjust(left=.25)
plt.subplots_adjust(top=.95)
plt.subplots_adjust(bottom=.05)

for i, c in zip(indices, clf_names):
    plt.text(-.3, i, c, fontsize=14)

plt.show()
```

    ================================================================================
    Perceptron
    ________________________________________________________________________________
    Training: 
    Perceptron(class_weight={'cantopop': 2.78, 'chinese indie': 20.0,
                             'classic mandopop': 4.0, 'mandopop': 7.0,
                             'taiwan pop': 6.0},
               n_jobs=-1)
    train time: 0.123s
    test time:  0.003s
    accuracy:   0.555
    dimensionality: 50000
    density: 0.814836
    top 10 keywords per class:
    chinese indie: 路过 仿似 麻醉 俩 巡演 若伤 有生之年 令 雨里 监制
    classic mandopop: 公司 无菇 真理 向往 工程 难以形容 曰 沉默不语 飘来飘去 两天
    mandopop: 瓢泼 一大堆 尽全力 落寞 目屎 泪珠 成河 装不出 多心 不由自主
    taiwan pop: 安置 缓慢 不凡 最大 品味 采取行动 自然而然 尝 录音 不管怎样
    cantopop: 秃头 下定决心 穿越 落笔 丢失 布鲁士 前方 悬着 得来不易 心若倦
    
    classification report:
                      precision    recall  f1-score   support
    
       chinese indie       0.75      0.71      0.73      1665
    classic mandopop       0.29      0.31      0.30       218
            mandopop       0.55      0.55      0.55      1213
          taiwan pop       0.34      0.36      0.35       652
            cantopop       0.45      0.48      0.46       812
    
            accuracy                           0.56      4560
           macro avg       0.48      0.48      0.48      4560
        weighted avg       0.56      0.56      0.56      4560
    
    confusion matrix:
    [[1174   50  228  109  104]
     [  58   67   42   32   19]
     [ 174   49  671  143  176]
     [  78   32  131  234  177]
     [  74   30  157  164  387]]
    
    ================================================================================
    Passive-Aggressive
    ________________________________________________________________________________
    Training: 
    PassiveAggressiveClassifier(class_weight={'cantopop': 2.78,
                                              'chinese indie': 20.0,
                                              'classic mandopop': 4.0,
                                              'mandopop': 7.0, 'taiwan pop': 6.0},
                                n_jobs=-1)
    train time: 0.149s
    test time:  0.003s
    accuracy:   0.561
    dimensionality: 50000
    density: 0.912172
    top 10 keywords per class:
    chinese indie: 谭 有生之年 下落不明 说法 似 借着 置身事外 令 雨里 监制
    classic mandopop: 公园 属相 小星星 无菇 难以形容 微醺 公司 飘来飘去 工程 曰
    mandopop: 多心 尽全力 表示 词 成河 瞑 不由自主 耶耶爱 统筹 泪珠
    taiwan pop: 缓慢 解 配唱 装乖 微加 控制不了 不管怎样 感伤 共看 尝
    cantopop: 走火入魔 大提琴 投降 穿越 心若倦 布鲁士 二十一 得来不易 寓言 下定决心
    
    classification report:
                      precision    recall  f1-score   support
    
       chinese indie       0.72      0.75      0.73      1665
    classic mandopop       0.32      0.25      0.28       218
            mandopop       0.55      0.56      0.55      1213
          taiwan pop       0.37      0.35      0.36       652
            cantopop       0.44      0.44      0.44       812
    
            accuracy                           0.56      4560
           macro avg       0.48      0.47      0.47      4560
        weighted avg       0.55      0.56      0.56      4560
    
    confusion matrix:
    [[1241   31  214   86   93]
     [  69   55   46   29   19]
     [ 210   33  678  120  172]
     [ 100   30  124  226  172]
     [ 107   23  175  149  358]]
    
    ================================================================================
    kNN
    ________________________________________________________________________________
    Training: 
    KNeighborsClassifier(n_jobs=-1, n_neighbors=10)
    train time: 0.023s
    test time:  2.505s
    accuracy:   0.387
    classification report:
                      precision    recall  f1-score   support
    
       chinese indie       0.72      0.31      0.44      1665
    classic mandopop       0.52      0.06      0.10       218
            mandopop       0.34      0.74      0.47      1213
          taiwan pop       0.35      0.06      0.10       652
            cantopop       0.27      0.35      0.31       812
    
            accuracy                           0.39      4560
           macro avg       0.44      0.31      0.28      4560
        weighted avg       0.48      0.39      0.36      4560
    
    confusion matrix:
    [[524   4 793  28 316]
     [ 30  12 130   1  45]
     [ 74   2 902  10 225]
     [ 50   3 371  40 188]
     [ 47   2 442  34 287]]
    
    ================================================================================
    Random forest
    ________________________________________________________________________________
    Training: 
    RandomForestClassifier(class_weight={'cantopop': 2.78, 'chinese indie': 20.0,
                                         'classic mandopop': 4.0, 'mandopop': 7.0,
                                         'taiwan pop': 6.0},
                           n_jobs=-1)
    train time: 5.527s
    test time:  0.067s
    accuracy:   0.590
    classification report:
                      precision    recall  f1-score   support
    
       chinese indie       0.73      0.76      0.75      1665
    classic mandopop       0.84      0.17      0.28       218
            mandopop       0.50      0.72      0.59      1213
          taiwan pop       0.64      0.19      0.30       652
            cantopop       0.46      0.49      0.48       812
    
            accuracy                           0.59      4560
           macro avg       0.63      0.46      0.48      4560
        weighted avg       0.61      0.59      0.57      4560
    
    confusion matrix:
    [[1261    5  285   16   98]
     [  71   36   78   10   23]
     [ 182    1  869   18  143]
     [  93    1  235  125  198]
     [ 112    0  275   26  399]]
    
    ================================================================================
    L2 penalty
    ________________________________________________________________________________
    Training: 
    LinearSVC(dual=False)
    train time: 1.054s
    test time:  0.002s
    accuracy:   0.602
    dimensionality: 50000
    density: 1.000000
    top 10 keywords per class:
    chinese indie: 兴奋 下落不明 谭 极 几多 便 当天 似 令 监制
    classic mandopop: 真理 公园 飘来飘去 公司 出走 两天 沉默不语 合音 工程 曰
    mandopop: 表示 涙 恰恰 落寞 不由自主 寒冷 成河 心田 目屎 统筹
    taiwan pop: 变形 不管怎样 感伤 地平线 之上 一号 日暮 解 配唱 录音
    cantopop: 惊天动地 前方 抬起 得来不易 寓言 投降 穿越 一步步 师 编写
    
    classification report:
                      precision    recall  f1-score   support
    
       chinese indie       0.75      0.78      0.77      1665
    classic mandopop       0.58      0.22      0.32       218
            mandopop       0.57      0.65      0.60      1213
          taiwan pop       0.43      0.35      0.39       652
            cantopop       0.47      0.46      0.46       812
    
            accuracy                           0.60      4560
           macro avg       0.56      0.49      0.51      4560
        weighted avg       0.60      0.60      0.59      4560
    
    confusion matrix:
    [[1307   10  194   62   92]
     [  66   48   64   20   20]
     [ 193    8  788   82  142]
     [  86   11  149  229  177]
     [  97    6  198  136  375]]
    
    ________________________________________________________________________________
    Training: 
    SGDClassifier()
    train time: 0.258s
    test time:  0.003s
    accuracy:   0.597
    dimensionality: 50000
    density: 0.799892
    top 10 keywords per class:
    chinese indie: 不可 当天 仍然 极 没法 几多 监制 便 令 似
    classic mandopop: 监唱 出走 公司 悄然 一吹 两天 飘来飘去 合音 工程 曰
    mandopop: 往事 我俩 伊 相思 涙 昨夜 心头 阮 今夜 真情
    taiwan pop: 配唱 没差 解 地平线 谱曲 人声 边界 卢广仲 编辑 录音
    cantopop: 玛丽 星球 录音室 室 告白 穿越 已读 师 录制 编写
    
    classification report:
                      precision    recall  f1-score   support
    
       chinese indie       0.72      0.81      0.76      1665
    classic mandopop       0.56      0.13      0.21       218
            mandopop       0.53      0.71      0.61      1213
          taiwan pop       0.46      0.22      0.30       652
            cantopop       0.48      0.42      0.45       812
    
            accuracy                           0.60      4560
           macro avg       0.55      0.46      0.47      4560
        weighted avg       0.58      0.60      0.57      4560
    
    confusion matrix:
    [[1344    3  222   27   69]
     [  84   28   78   13   15]
     [ 192    3  862   45  111]
     [ 120   11  206  146  169]
     [ 128    5  251   86  342]]
    
    ================================================================================
    L1 penalty
    ________________________________________________________________________________
    Training: 
    LinearSVC(dual=False, penalty='l1')
    train time: 3.032s
    test time:  0.002s
    accuracy:   0.586
    dimensionality: 50000
    density: 0.075076
    top 10 keywords per class:
    chinese indie: 风花雪月 下落不明 快些 似 编监 红日 令 这生 万语千言 监制
    classic mandopop: 子夜 亮起 看近 工程 碰巧 种籽 合音 娉婷 曰 互比
    mandopop: 情丝 成河 一会儿 携带 风雨无阻 张震岳 恰恰 多样 统筹 目屎
    taiwan pop: 不如说 惊鸿一瞥 变形 抜 语言学 童话世界 两分钟 卢广仲 无限期 透进
    cantopop: 阁 精准 师 毋知 监牢 当梦 才刚 严爵 落笔 心若倦
    
    classification report:
                      precision    recall  f1-score   support
    
       chinese indie       0.74      0.77      0.76      1665
    classic mandopop       0.39      0.11      0.18       218
            mandopop       0.54      0.66      0.60      1213
          taiwan pop       0.40      0.30      0.34       652
            cantopop       0.46      0.45      0.46       812
    
            accuracy                           0.59      4560
           macro avg       0.51      0.46      0.47      4560
        weighted avg       0.57      0.59      0.57      4560
    
    confusion matrix:
    [[1284    7  242   56   76]
     [  76   25   66   23   28]
     [ 182    9  800   86  136]
     [  91   13  161  198  189]
     [  97   10  205  134  366]]
    
    ________________________________________________________________________________
    Training: 
    SGDClassifier(penalty='l1')
    train time: 0.235s
    test time:  0.002s
    accuracy:   0.533
    dimensionality: 50000
    density: 0.008032
    top 10 keywords per class:
    chinese indie: 别要 只得 监制 当天 极 没法 几多 便 令 似
    classic mandopop: 对望 对焦 对照 对爱施 对手戏 需留 岑林 蘑菇 归一 曰
    mandopop: 亲像 今夜 相逢 涙 阮 目屎 相思 昨夜 心头 真情
    taiwan pop: 李硕 退一步 编辑 卡拉 击杀 梦想家 立地成佛 黑吃黑 公公 卢广仲
    cantopop: 告白 洪申豪 山楂树 录制 旅客 坏掉 窒爱 师 抠 不屑
    
    classification report:
                      precision    recall  f1-score   support
    
       chinese indie       0.62      0.82      0.71      1665
    classic mandopop       0.13      0.05      0.07       218
            mandopop       0.49      0.65      0.56      1213
          taiwan pop       0.36      0.01      0.02       652
            cantopop       0.41      0.33      0.37       812
    
            accuracy                           0.53      4560
           macro avg       0.40      0.37      0.34      4560
        weighted avg       0.49      0.53      0.48      4560
    
    confusion matrix:
    [[1363   10  225    3   64]
     [ 100   10   86    2   20]
     [ 302   17  783    3  108]
     [ 202   22  226    8  194]
     [ 233   18  287    6  268]]
    
    ================================================================================
    Elastic-Net penalty
    ________________________________________________________________________________
    Training: 
    SGDClassifier(penalty='elasticnet')
    train time: 0.572s
    test time:  0.002s
    accuracy:   0.576
    dimensionality: 50000
    density: 0.158920
    top 10 keywords per class:
    chinese indie: 不可 仍然 当天 极 几多 没法 监制 便 令 似
    classic mandopop: 无菇 班房 看近 脱脂 监唱 合音 一吹 工程 飘来飘去 曰
    mandopop: 轻轻地 相逢 相思 涙 伊 昨夜 心头 今夜 阮 真情
    taiwan pop: 没差 高尚情操 这么一来 自然而然 盖世英雄 边界 编辑 录音 谱曲 卢广仲
    cantopop: 得来不易 录音室 回不来 玛丽 室 告白 师 已读 录制 编写
    
    classification report:
                      precision    recall  f1-score   support
    
       chinese indie       0.66      0.82      0.73      1665
    classic mandopop       0.44      0.05      0.09       218
            mandopop       0.53      0.72      0.61      1213
          taiwan pop       0.45      0.14      0.21       652
            cantopop       0.46      0.36      0.41       812
    
            accuracy                           0.58      4560
           macro avg       0.51      0.42      0.41      4560
        weighted avg       0.55      0.58      0.54      4560
    
    confusion matrix:
    [[1357    2  228   16   62]
     [  97   11   79   12   19]
     [ 233    3  877   25   75]
     [ 164    6  208   89  185]
     [ 200    3  260   55  294]]
    
    ================================================================================
    NearestCentroid (aka Rocchio classifier)
    ________________________________________________________________________________
    Training: 
    NearestCentroid()
    train time: 0.025s
    test time:  0.004s
    accuracy:   0.585
    classification report:
                      precision    recall  f1-score   support
    
       chinese indie       0.84      0.72      0.78      1665
    classic mandopop       0.24      0.22      0.23       218
            mandopop       0.58      0.63      0.60      1213
          taiwan pop       0.34      0.40      0.37       652
            cantopop       0.46      0.48      0.47       812
    
            accuracy                           0.58      4560
           macro avg       0.49      0.49      0.49      4560
        weighted avg       0.60      0.58      0.59      4560
    
    confusion matrix:
    [[1205   41  201  116  102]
     [  47   48   67   39   17]
     [ 106   53  763  154  137]
     [  34   29  129  264  196]
     [  36   30  151  209  386]]
    
    ================================================================================
    Naive Bayes
    ________________________________________________________________________________
    Training: 
    MultinomialNB(alpha=0.01)
    train time: 0.073s
    test time:  0.004s
    accuracy:   0.620
    dimensionality: 50000
    density: 1.000000
    top 10 keywords per class:
    chinese indie: 我心 知 得到 没法 不必 仍然 愿 便 令 似
    classic mandopop: 吹 害怕 应该 温暖 依然 青春 风 不用 阳光 曰
    mandopop: 回头 是不是 地方 感情 泪 心情 喔 伤心 思念 阮
    taiwan pop: 习惯 喔 不用 越 勇敢 我爱你 看见 害怕 想念 微笑
    cantopop: 发现 想念 改变 就让 勇敢 存在 声 录音室 微笑 喔
    
    classification report:
                      precision    recall  f1-score   support
    
       chinese indie       0.80      0.79      0.79      1665
    classic mandopop       0.76      0.20      0.32       218
            mandopop       0.57      0.69      0.62      1213
          taiwan pop       0.43      0.32      0.37       652
            cantopop       0.47      0.53      0.50       812
    
            accuracy                           0.62      4560
           macro avg       0.61      0.51      0.52      4560
        weighted avg       0.63      0.62      0.61      4560
    
    confusion matrix:
    [[1312    5  197   59   92]
     [  61   44   73   17   23]
     [ 146    3  832   83  149]
     [  53    4  164  209  222]
     [  71    2  194  113  432]]
    
    ________________________________________________________________________________
    Training: 
    BernoulliNB(alpha=0.01)
    train time: 0.066s
    test time:  0.007s
    accuracy:   0.616
    dimensionality: 50000
    density: 1.000000
    top 10 keywords per class:
    chinese indie: 我心 变 得到 仍然 不必 没法 愿 便 令 似
    classic mandopop: 风 脸 变 总 感情 青春 依然 阳光 天 温暖
    mandopop: 留下 风 回头 日子 轻轻 泪 伤心 感情 心情 思念
    taiwan pop: 想念 思念 脸 发现 害怕 吉他 录音 看着 看见 微笑
    cantopop: 录音师 害怕 录音室 觉得 勇敢 吉他 发现 编写 声 微笑
    
    classification report:
                      precision    recall  f1-score   support
    
       chinese indie       0.86      0.75      0.80      1665
    classic mandopop       0.66      0.25      0.36       218
            mandopop       0.56      0.71      0.62      1213
          taiwan pop       0.39      0.36      0.38       652
            cantopop       0.47      0.51      0.49       812
    
            accuracy                           0.62      4560
           macro avg       0.59      0.52      0.53      4560
        weighted avg       0.63      0.62      0.62      4560
    
    confusion matrix:
    [[1246   12  233   79   95]
     [  58   54   67   16   23]
     [  84    5  860  117  147]
     [  27    5  178  237  205]
     [  29    6  208  156  413]]
    
    ================================================================================
    LinearSVC with L1-based feature selection
    ________________________________________________________________________________
    Training: 
    Pipeline(steps=[('feature_selection',
                     SelectFromModel(estimator=LinearSVC(dual=False,
                                                         penalty='l1'))),
                    ('classification', LinearSVC())])
    train time: 3.475s
    test time:  0.005s
    accuracy:   0.586
    classification report:
                      precision    recall  f1-score   support
    
       chinese indie       0.73      0.77      0.75      1665
    classic mandopop       0.49      0.17      0.25       218
            mandopop       0.55      0.65      0.60      1213
          taiwan pop       0.39      0.31      0.35       652
            cantopop       0.45      0.43      0.44       812
    
            accuracy                           0.59      4560
           macro avg       0.52      0.47      0.48      4560
        weighted avg       0.57      0.59      0.57      4560
    
    confusion matrix:
    [[1289    9  219   63   85]
     [  74   37   63   21   23]
     [ 192   10  791   81  139]
     [  95   13  155  202  187]
     [ 106    6  203  145  352]]
    



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_311_1.png)
    


### 9.4. Unsupervised Classifiation Models to Identify Topics
Is it possible to identify topics using the lyrics. This could be useful to create playlists based on a topic a person is trying to learn. Clustering songs together based on common words or themes could also be helpful for people learning a new language as they would encounter similar words in each song. 

#### 9.4.1. KMeans TfidfVectorizer


```python
data = df_tracks_master_clean3_lyrics_genre_hsk_v2
lyrics = data.lyrics
```


```python
from numpy import unique
from numpy import where
from sklearn.datasets import make_classification
from sklearn.cluster import KMeans
from matplotlib import pyplot
# define dataset

# fit the tvec on the 35k rows of lyrics, will take include the 10k top features in terms of scores. 
# Higher scores go to words that have more disciminating power. I.e. those that appear frequently in some documents,
# but occur relatively less freqently overall. 
tvec = TfidfVectorizer(stop_words=updated_stopwords,max_df=0.3,max_features=10000,tokenizer=man_token_jie, norm='l2')
tvec.fit(lyrics)

# as we fit on all lyrics, and we are transoforming the 35k lyric rows into a dataframe, all 10k features will occur
# at least once. 
X_transformed = tvec.transform(lyrics)


# Consider X_transform each row is a vector, pointing to a position in 10k dimensional space.KMeans looks at these 
# points in 10k dimensional space and finds the 60 best centroids. 
from sklearn.cluster import KMeans
modelkmeans = KMeans(n_clusters=60, init='k-means++', n_init=100)
modelkmeans.fit(X_transformed)
```




    KMeans(n_clusters=60, n_init=100)




```python
# think of the X_matrix as mapping each song to a position in 10k dimensional space based on its lyrics
X_transformed.toarray().shape
```




    (35921, 10000)




```python
# the dimensions are based on the word features available
tvec.get_feature_names
```




    <bound method CountVectorizer.get_feature_names of TfidfVectorizer(max_df=0.3, max_features=10000,
                    stop_words={'、', '。', '〈', '〉', '《', '》', '一', '一个', '一些', '一何',
                                '一切', '一则', '一方面', '一旦', '一来', '一样', '一种', '一般',
                                '一转眼', '七', '万一', '三', '上', '上下', '下', '不', '不仅',
                                '不但', '不光', '不单', ...},
                    tokenizer=<function man_token_jie at 0x7fdb0b4cc940>)>




```python
# for example if two songs both had 一则' and '一方面' they would be mapped to a similar dimensional space
```


```python
# these are the labels assigned to each of the 35k songs (60 labels)
modelkmeans.labels_
```




    array([58,  3, 29, ..., 20, 42, 29], dtype=int32)




```python
# number of labels matches the number of songs
modelkmeans.labels_.shape
```




    (35921,)




```python
# update the main dataframe so that labels can be seen
data['labels'] = modelkmeans.labels_
```


```python
data.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>artist_href</th>
      <th>artist</th>
      <th>artist_uri</th>
      <th>artist_type</th>
      <th>song_available_markets</th>
      <th>song_duration_ms</th>
      <th>is_explicit</th>
      <th>song_href</th>
      <th>preview_url</th>
      <th>track_number</th>
      <th>song_type</th>
      <th>length</th>
      <th>popularity</th>
      <th>album</th>
      <th>album_type</th>
      <th>album_artwork_link</th>
      <th>album_tracks_num</th>
      <th>album_uri</th>
      <th>album_release_date</th>
      <th>acousticness</th>
      <th>danceability</th>
      <th>energy</th>
      <th>instrumentalness</th>
      <th>liveness</th>
      <th>loudness</th>
      <th>speechiness</th>
      <th>tempo</th>
      <th>time_signature</th>
      <th>lyrics</th>
      <th>time_stamp</th>
      <th>genres</th>
      <th>lyrics_len</th>
      <th>hsk1</th>
      <th>hsk2</th>
      <th>hsk3</th>
      <th>hsk4</th>
      <th>hsk5</th>
      <th>hsk6</th>
      <th>hsk_level</th>
      <th>hsk_level_6_80p</th>
      <th>hsk_level_5_80p</th>
      <th>hsk_level_4_80p</th>
      <th>hsk_level_3_80p</th>
      <th>hsk_level_2_80p</th>
      <th>hsk_level_1_80p</th>
      <th>hsk_level_5_75p</th>
      <th>hsk_level_4_75p</th>
      <th>hsk_level_3_75p</th>
      <th>hsk_level_2_75p</th>
      <th>hsk_level_1_75p</th>
      <th>genre_0</th>
      <th>genre_1</th>
      <th>genre_2</th>
      <th>Genre1</th>
      <th>Genre2</th>
      <th>Genre3</th>
      <th>Genre4</th>
      <th>Genre5</th>
      <th>Genre6</th>
      <th>Genre7</th>
      <th>weblink</th>
      <th>groupings</th>
      <th>labels</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>英雄 (Live)</td>
      <td>https://api.spotify.com/v1/artists/2elBjNSdBE2...</td>
      <td>Jay Chou</td>
      <td>spotify:artist:2elBjNSdBE2Y3f0j1mjrql</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>182040.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/1i31BYKv1La3...</td>
      <td>https://p.scdn.co/mp3-preview/191037a026d9e098...</td>
      <td>1.0</td>
      <td>track</td>
      <td>182040.0</td>
      <td>31.0</td>
      <td>周杰倫地表最強世界巡迴演唱會</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273494cdb...</td>
      <td>25.0</td>
      <td>spotify:album:5lqE2qT3nGLW6FGoJYfwUT</td>
      <td>2019-11-01</td>
      <td>0.000172</td>
      <td>0.522</td>
      <td>0.908</td>
      <td>0.002500</td>
      <td>0.458</td>
      <td>-8.747</td>
      <td>0.0596</td>
      <td>118.987</td>
      <td>4.0</td>
      <td>英雄词曲人生不是一个人的游戏一起奋斗一起超越一起杀吧兄弟好战好胜战胜逆命管他天赋够不够我们都...</td>
      <td>00:00.00,00:10.81,00:21.62,00:32.43,00:34.37,0...</td>
      <td>NaN</td>
      <td>288</td>
      <td>0.280303</td>
      <td>0.469697</td>
      <td>0.583333</td>
      <td>0.666667</td>
      <td>0.856061</td>
      <td>0.946970</td>
      <td>5</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>c-pop</td>
      <td>mandopop</td>
      <td>taiwan pop</td>
      <td>zhongguo feng</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>https://open.spotify.com/track/1i31BYKv1La3wVC...</td>
      <td>41</td>
      <td>58</td>
    </tr>
    <tr>
      <th>1</th>
      <td>雙截棍 (Live)</td>
      <td>https://api.spotify.com/v1/artists/2elBjNSdBE2...</td>
      <td>Jay Chou</td>
      <td>spotify:artist:2elBjNSdBE2Y3f0j1mjrql</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>145173.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/77uwbOEaj6nM...</td>
      <td>https://p.scdn.co/mp3-preview/80c09c504a54344b...</td>
      <td>2.0</td>
      <td>track</td>
      <td>145173.0</td>
      <td>30.0</td>
      <td>周杰倫地表最強世界巡迴演唱會</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273494cdb...</td>
      <td>25.0</td>
      <td>spotify:album:5lqE2qT3nGLW6FGoJYfwUT</td>
      <td>2019-11-01</td>
      <td>0.002810</td>
      <td>0.461</td>
      <td>0.953</td>
      <td>0.000000</td>
      <td>0.424</td>
      <td>-6.440</td>
      <td>0.1360</td>
      <td>101.037</td>
      <td>4.0</td>
      <td>双截棍词曲岩烧店的烟味弥漫店里面的妈妈桑教拳脚武术的老板练铁沙掌硬底子功夫最擅长还会金钟罩铁...</td>
      <td>00:00.00,00:09.79,00:19.58,00:29.37,00:31.72,0...</td>
      <td>NaN</td>
      <td>370</td>
      <td>0.320000</td>
      <td>0.460000</td>
      <td>0.670000</td>
      <td>0.810000</td>
      <td>0.870000</td>
      <td>0.980000</td>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>c-pop</td>
      <td>mandopop</td>
      <td>taiwan pop</td>
      <td>zhongguo feng</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>https://open.spotify.com/track/77uwbOEaj6nMy6T...</td>
      <td>41</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>開不了口 (Live)</td>
      <td>https://api.spotify.com/v1/artists/2elBjNSdBE2...</td>
      <td>Jay Chou</td>
      <td>spotify:artist:2elBjNSdBE2Y3f0j1mjrql</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>272973.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/5OR1Fbd7RSI1...</td>
      <td>https://p.scdn.co/mp3-preview/82a62689347391e7...</td>
      <td>3.0</td>
      <td>track</td>
      <td>272973.0</td>
      <td>32.0</td>
      <td>周杰倫地表最強世界巡迴演唱會</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273494cdb...</td>
      <td>25.0</td>
      <td>spotify:album:5lqE2qT3nGLW6FGoJYfwUT</td>
      <td>2019-11-01</td>
      <td>0.001760</td>
      <td>0.519</td>
      <td>0.612</td>
      <td>0.000002</td>
      <td>0.564</td>
      <td>-10.634</td>
      <td>0.0267</td>
      <td>139.944</td>
      <td>4.0</td>
      <td>开不了口词曲才离开没多久就开始担心今天的你过得好不好整个画面是你嘴嘟嘟那可爱的模样还有在你身...</td>
      <td>00:00.00,00:24.00,00:48.00,01:12.01,01:14.58,0...</td>
      <td>NaN</td>
      <td>299</td>
      <td>0.404762</td>
      <td>0.595238</td>
      <td>0.833333</td>
      <td>0.892857</td>
      <td>0.928571</td>
      <td>0.988095</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>c-pop</td>
      <td>mandopop</td>
      <td>taiwan pop</td>
      <td>zhongguo feng</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>https://open.spotify.com/track/5OR1Fbd7RSI18Oo...</td>
      <td>2</td>
      <td>29</td>
    </tr>
    <tr>
      <th>3</th>
      <td>床邊故事 (Live)</td>
      <td>https://api.spotify.com/v1/artists/2elBjNSdBE2...</td>
      <td>Jay Chou</td>
      <td>spotify:artist:2elBjNSdBE2Y3f0j1mjrql</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>220240.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/7MmT3xzugKwe...</td>
      <td>https://p.scdn.co/mp3-preview/5316d5602f55d671...</td>
      <td>4.0</td>
      <td>track</td>
      <td>220240.0</td>
      <td>29.0</td>
      <td>周杰倫地表最強世界巡迴演唱會</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273494cdb...</td>
      <td>25.0</td>
      <td>spotify:album:5lqE2qT3nGLW6FGoJYfwUT</td>
      <td>2019-11-01</td>
      <td>0.039800</td>
      <td>0.640</td>
      <td>0.844</td>
      <td>0.000005</td>
      <td>0.122</td>
      <td>-9.645</td>
      <td>0.0539</td>
      <td>106.033</td>
      <td>4.0</td>
      <td>床边故事词曲从前从前有只猫头鹰它站在屋顶屋顶后面一遍森林森林很安静安静的钢琴在大厅阁楼里仔细...</td>
      <td>00:00.00,00:06.45,00:12.91,00:19.37,00:20.74,0...</td>
      <td>NaN</td>
      <td>346</td>
      <td>0.368852</td>
      <td>0.500000</td>
      <td>0.655738</td>
      <td>0.803279</td>
      <td>0.901639</td>
      <td>0.950820</td>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>c-pop</td>
      <td>mandopop</td>
      <td>taiwan pop</td>
      <td>zhongguo feng</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>https://open.spotify.com/track/7MmT3xzugKweEte...</td>
      <td>41</td>
      <td>29</td>
    </tr>
    <tr>
      <th>4</th>
      <td>夜曲+竊愛 (Live)</td>
      <td>https://api.spotify.com/v1/artists/2elBjNSdBE2...</td>
      <td>Jay Chou</td>
      <td>spotify:artist:2elBjNSdBE2Y3f0j1mjrql</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>222013.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/4vlzCpNsDFBa...</td>
      <td>https://p.scdn.co/mp3-preview/446b1630b52837ae...</td>
      <td>5.0</td>
      <td>track</td>
      <td>222013.0</td>
      <td>41.0</td>
      <td>周杰倫地表最強世界巡迴演唱會</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273494cdb...</td>
      <td>25.0</td>
      <td>spotify:album:5lqE2qT3nGLW6FGoJYfwUT</td>
      <td>2019-11-01</td>
      <td>0.028300</td>
      <td>0.538</td>
      <td>0.610</td>
      <td>0.005630</td>
      <td>0.386</td>
      <td>-10.513</td>
      <td>0.1680</td>
      <td>174.093</td>
      <td>4.0</td>
      <td>夜曲夜曲词曲窃爱词曲一群嗜血的蚂蚁被腐肉所吸引我面无表情看孤独的风景失去你失去你当鸽子不再象...</td>
      <td>00:00.00,00:03.37,00:03.70,00:04.45,00:05.10,0...</td>
      <td>NaN</td>
      <td>414</td>
      <td>0.233333</td>
      <td>0.306667</td>
      <td>0.526667</td>
      <td>0.680000</td>
      <td>0.813333</td>
      <td>0.933333</td>
      <td>5</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>c-pop</td>
      <td>mandopop</td>
      <td>taiwan pop</td>
      <td>zhongguo feng</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>https://open.spotify.com/track/4vlzCpNsDFBaW99...</td>
      <td>20</td>
      <td>32</td>
    </tr>
  </tbody>
</table>
</div>




```python
# 254 songs in each category
data[data['labels'] == 1].lyrics.shape
```




    (254,)




```python
# now look at what are the most discerning features in each category
```


```python
# the first category (i.e. label 1)
# this is the matrix for all songs in category 1 (i.e. matrix mapping them to dimensional space)
tvec.transform(data[data['labels'] == 1].lyrics).toarray()
```




    array([[0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 0., 0.],
           ...,
           [0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 0., 0.]])




```python
group1 = pd.DataFrame(tvec.transform(data[data['labels'] == 1].lyrics).toarray(),columns =tvec.get_feature_names() )
```


```python
group1
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>一一</th>
      <th>一万</th>
      <th>一万个</th>
      <th>一万年</th>
      <th>一万次</th>
      <th>一万遍</th>
      <th>一下</th>
      <th>一下子</th>
      <th>一不小心</th>
      <th>一世</th>
      <th>一丝</th>
      <th>一丝丝</th>
      <th>一个个</th>
      <th>一个圈</th>
      <th>一串</th>
      <th>一举一动</th>
      <th>一二三</th>
      <th>一二三四</th>
      <th>一于</th>
      <th>一人</th>
      <th>一代</th>
      <th>一件</th>
      <th>一份</th>
      <th>一会</th>
      <th>一会儿</th>
      <th>一位</th>
      <th>一体</th>
      <th>一关</th>
      <th>一再</th>
      <th>一出</th>
      <th>一出戏</th>
      <th>一刀</th>
      <th>一刀两断</th>
      <th>一分</th>
      <th>一分一秒</th>
      <th>一分钟</th>
      <th>一切都是</th>
      <th>一别</th>
      <th>一到</th>
      <th>一刹</th>
      <th>...</th>
      <th>黄土</th>
      <th>黄昏</th>
      <th>黄沙</th>
      <th>黄河</th>
      <th>黄算</th>
      <th>黄色</th>
      <th>黄金</th>
      <th>黎明</th>
      <th>黎明前</th>
      <th>黏</th>
      <th>黑</th>
      <th>黑发</th>
      <th>黑夜</th>
      <th>黑暗</th>
      <th>黑洞</th>
      <th>黑白</th>
      <th>黑眼圈</th>
      <th>黑色</th>
      <th>默契</th>
      <th>默念</th>
      <th>默然</th>
      <th>默许</th>
      <th>默默</th>
      <th>默默地</th>
      <th>黯淡</th>
      <th>黯然</th>
      <th>鼓</th>
      <th>鼓励</th>
      <th>鼓和声</th>
      <th>鼓声</th>
      <th>鼓手</th>
      <th>鼓掌</th>
      <th>鼓楼</th>
      <th>鼓舞</th>
      <th>鼓起勇气</th>
      <th>鼻子</th>
      <th>齐</th>
      <th>齐来</th>
      <th>齐齐</th>
      <th>龙</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.223017</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.051451</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.02679</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.063826</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.027825</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>249</th>
      <td>0.142957</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>250</th>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>251</th>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>252</th>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>253</th>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
<p>254 rows × 10000 columns</p>
</div>




```python
# as an example, looking at 鼻子 (nose)
# its weighting assigned is 0.0278
# if 鼻子 occurs in a song then that song is mapped to 鼻子 with weight 0.0278
# if it occurs twice that song is mapped to 鼻子 with 0.0278*2.
# higher weights indicate that the word mapping will be further from the centre
group1['鼻子'].value_counts()
```




    0.000000    252
    0.051451      1
    0.027825      1
    Name: 鼻子, dtype: int64




```python
# the word with the greatest weighting accross all songs in this bucket is 再见 goodbye.
# it can be seen that 再见 has both the greatest weighting in this category (0.29) and also occurs the most frequently
# hence when summed accross songs has a value of 74. This weighting frequency holds up for all words.
print('Group1 words with greatest aggregate weighting:')
print(group1.sum().sort_values(ascending=False)[:30])
print('Group1 greatest weightings:')
print(group1.mean().sort_values(ascending=False)[:30])
```

    Group1 words with greatest aggregate weighting:
    再见     74.702798
    脸       5.783756
    一天      5.168989
    相见      4.822330
    想念      4.729018
    一面      4.647293
    也许      4.636408
    时间      4.490539
    永远      4.437379
    离别      4.391933
    走       4.217593
    告别      4.022310
    思念      3.985854
    最后      3.962825
    没有      3.861208
    眼泪      3.743197
    回忆      3.688381
    不要      3.620866
    一点      3.594091
    来不及     3.243713
    怀念      3.090310
    不能      3.012689
    不再      2.997391
    离开      2.934047
    从前      2.914221
    我会      2.885348
    不会      2.844505
    没       2.824049
    那天      2.739196
    不想      2.698365
    dtype: float64
    Group1 greatest weightings:
    再见     0.294106
    脸      0.022771
    一天     0.020350
    相见     0.018986
    想念     0.018618
    一面     0.018296
    也许     0.018254
    时间     0.017679
    永远     0.017470
    离别     0.017291
    走      0.016605
    告别     0.015836
    思念     0.015692
    最后     0.015602
    没有     0.015202
    眼泪     0.014737
    回忆     0.014521
    不要     0.014255
    一点     0.014150
    来不及    0.012771
    怀念     0.012167
    不能     0.011861
    不再     0.011801
    离开     0.011551
    从前     0.011473
    我会     0.011360
    不会     0.011199
    没      0.011118
    那天     0.010784
    不想     0.010623
    dtype: float64



```python

```


```python
# for each of the 60 labels, look at the words that were given the highest score in terms of their ability to distinguish
# from each of the other 60 labels. 
topics = {}
for i in range(60):
#     display(pd.DataFrame(tvec.transform(data[data['labels']==i].lyrics).toarray(), columns =tvec.get_feature_names()).sum().sort_values(ascending=False)[:20])
    topics[i] = pd.DataFrame(tvec.transform(data[data['labels']==i].lyrics).toarray(), columns =tvec.get_feature_names()).sum().sort_values(ascending=False)[:20]
```


```python
# Initial Analysis of topics: 
# Lots of the same ideas coming out.

# POSTIVE sentiments:
#
# NEGATIVE sentiments:
# grief, fear, longing, 
# NEUTRAL sentiments:
# family / growing up, romance, moon/ stars/night, friendship, 

# japanese characters, cantonese characters, 

# # 0-2 vague: goodbye, time,
# 3: youth, flowers, lightness, 
# 4,5: negative feeling
# 6: lonely, taciturn. 
# 7: flying, freedom, sky,
# 8,9: slightly negative,
# 10: love
# 11: family and growing up,
# 12,14, 16, 19: music terminology should be deleted
# 13: dreams, spirituality
# 15: departing,
# 17: nothing
# 18: boys and girls, romance
# 20: through life,
# 21: love
# 22: happiness
# 23: moon, stars, night, twinkling 
# 24: nothing
# 25: grief
# 26: cantonese words (maybe these should be filtered out)
# 27: happiness
# 28: fear
# 29: nothing
# 30: nothing
# 31: nothing
# 32: romance
# 33: today, yesterday
# 34: negative feeling
# 35: tomorrow, yesterday,
# 36: nothing
# 37: side by side, together,
# 38: boys and girls love, 
# 39: mistakes
# 40: nothing much
# 41: sweet happy memories
# 42: wishing dreaming, 
# 43: not much
# 44: not much
# 45: words to be removed
# 46: flying heavens
# 47: friendship
# 48: not able to
# 49: boys and girls
# 50: nothing much
# 51: nothing much
# 52: nothing much
# 53: love
# 54: love
# 55: nothing much
# 56: nothing much
# 57: nothing much
# 58: together
# 59:  Japanese characters



topics
```




    {0: 永远    105.628771
     不会     13.248149
     我会      8.605710
     忘记      7.538167
     不变      7.194917
     没有      7.115805
     世界      7.063160
     时间      6.398662
     身边      6.343440
     不再      6.230700
     改变      6.213841
     生命      5.900713
     心       5.631121
     愿       5.592855
     感觉      5.548468
     我俩      5.464185
     失去      5.445065
     美丽      5.404034
     怀念      5.352528
     也许      5.203141
     dtype: float64,
     1: 再见     74.702798
     脸       5.783756
     一天      5.168989
     相见      4.822330
     想念      4.729018
     一面      4.647293
     也许      4.636408
     时间      4.490539
     永远      4.437379
     离别      4.391933
     走       4.217593
     告别      4.022310
     思念      3.985854
     最后      3.962825
     没有      3.861208
     眼泪      3.743197
     回忆      3.688381
     不要      3.620866
     一点      3.594091
     来不及     3.243713
     dtype: float64,
     2: 现在     94.686882
     未来     16.590336
     曾经     12.393024
     没有     10.783314
     没       9.067866
     不会      8.139845
     真的      8.007453
     不再      7.942898
     已经      7.757705
     听       7.717387
     知道      7.679866
     过去      7.286486
     太       7.074691
     不要      6.911049
     明白      6.888330
     世界      6.862405
     不想      6.777633
     时间      6.579035
     走       6.485155
     是不是     6.462160
     dtype: float64,
     3: 笑     34.144281
     青春    29.977142
     花     29.457033
     相思    29.233725
     吹     29.183549
     问     29.153604
     姑娘    28.677974
     少年    27.712510
     一片    27.264305
     风     27.040144
     听     26.615091
     人生    25.748122
     雨     25.580150
     人间    25.473583
     不知    25.150069
     开     23.947790
     思念    23.746865
     愿     23.695458
     路     22.808475
     轻轻    22.693383
     dtype: float64,
     4: 未     73.549291
     便     60.546985
     令     54.218916
     没     53.648639
     似     51.498456
     讲     48.885649
     其实    48.318978
     仍然    45.509513
     没法    43.905103
     太     43.012049
     得到    36.749783
     没有    35.769255
     情人    34.865877
     原来    33.500774
     恋爱    33.319192
     不必    32.456940
     愿     32.362899
     不可    32.019062
     吻     30.382319
     知     29.592117
     dtype: float64,
     5: 不要    171.640485
     没有     13.363952
     问      12.904089
     太      10.929134
     知道     10.208031
     走       9.364251
     告诉      8.975192
     不会      8.661712
     爱情      7.949855
     眼泪      7.947888
     想要      7.713258
     心       7.432057
     不再      7.300992
     相信      7.266182
     已经      7.251488
     过去      7.199221
     哭       7.103960
     没       6.668420
     世界      6.607648
     心里      6.436075
     dtype: float64,
     6: 寂寞    89.747800
     没有     9.044640
     温柔     7.953439
     懂      6.225826
     以后     6.157298
     承诺     6.139995
     沉默     5.935342
     回忆     5.910579
     心      5.570026
     不会     5.473823
     爱情     5.470175
     也许     5.409446
     不能     5.404448
     快乐     5.272845
     总是     5.266437
     手      4.962319
     感觉     4.950394
     走      4.832038
     太      4.764204
     最后     4.735732
     dtype: float64,
     7: 自由    67.395268
     没有     7.531925
     温柔     6.999267
     走      6.496808
     天空     5.708837
     世界     5.024223
     太      4.927096
     不会     4.789757
     生活     4.658428
     快乐     4.640238
     不想     4.616333
     不再     4.539843
     想要     4.500838
     懂      4.208511
     不要     4.073756
     飞翔     3.972337
     感受     3.911086
     知道     3.839610
     放手     3.716025
     爱情     3.701450
     dtype: float64,
     8: 知道    132.741047
     没有     12.935615
     真的     11.471390
     已经      9.623229
     不会      9.036701
     心里      8.612783
     太       8.385723
     我会      7.507762
     不想      7.402200
     其实      7.317703
     走       7.305134
     拥抱      6.973250
     没       6.882751
     需要      6.759328
     想要      6.721817
     不要      6.670943
     现在      6.136534
     心       6.121943
     不能      5.982560
     一直      5.965353
     dtype: float64,
     9: 不能    92.424415
     没有     9.458963
     一次     7.479449
     温柔     6.147404
     爱情     5.802839
     无法     5.701754
     心里     5.025994
     知道     4.939298
     心      4.916945
     寂寞     4.883791
     告诉     4.770109
     世界     4.752156
     忘记     4.694179
     没      4.665340
     不会     4.555052
     最后     4.535422
     曾经     4.490826
     走      4.437396
     真的     4.428750
     放弃     4.418189
     dtype: float64,
     10: 我爱你     71.810774
     知道       4.629361
     心        4.375029
     月亮代表     3.868583
     一句       3.843194
     来不及      2.931112
     有多深      2.898044
     世界       2.850292
     没有       2.743492
     爱情       2.734060
     问        2.692590
     不会       2.520699
     永远       2.495128
     对不起      2.400097
     呼吸       2.400071
     生命       2.296185
     无法       2.273590
     爱着       2.272253
     懂        2.269548
     吻        2.266068
     dtype: float64,
     11: 妈妈     40.452303
     爸爸      8.805626
     长大      2.391507
     回家      2.323989
     家       2.301658
     不要      1.993222
     没有      1.713885
     吃       1.697316
     一起      1.691736
     告诉      1.667642
     娃娃      1.651479
     我会      1.519063
     带       1.468005
     知道      1.467189
     大汉      1.462992
     第一次     1.455512
     很小      1.441329
     听       1.435982
     声音      1.397777
     咿       1.386411
     dtype: float64,
     12: 录音室    40.898283
     处理     37.070853
     后期     35.577549
     混音     28.954714
     带      21.673641
     工程师    19.528308
     母带     17.200629
     师母     17.005122
     录音师    16.619798
     声      15.791655
     工程     15.636967
     编写     15.183250
     录音     12.568372
     师      10.406072
     助理     10.392919
     弦乐     10.266098
     吉他     10.151207
     贝斯      9.770195
     配唱      9.398231
     鼓       7.612420
     dtype: float64,
     13: 梦     119.013206
     一场     16.486223
     风       8.673940
     心       8.600175
     没有      8.038734
     懂       7.746027
     天空      7.697726
     匆匆      7.571164
     曾经      7.445198
     走       7.350479
     红尘      7.172313
     不会      6.982369
     不再      6.535907
     温柔      6.471654
     世界      6.383268
     就让      6.288819
     吹       6.201117
     回忆      6.022044
     拥有      5.822562
     等待      5.806300
     dtype: float64,
     14: 混音     86.334803
     录音     63.547508
     弦乐     58.996285
     吉他     52.778271
     声      45.511560
     音乐     39.780922
     贝斯     36.919526
     录音师    35.443742
     母带     34.753060
     编写     33.145185
     人声     32.893409
     录音室    32.042885
     鼓      31.590718
     工程师    29.965166
     监制     25.127720
     录音棚    25.015796
     师      19.735469
     发行     19.084218
     配唱     18.904953
     带      18.492162
     dtype: float64,
     15: 离开    105.126578
     明白     11.177867
     没有     10.656004
     不会     10.011611
     回来      8.981698
     没       8.691410
     不再      8.287054
     不要      8.240510
     回忆      8.198049
     也许      8.021821
     以后      7.496618
     走       7.429370
     不能      7.333150
     未来      7.067595
     存在      6.965725
     已经      6.626474
     等待      6.555960
     心       6.266460
     太       6.257535
     曾经      6.220858
     dtype: float64,
     16: 唱      88.093868
     一首歌    13.703119
     听      11.568754
     情歌     10.612290
     歌       7.901949
     首歌      7.702368
     歌声      6.267277
     出       5.867842
     一首      5.523552
     一起      5.443619
     歌唱      5.188597
     轻轻      4.817822
     没有      4.766607
     唱歌      4.598340
     走       4.291937
     世界      4.230493
     没       3.955753
     也许      3.772666
     陪       3.772086
     有没有     3.686482
     dtype: float64,
     17: 喔      87.020692
     没有      4.015346
     知道      2.494825
     耶       2.288662
     走       2.274189
     现在      2.212675
     世界      2.203772
     真的      2.192197
     是否      2.095684
     梦       1.978783
     不能      1.960410
     太       1.914832
     心情      1.900559
     越来越     1.875325
     没       1.846751
     心       1.833271
     呜呜      1.817072
     不会      1.789980
     快       1.759296
     需要      1.728916
     dtype: float64,
     18: 男     111.335704
     女     102.189474
     合      31.577017
     男女     10.875604
     男合     10.175479
     女合      5.987225
     爱情      5.916059
     没有      5.065999
     没       4.732899
     合女      4.533583
     幸福      4.304001
     一起      4.182110
     心       4.122789
     一生      3.836896
     难       3.680168
     一次      3.662524
     永远      3.655086
     太       3.647563
     陪       3.617292
     不要      3.453629
     dtype: float64,
     19: 歌      56.662268
     一首     19.648131
     唱      16.197821
     写      14.480483
     听       8.032944
     情歌      7.432182
     歌词      5.374957
     没有      4.542798
     首歌      4.422783
     旋律      4.283950
     一首歌     3.610633
     快乐      3.594231
     完       3.559838
     这首      3.371674
     曲       3.332964
     思念      2.955309
     歌声      2.895549
     太       2.847847
     诗       2.828240
     梦       2.809288
     dtype: float64,
     20: 一生    109.392098
     愿      10.395964
     一世      9.313966
     永远      8.576865
     心       8.007499
     一次      7.258007
     没有      7.188289
     未       7.148395
     人生      6.918856
     没       6.540282
     心中      6.458710
     不必      6.440680
     伴       6.403175
     问       6.350680
     真       6.251732
     命运      6.176497
     知道      6.155749
     几多      6.036986
     愿意      5.892525
     不会      5.872010
     dtype: float64,
     21: 爱人    50.224759
     没有     2.756664
     爱情     2.436205
     心      2.360932
     我会     2.266494
     吻      2.257078
     青春     2.138942
     眼睛     2.108561
     欺骗     2.078795
     美丽     2.060561
     没      2.015813
     怀念     1.983916
     原来     1.959706
     幸福     1.939799
     爱到     1.907273
     猜      1.892945
     走      1.888384
     也许     1.841497
     听      1.839619
     如今     1.826957
     dtype: float64,
     22: 幸福    103.605484
     爱情     10.440307
     没有      9.582518
     哭       9.416647
     快乐      9.262064
     相信      9.040310
     孤独      9.039213
     我要      8.787460
     不会      7.845368
     最后      7.839908
     满足      7.736399
     永远      7.624389
     路       7.576582
     太       7.510832
     拥抱      7.480742
     清楚      7.299737
     微笑      7.227311
     世界      7.211156
     走       7.182439
     一起      7.178922
     dtype: float64,
     23: 月亮      36.546086
     星星      35.670866
     天上       5.548215
     月光       5.086563
     今晚       4.891574
     没有       4.027177
     美丽       3.979587
     夜空       3.939104
     一颗       3.906225
     眼睛       3.847776
     太阳       3.754093
     夜里       3.621003
     天空       3.618332
     世界       3.336579
     远方       3.266277
     闪烁       3.228209
     千千阙歌     3.092271
     路上       2.883851
     千千       2.867197
     晚星亮      2.781431
     dtype: float64,
     24: 心     139.435191
     一颗     19.961488
     没有     13.452431
     不会     12.241761
     一次     10.967362
     真的      8.811586
     知道      8.711684
     感觉      8.570718
     已经      8.532342
     不再      8.496245
     失去      8.331144
     懂       8.288153
     回忆      8.049113
     爱情      7.908781
     走       7.744585
     过去      7.674167
     相信      7.399514
     温柔      7.107133
     忘       7.071021
     曾经      7.022551
     dtype: float64,
     25: 伤心    62.012555
     爱情     5.693489
     心      5.254197
     不再     5.097099
     不要     4.772991
     忘记     4.754192
     回忆     4.654531
     知道     4.370394
     没有     4.235938
     已经     4.154950
     过去     4.012238
     太      3.992960
     走      3.967289
     问      3.931158
     不会     3.871518
     难过     3.750904
     曾经     3.645617
     快乐     3.613932
     相信     3.525916
     永远     3.501921
     dtype: float64,
     26: 唔    30.371386
     嘅    23.639872
     冇    15.435874
     咁    15.230181
     佢    14.805122
     系    13.904293
     话     8.585443
     乜     8.504949
     啲     6.726890
     咗     5.939024
     讲     5.839705
     揾     5.765682
     咪     5.661193
     哋     5.555205
     嘢     5.345114
     嚟     4.742041
     晒     4.480832
     睇     4.420994
     喺     4.318428
     俾     3.619739
     dtype: float64,
     27: 快乐    101.788215
     没有      7.587221
     一起      6.247306
     真的      6.006217
     笑       5.976204
     心       5.707040
     不要      5.612407
     感觉      5.410531
     不想      5.354696
     美丽      5.353321
     幸福      5.102771
     生活      4.897120
     原来      4.752558
     悲伤      4.742193
     懂       4.690397
     需要      4.647553
     不会      4.646163
     寂寞      4.513077
     拥有      4.414542
     没       4.374136
     dtype: float64,
     28: 怕     79.381176
     没有     6.767753
     太      6.757737
     没      6.275617
     不要     6.189165
     听      4.776772
     知道     4.752036
     不会     4.620107
     不必     4.521243
     不想     4.513852
     应该     4.462330
     害怕     4.445024
     也许     4.380003
     突然     4.347133
     笑      4.171759
     需要     3.995035
     懂      3.851259
     难      3.813378
     愿      3.779953
     爱情     3.775772
     dtype: float64,
     29: 太     71.773015
     不会    68.505662
     没     67.814352
     时间    64.459488
     没有    63.017222
     想要    53.829450
     不想    50.546134
     感觉    50.393617
     真的    50.166833
     需要    48.433675
     生活    45.934573
     不再    44.196102
     一点    44.176764
     也许    43.914079
     已经    41.135078
     听     39.914005
     最后    39.443708
     懂     39.037213
     相信    38.158616
     世界    37.670107
     dtype: float64,
     30: 有人    63.750342
     总      5.108152
     没有     4.078088
     真的     3.922472
     世界     3.766961
     听      3.601776
     太      3.306865
     陪      3.036227
     需要     2.944435
     知道     2.935699
     懂      2.847905
     人生     2.837166
     明白     2.758777
     不会     2.713897
     新      2.595835
     也许     2.592921
     寂寞     2.551623
     我会     2.543161
     不能     2.366508
     音乐     2.350637
     dtype: float64,
     31: 是否    95.394830
     没有     9.583561
     问      8.274366
     心      7.920865
     不再     6.928778
     依然     6.903474
     知道     6.863456
     也许     6.745867
     曾经     6.618534
     梦      6.568474
     不会     6.522291
     愿意     6.405666
     真的     6.357902
     已经     6.292088
     回到     6.126385
     世界     5.934342
     总是     5.866763
     一天     5.830051
     寂寞     5.819418
     能够     5.716383
     dtype: float64,
     32: 爱情    149.064635
     没有     15.456574
     心       9.327493
     不会      8.687348
     不要      8.442550
     世界      8.322555
     相信      8.212498
     一次      7.965400
     没       7.839498
     也许      7.775485
     曾经      7.665444
     总是      7.661634
     美丽      7.619877
     知道      7.565540
     走       7.318148
     心里      7.286278
     告诉      7.153371
     温柔      7.083477
     最后      6.921398
     不能      6.918956
     dtype: float64,
     33: 今天    89.785508
     昨天    11.094275
     当天     8.328755
     没      5.783928
     明天     5.520838
     没有     5.383417
     回家     5.314297
     太      5.149974
     一天     5.085073
     过去     4.828667
     讲      4.815186
     不要     4.778787
     一起     4.726283
     当初     4.560745
     知道     4.534971
     我心     4.503073
     不想     4.282143
     走      4.125259
     需要     4.101155
     似      4.064750
     dtype: float64,
     34: 没有    173.991918
     没      17.373603
     不会     12.337568
     知道     11.930063
     需要     11.544871
     时间     11.276723
     已经     10.930879
     太      10.693840
     世界     10.483061
     从来     10.274283
     感觉     10.097440
     走       9.805177
     失去      9.085691
     心里      8.976440
     最后      8.864789
     忘记      8.748658
     不再      8.669902
     相信      8.479139
     眼泪      8.238876
     心       8.214425
     dtype: float64,
     35: 明天    81.032090
     没有     8.649691
     昨天     8.041680
     世界     7.742519
     今天     6.867213
     也许     5.408937
     心      5.195228
     走      5.162177
     时间     4.983618
     不要     4.749829
     不会     4.739005
     没      4.732710
     未来     4.629346
     微笑     4.418113
     面对     4.382609
     生命     4.296740
     知道     4.249720
     改变     4.245710
     我要     4.034632
     脸      4.024826
     dtype: float64,
     36: 找     61.509666
     不到    52.369739
     没有     8.938114
     听      6.713504
     感觉     6.112144
     走      6.103158
     太      5.496645
     不想     5.227573
     得到     5.073410
     需要     5.064504
     世界     4.976930
     不要     4.826533
     哭      4.812280
     知道     4.650168
     时间     4.540017
     笑      4.522481
     没      4.447409
     找到     4.395501
     不会     4.204557
     最后     4.202849
     dtype: float64,
     37: 陪     76.053325
     一起     9.651911
     身边     8.401968
     我会     8.065397
     走      7.786706
     世界     6.486273
     一直     6.473695
     没有     5.767687
     时间     5.636637
     幸福     5.348152
     身旁     5.323081
     生命     4.818683
     不会     4.737966
     看着     4.561457
     没      4.432032
     永远     4.420712
     也许     4.172757
     不能     4.069431
     曾经     4.057137
     不要     3.978614
     dtype: float64,
     38: 女人    62.700611
     男人    19.762039
     爱情     4.009917
     心      3.553806
     美丽     2.949367
     太      2.798877
     每个     2.707716
     一生     2.705279
     真      2.704206
     感情     2.690922
     花      2.686546
     不能     2.623391
     总是     2.509364
     寂寞     2.451859
     坏      2.435129
     知道     2.408660
     容易     2.307477
     走      2.301182
     付出     2.297096
     天真     2.254330
     dtype: float64,
     39: 错     70.872882
     没有     8.872671
     不要     5.517583
     太      5.478968
     没      5.357358
     走      4.771687
     寂寞     4.661966
     难过     4.338795
     当初     4.065730
     不能     4.052129
     时间     4.045516
     眼泪     3.907351
     懂      3.728947
     痛      3.599684
     不会     3.558086
     问      3.471030
     恨      3.466704
     也许     3.440053
     其实     3.368740
     相信     3.333338
     dtype: float64,
     40: 走     123.894875
     没有     13.332963
     路      13.048734
     回头     11.356968
     手      10.327385
     带       9.959880
     最后      8.489141
     没       8.101878
     真的      7.855441
     不会      7.753546
     曾经      7.247348
     温柔      6.879678
     也许      6.771810
     尽头      6.606161
     梦       6.569773
     远       6.523125
     以后      6.431122
     已经      6.429942
     不要      6.411486
     一直      6.351853
     dtype: float64,
     41: 甜蜜    44.195598
     美丽     4.542091
     回忆     4.526470
     心      4.144709
     不会     3.963094
     太      3.833819
     温柔     3.762103
     爱情     3.743033
     幸福     3.700304
     过去     3.633665
     心里     3.602596
     想起     3.293361
     没有     3.282282
     忘记     3.239157
     笑      3.193232
     分手     3.035472
     浪漫     2.994163
     一起     2.968954
     恋爱     2.842297
     呼吸     2.759252
     dtype: float64,
     42: 希望    73.048798
     梦想     6.500209
     不会     6.395207
     世界     6.138959
     知道     5.564977
     时间     5.440235
     能够     5.379446
     没有     5.174786
     心里     5.106660
     心      4.794927
     身边     4.765904
     感觉     4.668542
     力量     4.561132
     我会     4.498052
     一天     4.480389
     没      4.300485
     想要     4.276342
     梦      4.193042
     相信     4.189891
     最后     4.047123
     dtype: float64,
     43: 喜欢     97.193332
     没有      5.960799
     一起      5.599372
     感觉      5.363124
     知道      5.210929
     太       5.040542
     世界      4.563656
     真的      4.537655
     想要      3.989676
     爱情      3.880912
     快乐      3.770949
     爱上你     3.767823
     听       3.751859
     没       3.685032
     不要      3.573498
     陪       3.398254
     其实      3.373376
     一直      3.315899
     永远      3.251792
     起来      2.943072
     dtype: float64,
     44: 地方    76.007873
     流浪     9.208633
     没有     8.066392
     天堂     7.461334
     身旁     7.281315
     模样     6.711486
     遗忘     6.008603
     家      5.625220
     遥远     5.471466
     方向     5.129807
     梦想     5.084199
     世界     5.015267
     阳光     4.876233
     一直     4.777665
     温暖     4.772902
     知道     4.717374
     翅膀     4.401052
     走      4.365667
     飞翔     4.255792
     离开     4.196075
     dtype: float64,
     45: 阮     85.814792
     甲     12.363774
     拢     12.296264
     讲      9.949756
     袂      9.770911
     搁      9.393371
     伊      9.373736
     暝      8.296473
     不知     8.058211
     心肝     7.471329
     孤单     7.202615
     欲      7.158122
     惦      6.471325
     心      6.240057
     心情     6.207517
     呒      6.204630
     犹原     5.455253
     青春     5.435285
     亲像     5.343202
     已经     5.118971
     dtype: float64,
     46: 飞     70.627160
     天空     7.010613
     想要     6.245464
     世界     5.846372
     起来     5.382905
     没有     4.841684
     美      4.012742
     我要     3.867001
     心      3.827012
     飞过     3.700030
     没      3.572059
     走      3.571566
     风筝     3.512375
     翅膀     3.440736
     追      3.342375
     一只     3.273134
     知道     3.135383
     蝴蝶     3.110923
     远      2.952746
     自由     2.935937
     dtype: float64,
     47: 朋友    76.745169
     一起     6.185801
     走      5.760722
     没有     5.550525
     不要     4.663833
     以后     3.770618
     不会     3.687496
     左右     3.407790
     快乐     3.390255
     情人     3.187123
     人生     3.178484
     陪      3.172354
     理由     3.168612
     寂寞     3.067181
     变成     3.006833
     能够     2.982435
     找      2.981196
     没      2.925964
     干杯     2.883784
     知道     2.833830
     dtype: float64,
     48: 无法    59.925774
     美丽    56.867144
     世界     9.169852
     没有     8.990559
     忘记     7.690107
     已经     7.200981
     不会     7.166681
     生命     7.160558
     眼睛     6.833615
     相信     6.746285
     一次     6.487968
     爱情     6.454316
     呼吸     6.220916
     知道     6.167691
     心      6.065829
     时间     5.961806
     感觉     5.905792
     太      5.818247
     也许     5.754476
     真的     5.682480
     dtype: float64,
     49: 男女     74.424048
     女      20.481731
     男合     17.447281
     男      17.394170
     合      12.988850
     男合女     6.230502
     合女      6.010527
     女合      3.795154
     合合      2.486423
     一生      1.485092
     永远      1.399165
     幸福      1.243968
     快乐      1.224947
     爱情      1.116837
     想念      1.063544
     今天      1.036804
     浪漫      1.031951
     不要      1.029752
     没有      1.017731
     真       1.007115
     dtype: float64,
     50: 记得    79.242821
     恭喜    17.878365
     忘记     7.470963
     忘      6.973161
     没有     5.930628
     以后     5.878624
     不会     5.760518
     回忆     5.750940
     永远     5.687833
     时间     5.417668
     走      5.332679
     我会     4.880682
     没      4.823251
     一起     4.820356
     世界     4.666846
     想起     4.456615
     曾经     4.431791
     快乐     4.346166
     太      4.250377
     是否     4.239859
     dtype: float64,
     51: 噢     47.486655
     嫂子     3.623965
     狗      1.685616
     不要     1.429919
     想要     1.420108
     也许     1.330476
     凉      1.320424
     没有     1.252458
     喔      1.209417
     故乡     1.153913
     爸爸     1.146009
     梦      0.995333
     哭      0.994728
     一起     0.990438
     一直     0.967940
     耶      0.950285
     我会     0.904323
     走      0.893629
     蛋      0.889293
     知道     0.880429
     dtype: float64,
     52: 世界    133.228500
     没有     13.020890
     整个     10.512670
     改变     10.511537
     时间      9.535177
     生命      9.273834
     未来      8.771099
     宇宙      8.625710
     心       8.317658
     感觉      7.890957
     生活      7.724410
     不会      7.526059
     每个      7.316911
     太       6.984559
     梦想      6.959621
     需要      6.733718
     听       6.543599
     告诉      6.423340
     天空      6.385515
     美丽      6.354356
     dtype: float64,
     53: 心中    91.085884
     我心     8.839926
     心里     8.382485
     心      8.030835
     愿      7.157172
     轻轻     6.981305
     不再     6.883967
     风      6.731321
     没有     6.355357
     令      6.180261
     泪      6.090298
     感觉     5.687528
     似      5.680948
     知道     5.539970
     愿意     5.428101
     雨      5.391163
     永远     5.382635
     不知     5.346586
     始终     5.338856
     永不     5.338801
     dtype: float64,
     54: 亲爱    61.218864
     永远     3.350887
     没有     3.093427
     想要     2.589218
     走      2.580960
     世界     2.521676
     不要     2.498784
     时间     2.466042
     爱情     2.439793
     宝贝     2.431426
     知道     2.319496
     告诉     2.244189
     是否     2.226724
     朋友     2.145487
     小孩     2.106407
     快乐     2.070420
     现在     2.014432
     姑娘     1.971106
     哭      1.908714
     日子     1.872968
     dtype: float64,
     55: 哒     25.062054
     嘟      0.961675
     滴答     0.605293
     听到     0.587503
     喔      0.567323
     真的     0.558390
     爱情     0.546558
     哔      0.528186
     说谎     0.517474
     重新     0.491752
     唔      0.472061
     滴哒     0.449177
     笑笑     0.442798
     不能     0.439053
     唱      0.432954
     绿洲     0.431138
     夏夜     0.429248
     反复     0.424609
     透露     0.422630
     装作     0.420579
     dtype: float64,
     56: 慢慢    68.900547
     没有     6.766385
     时间     6.000155
     习惯     5.363589
     走      5.073890
     心      4.810031
     温柔     4.727471
     已经     4.669631
     回忆     4.612083
     过去     4.539813
     忘记     4.067634
     变成     3.973822
     不再     3.859884
     爱情     3.842851
     最后     3.815537
     轻轻     3.671320
     太      3.519374
     不能     3.414186
     遗忘     3.413763
     感觉     3.390565
     dtype: float64,
     57: 一天    121.712148
     身边      9.493842
     时间      8.284362
     一点      8.283975
     永远      7.628892
     世界      7.492190
     发现      7.347265
     没       7.265644
     没有      7.220458
     我会      6.716181
     一年      6.593051
     生命      6.483840
     知道      6.407598
     不会      6.400761
     终于      5.802482
     走       5.521740
     一次      5.488430
     感觉      5.300907
     脸       5.238262
     不想      5.207243
     dtype: float64,
     58: 一起    109.139306
     没有     10.256612
     世界      7.370462
     不要      7.186015
     走       6.856582
     相信      6.538589
     分离      6.380240
     快乐      6.300768
     两个      6.269181
     知道      5.826030
     没       5.786671
     忘记      5.570298
     永远      5.507497
     心里      5.495958
     分享      5.301164
     太       5.246926
     最后      5.050136
     每天      4.993765
     时间      4.967178
     美丽      4.949999
     dtype: float64,
     59: 気     20.675664
     見     20.588631
     涙     18.056973
     誰     14.983123
     変     14.148380
     私     13.139537
     帰     12.006513
     君     11.382737
     顔     10.690412
     愛     10.368530
     夢      9.700493
     今日     7.675529
     時      7.593204
     僕      7.459611
     明日     6.542896
     二人     6.477330
     言      6.393913
     作曲     6.291083
     出      5.499763
     泣      5.486116
     dtype: float64}




```python
group_all = pd.DataFrame(tvec.transform(data.lyrics).toarray(),columns =tvec.get_feature_names() )
group_all
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>一一</th>
      <th>一万</th>
      <th>一万个</th>
      <th>一万年</th>
      <th>一万次</th>
      <th>一万遍</th>
      <th>一下</th>
      <th>一下子</th>
      <th>一不小心</th>
      <th>一世</th>
      <th>一丝</th>
      <th>一丝丝</th>
      <th>一个个</th>
      <th>一个圈</th>
      <th>一串</th>
      <th>一举一动</th>
      <th>一二三</th>
      <th>一二三四</th>
      <th>一于</th>
      <th>一人</th>
      <th>一代</th>
      <th>一件</th>
      <th>一份</th>
      <th>一会</th>
      <th>一会儿</th>
      <th>一位</th>
      <th>一体</th>
      <th>一关</th>
      <th>一再</th>
      <th>一出</th>
      <th>一出戏</th>
      <th>一刀</th>
      <th>一刀两断</th>
      <th>一分</th>
      <th>一分一秒</th>
      <th>一分钟</th>
      <th>一切都是</th>
      <th>一别</th>
      <th>一到</th>
      <th>一刹</th>
      <th>...</th>
      <th>黄土</th>
      <th>黄昏</th>
      <th>黄沙</th>
      <th>黄河</th>
      <th>黄算</th>
      <th>黄色</th>
      <th>黄金</th>
      <th>黎明</th>
      <th>黎明前</th>
      <th>黏</th>
      <th>黑</th>
      <th>黑发</th>
      <th>黑夜</th>
      <th>黑暗</th>
      <th>黑洞</th>
      <th>黑白</th>
      <th>黑眼圈</th>
      <th>黑色</th>
      <th>默契</th>
      <th>默念</th>
      <th>默然</th>
      <th>默许</th>
      <th>默默</th>
      <th>默默地</th>
      <th>黯淡</th>
      <th>黯然</th>
      <th>鼓</th>
      <th>鼓励</th>
      <th>鼓和声</th>
      <th>鼓声</th>
      <th>鼓手</th>
      <th>鼓掌</th>
      <th>鼓楼</th>
      <th>鼓舞</th>
      <th>鼓起勇气</th>
      <th>鼻子</th>
      <th>齐</th>
      <th>齐来</th>
      <th>齐齐</th>
      <th>龙</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.103094</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.032956</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.178812</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.05861</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>35916</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>35917</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.304936</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>35918</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.239766</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.099697</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>35919</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>35920</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
<p>35921 rows × 10000 columns</p>
</div>




```python
data[data['labels']==11].lyrics
```




    14       爸词曲我听说通常在战争后就会换来和平为什么看到我的爸爸一直打我妈妈就因为喝醉酒我真的看不下去...
    41       打架舞词曲编曲制作人想跟我干架没打听好吗我的拳头可是毫无牵挂拉筋一字马再送你电影里面的如来神...
    109      爸词曲听说通常在战争后就会换来和平为什么看到我的爸爸一直打我妈妈就因为喝醉酒他就能拿我妈出气...
    2347     鲁冰花词曲编曲啊啊夜夜想起妈妈的话闪闪的泪光鲁冰花天上的星星不说话地上的娃娃想妈妈天上的眼睛...
    2880     永远是个小娃娃词曲妈妈有一个好娃娃娃娃有一个好妈妈树发芽娃娃已长大哦哦可知道我害怕哦哦外面的...
    3156     一间房一家亲明明想安定一加一等于三明明很清醒很清醒一举起一双手明明想裸泳一张开一张嘴明明很干...
    3403     推动摇篮的手词曲那些很小很小的时候很多事情其实我不是那么的记得还好有人一边回忆一边告诉我就算...
    3895     还是跟妈妈说西施槟榔高速公路上正点女王全面开放从来没有想过自己这么狂妄月色之下机车之上你在凝...
    4341     马戏团词曲傍晚了烛光皮影戏是真还是假今晚是神秘剧团期待的首映人们坐立不安孩子们手里紧紧抓住棉...
    4485     孤儿泪一滴泪有谁知道眼里的泪水一滴泪有谁知道天冷了谁来管我树上的小鸟也有妈妈来陪除了叹息和流...
    4914     妈妈请你也保重词曲编曲若想起故乡目屎就流落来免挂意请你放心我的阿母虽然是孤单一个虽然是孤单一...
    5825     回家对着你妈妈词曲夜在笑好轻挑你的眼穿梭飘谁又是你目标若是我我不会心跳你愿吗为何仍未说话如想...
    6500     小宝宝词曲小宝宝快将出世期待那天你叫我爸爸像我吗更像妈妈吗我想亲一下给你听莫扎特从没有想你变...
    6822     妈妈说词曲妈妈说也别太执着不要执着妈妈说也要学会知足妈妈的有时候请不有时候辛苦有时候停步这就...
    7830     牡丹忧词曲编曲娘生儿连心肉儿行千里母担忧儿想娘亲难叩首娘想儿来泪双流雪团圆声此起彼落她凭一件...
    8430     白兔儿乖乖词曲白兔儿乖乖不开不开我不开妈妈没回来白兔儿乖乖不开不开我不开妈妈没回来妈妈不在大...
    8770     她的妈妈不爱我词曲今天天气好好好得让人脸发烧达令达令昨天晚上真的好想你嘿电话里面约好了就老地...
    8804     第一次词曲望着你慢慢熟睡去的你这一种感觉永远都不会死逐渐地学会心跳这么短短的数秒仿佛太短我终...
    9758     不要太乖词曲不要太乖读爱情在喜怒哀乐后得到了就像千年的秘笈学很久的大招睡了一觉隔天醒来突然我...
    9924     妈妈词曲妈妈世界如此大岂没我容身的地方我看着我深爱的人血在流淌我看着无辜的孩子失去了家妈妈当...
    11469    快乐的甘蔗人词曲编曲要安怎对妳说出心里话三更半夜咿咿啊啊袂等你大汉煞不记得一个人袂大汉我的妈...
    11472    印尼甘蔗人词曲要安怎对你说出心里话三更半夜咱家阿猫仔在灰咿咿啊啊又想要吃奶奶要等你大汉可能还...
    11497    妈妈十块词曲编曲我要跟妈妈讨十块来去找朋友出来逛街虽然我只有十块银但是我感觉真满足我朋友大牛...
    12077    让你妈妈都惭愧词曲编曲制作人录音室录音师混音室混音师蜘蛛人你攻击别人妈妈真的太伤人你这张臭嘴...
    12095    让你妈妈词曲够呛吧气氛忍不住一触即发怕了吧有人的关节想要说话疯狂的欲望不要忍耐秀不分对象一家...
    12173    让你妈妈一下词曲让你妈妈一下够呛吧气氛忍不住一触即发怕了吧有人的关节想要说话疯狂的欲望不要忍...
    13765    妈妈宝贝词曲青青的草地蓝蓝天多美丽的世界大手拉小手带我走我是妈妈的宝贝我一天天长大你一天天老...
    13959    白老鼠词曲编曲制作儿童和声小老鼠偷油吃叫妈妈他叽哩咕噜滚叽哩咕噜滚小老鼠偷油吃叫妈妈他叽哩咕...
    14076    古调流浪记词曲我就这样告别山下的家我实在不愿轻易让眼泪流下我以为我并不差我就这样自己照顾自己...
    14480    很想当妈妈词曲用了过份臂力双手揽紧你用了过份努力不松一口气如愿永远日夜一起被你摄入镜内画面多...
    14483    守望天使作词作曲空荡的街道不能回的家弟弟隔着玻璃喊妈妈孤单的房间不敢关的灯姐姐对着镜头画她们...
    16133    爸爸妈妈词曲恨不得快点天下父母都这般期盼着我们啊我们已健康快来祈祷父母长命百岁啊请记得要常回...
    16374    妈妈我想你词曲我第一次睁开眼睛看见的是你我第一次哭泣为我擦干的是你我第一次跌倒时搀扶的是你我...
    16486    妹妹要快乐词曲一个人在公园的大树下她看着别人妈妈带着宝贝回家背起了脏脏的布娃娃她小小声的问谁...
    16603    火柴天堂词曲走在寒冷下雪的夜空卖着火柴温饱我的梦一步步冰冻人情寒冷冰冻我的手一包火柴燃烧我的...
    16632    小人物向前冲词曲生活中每张脸孔喜怒哀乐看不懂不羡慕谁的成功天生我材必有用人生有始也有终别把得...
    16785    伟大的妈妈妈妈一家几口艰辛不顾妈妈管教倍操心朝晚尽心力爱护照顾爱女不计辛劳节妇斗志多敬慕妈妈...
    18733    推动摇篮的手词曲那些很小很小的时候很多事情其实还好有人一边回忆一边告诉我就算不太真实也很快乐...
    18737    妈妈我不要戴眼镜词曲妈妈我不要戴眼镜这个世界没有道理好的坏的谁分的清不如全部都忘记所有的专家...
    19244    小棉袄词曲编曲制作人清晨才真正的感觉到冬天到来花有一些忧伤怀念着童年的灿烂雪仿佛看到妈妈坐在...
    19577    快乐的甘蔗人词曲要安怎对你说出心里话三更半夜咿咿啊啊要等你大汉煞不记得一个人要大汉妈妈你们家...
    19684    我的词曲编曲第一次见你是在一个类似电视的屏幕里哦第二次见你你好像是在对我笑眯眯哦哦我的小心脏...
    20358    黑色信封词曲一天晚上我的一个朋友悄悄的来看我他的眼睛像外面的月亮是忧郁的他抓起我桌上的那个苹...
    20374    听妈妈的话词曲小朋友为什么我却在学画画别人在玩游戏我却靠在墙壁背我的我说我要一台大大的飞机但...
    20389    这个世界会好吗妈妈他们抛弃了我像歌唱一样抛弃了我妈妈我是多么爱你当我歌唱的时候我爱你只是那些...
    20640    不要告诉妈妈词曲嘿别忘了明天我们老地方见嘘这是我们的秘密请你不要告诉你妈妈明天我们老地方见吧...
    21167    听妈妈的话词曲编曲小朋友为什么我却在学画画别人在玩游戏我却靠在墙壁背我的我说我要一台大大的飞...
    22510    声音制作声音设计混录师拟音师制片统筹声音编辑配音导演小男孩小男孩妈妈童声演唱其他参配人员胡哲韵监制
    22514       声音制作声音设计混录师拟音师制片统筹声音编辑配音导演小男孩小男孩妈妈童声演唱其他参配人员监制
    22907    不能乱爱妈妈呀爸爸呀放去哪里了我的那个斗笠妈妈呀爸爸呀放去哪里了我的那个手帕你要那个做什么用...
    23673    好小孩的日记词曲昨天小毛脸上有一颗痘痘今天全身都是耶老师说种豆得豆长水痘牙齿痛妈妈说要听话如...
    24086    妈妈的笔记本词曲心情好好约你出来一起排队买一杯珍奶不小心把全部都喝完你说你妈妈叫你回家从来没...
    24401    吉祥三宝词曲爸爸哎太阳出来月亮回家了吗对啦星星出来太阳去哪里啦在天上我怎么找也找不到它它回家...
    24974    妈妈恰意的仔婿女儿妈妈金金惜慢慢宬无简单宬大汉家已青春伊讲女人眼睛得爱擘乎金妈妈心内恰意孖婿...
    25579    卡通人生词曲不要白白浪费一生不让时间把你生吞好好过你的卡通人生好好过你的卡通人生妈妈说有棒棒...
    26374    给爱惜的人词曲一千分钟工作时间栖身于这不足六呎七吋的工作间收音机都听到腐化想到一晚风里共某位...
    26534    妈妈要我出嫁歌词选自白俄罗斯民歌翻译者曲编曲音乐总监监制混音贝斯和声设计吉他吉他鼓手和声演唱...
    26832    词曲小时候我总是觉得妈妈罗里罗嗦严紧的看管不容许有半点犯错每当我滑个手机或打个游戏他总是觉得...
    27196    管他啦词曲我的爸爸叫我不要说说他昨天晚上在喝酒今天早上妈妈来问我我也不知道该怎么说我的爸爸叫...
    27701    一碗粥撰文张艾嘉从前有一个小男孩跟一个小女孩说如果我只有一碗粥一半我会给我的妈妈另一半我就会...
    27741    妈妈词曲妈妈妈妈我多想分给你一些我的力量我多想给你一颗轻松的心脏妈妈妈妈看到你的孩子在清晨难...
    27753    背影词曲我们正需要一场瓢泼的雨来冲洗这旧日的不喜鸟儿都飞到房檐下的新巢淋湿的身影要归去哪里昨...
    28266    前一段时间热播的都挺好每个人似乎都能从剧中的角色或多或少看到自己的影子或是联系到他人这部剧凸...
    28389    树子词曲高挂在树头上微胖显露枝枒膨胀膨胀微微颤抖的模样藏的不是害怕去吧去吧自以为的坚强却摔得...
    28788    红色康乃馨词曲摘下一朵红色的康乃馨轻插在胸前的口袋想起妈妈的叮咛心中充满无限的情怀十月怀胎没...
    29368    词曲神力女超人神力女超人无条件的爱无私的奉献穿上围裙战袍戴上手套柴米油盐酱醋茶酸甜苦辣一把抓...
    30715    爸爸妈妈词曲爸爸要争论昨日是谁错妈妈她心里一把火乖乖已长大莫问是谁错只想你一句讲清楚过渡期里...
    30795    逃亡列车亲爱的爸妈每次你们战争我多想有一次列车带我逃离这个地方看到别的孩子和父母嬉笑我羡慕得...
    31149    妈妈请你轻松吓词曲要我吃饱要我穿得暖叫我去街过马路小心点重复地说重复地说啰嗦也很温暖有餸有汤...
    31567    会相思的瓜词曲制作人编曲混音录音我家的篱笆墙上开了两朵花一朵是粉色的一朵是红的妈妈呀告诉我你...
    31635    妈妈的花环词曲妈妈为我编织了一个花环芬芳的花朵用绿叶来相串妈妈为我编织了一个花环心情像花朵永...
    31672    大武山美丽的妈妈词曲哎呀山里的歌声是那么的美丽哎呀唱呀用力的唱山谷里的歌声你是带不走的歌声是...
    32293    妈妈与我词曲响个不停的闹钟餐桌上面的早点是我和您早晨的序曲妈妈的手心里有忙碌快乐公车拥挤的人...
    32404    王雪红吃的菜词曲编曲有一天受邀到大老板家吃饭期待着今天的晚餐摆满着米其林般的灿烂看着那一道又...
    32414    常回家看看词曲找点空闲找点时间领着孩子常回家看看带上笑容陪同爱人常回家看看妈妈准备了一些唠叨...
    32609    外公的口琴词曲妈妈常说过去的事情家门外的小桥和小溪我问她有没有怀念的声音她说我的爸爸有个铜色...
    32692    我的主题曲词曲编曲妈妈说弹钢琴要气质的旋律但我怎么弹半天都是卡通主题曲爸爸要听古典但我还是喜...
    32702    猫的发际线词曲编曲吉他键盘合音编写录音师录音工作室混音师母带妈妈的皮沙发被我抓花爸爸的肉胳膊...
    32713    回家吃饭词曲编曲制作人吉他贝斯鼓和声录音师录音工作室混音师混音工作室母带母带工作室那天你把书...
    32952    天之大词曲编曲妈妈静静的静静淌在血里的牵挂妈妈我一生爱的襁褓有你晒过的衣服味道妈妈有了你离别...
    32976    泥娃娃词曲泥娃娃泥娃娃一个泥娃娃她有那鼻子也有那眉毛眼睛不会眨泥娃娃泥娃娃一个泥娃娃她有那鼻...
    33073    像个孩子为了出生而拼着像个妈妈为了家而忍着像个疯子为了疯而乐着像个为了钱活着她说得对她说得对...
    33320    朵朵花词曲编曲制作人和声录音师音频编辑混音母带监制音乐制作音乐营销有一个地方它是我的家山下河...
    33481    妈妈的便当菜当我年少时候我的妈妈对我说每个人都会有一颗星高挂在幽暗的苍穹之中当我懂事了以后我...
    33895    牌坊词曲你妈妈有个牌坊她说欲望需要遮挡你爸爸他又怎样他的愚昧虚伪成双本来你有你的思想可他们叫...
    33924    妈妈我要吃肉词曲我爬上了母猪山母猪山上有我的晚餐爬上了母猪山母猪山上有我的晚餐妈妈我要吃肉妈...
    34134    那一天妈妈问我童年最难忘的是什么在朦胧的记忆中难忘那小小的摇车它摇着日月它摇着妈妈无字的歌童...
    34137    妈妈教我一支歌词曲妈妈教我一支歌没有共产党就没有新中国这支歌从妈妈心头飞出这支歌伴随她走遍祖...
    34217    狗牙的故事词曲制作人录音工程师混音工程师一个老阿妈是非常虔诚的佛教徒因为她一直在国外她知道伟...
    34301    北京画报词曲曾经你听我打电话告诉妈妈说很快就有汽车和房子在安慰家人不是嘛在令人烦的迷津里她足...
    34651    一生报答词曲今天是感恩的日子感恩您把我带到人间给我温暖的家您用心血哺育我成长生育我该如何报答...
    34728    妈妈我想对您说话到嘴边又咽下妈妈我想对您笑眼里却点点泪花噢妈妈您的黑发泛起了霜花您的脸颊印着...
    34884    和平之城词曲静静地泥泞的城市里面脸还微微地疼痛着妈妈为我缝补的脸空空地星子还完好的天边手还微...
    34922    妈妈词曲原曲我停在校门口新修的那条便利店街道上人来人往和我童年看到的都不一样门卫叔叔不笑了他...
    34963    第四季黑莓灌木丛佩奇和她的家人在猪爷爷和猪奶奶的家里今天我要来做一些苹果黑莓的酥皮点心太好了...
    34983    家庭会议词曲三十岁了我还是一事无成有点辜负你们的期待三十岁了我还是没有女友只怪现在的人都太现...
    35324    带着妈妈的祝福上路词曲都说儿行千里母担忧今天才有了新的感受都说儿是娘的心头肉这才懂得临别时的...
    35342    妈妈的微笑词曲你像和煦的春风吹开柳梢跳动的鹅黄你像润物的细雨翠绿生机勃发的土壤翠绿生机勃发的...
    35682    妈妈我想回家妈妈我想要回家我不愿再流浪一下妈妈我想要回家我不愿再过这种混子生活妈妈我想要回家...
    35685    妈妈说的那些话词曲呢喃回响在耳旁枝头鸟儿在歌唱那年离家的时候似乎茫然沮丧一件简单的行囊背着遥...
    35724    毛主席像章词曲编曲我戴上毛主席像章走在街上我的心中埋藏着一个美丽的愿望一个五六岁的小女孩把我...
    35727    今天是你的生日词曲今天是你的生日妈妈我很想你想起年幼在你温暖的臂弯里直到有一天我长成一张青春...
    35733    今天是你的生日词曲今天是你的生日妈妈我很想你想起年幼在你温暖的臂弯里有一天我会长成一张青春的...
    35742    小时候妈妈对我讲大海就是我故乡海边出生海里成长大海啊大海是我生活的地方海风吹海浪涌随我飘流四...
    35785    中国龙小姐小姐亲亲我用嘴治好我的结巴先生先生啐啐我用痰拉拢我的表情农民农民疼疼我用粪种好我的...
    35891    哺育你总是在想着我们我们我们抬起头能看到你嘴里美味的虫子一万只小鸟一直在成长盘旋枝头不确定要...
    Name: lyrics, dtype: object




```python
# next steps: 
# (1) some words need to be removed, such as music terminologies.
# then group songs into the major categories that were obvious. 
# 

# POSTIVE sentiments:
#
# NEGATIVE sentiments:
# grief, fear, longing, 
# NEUTRAL sentiments:
# family / growing up, romance, moon/ stars/night, friendship, 

# then run the model again on each of these sub categories to further divide.
# but first what happens if run the model for only two groups?
# or even 4 groups?
# also would i get better results if i reduced the max_df to something much smaller (currently at 30%)

```


```python
# update stop_words

additional_words_to_remove = {'录音室','处理','后期', '混音', '带','工程师','母带','师母','录音师', '声', '工程','编写', '录音','师', '助理', '弦乐', '吉他', '贝斯','配唱',
                              '鼓','混音', '录音', '弦乐','吉他', '声', '音乐', '贝斯','录音师', '母带', '编写', '人声', '录音室', '鼓', '工程师', '监制', '录音棚',
                              '师', '发行', '配唱', '带','唱', '一首歌', '听', '情歌', '歌', '首歌', '歌声', '出', '一首', '一起', '歌唱','歌', '一首', '唱', '写', '听', '情歌', '歌词', '没有', '首歌', '旋律', '一首歌', '快乐', '完', '这首', '曲'}


```


```python
updated_stopwords_3 = updated_stopwords.copy()
updated_stopwords_3.update(additional_words_to_remove)
```


```python
print(len(updated_stopwords_3))
print(len(updated_stopwords))
```

    843
    843


2 Clusters Only


```python
# splitting into just two categories didn't yield any interpretable resuls
from numpy import unique
from numpy import where
from sklearn.datasets import make_classification
from sklearn.cluster import KMeans
from matplotlib import pyplot
# define dataset

# fit the tvec on the 35k rows of lyrics, will take include the 10k top features in terms of scores. 
# Higher scores go to words that have more disciminating power. I.e. those that appear frequently in some documents,
# but occur relatively less freqently overall. 
tvec = TfidfVectorizer(stop_words=updated_stopwords_3,max_df=0.6,max_features=10000,tokenizer=man_token_jie, norm='l2')
tvec.fit(lyrics)

# as we fit on all lyrics, and we are transoforming the 35k lyric rows into a dataframe, all 10k features will occur
# at least once. 
X_transformed = tvec.transform(lyrics)


# Consider X_transform each row is a vector, pointing to a position in 10k dimensional space.KMeans looks at these 
# points in 10k dimensional space and finds the 60 best centroids. 
from sklearn.cluster import KMeans
modelkmeans = KMeans(n_clusters=2, init='k-means++', n_init=100)
modelkmeans.fit(X_transformed)

# update the main dataframe so that labels can be seen
data['labels_2_splits'] = modelkmeans.labels_
# for each of the 60 labels, look at the words that were given the highest score in terms of their ability to distinguish

# from each of the other 2 labels. 
topics2 = {}
for i in range(2):
    topics2[i] = pd.DataFrame(tvec.transform(data[data['labels']==i].lyrics).toarray(), columns =tvec.get_feature_names()).sum().sort_values(ascending=False)[:50]
    
topics2    
    
```


```python
topics2
    
```




    {0: 永远    105.737591
     爱      29.236246
     不会     13.236363
     我会      8.568042
     忘记      7.600577
     不变      7.127243
     世界      7.074518
     时间      6.447390
     身边      6.385756
     改变      6.263312
     不再      6.224140
     生命      5.952343
     心       5.608687
     愿       5.600468
     感觉      5.533911
     我俩      5.461646
     美丽      5.413614
     失去      5.402633
     怀念      5.366989
     也许      5.180393
     知道      5.124179
     需要      5.070652
     梦       4.997212
     走       4.991050
     不能      4.900791
     不够      4.793236
     一直      4.664956
     心里      4.664622
     唯一      4.439465
     总是      4.372947
     过去      4.371056
     思念      4.288776
     爱着      4.279149
     明白      4.233655
     等待      4.182706
     变       4.162376
     爱情      4.116675
     忘       4.097349
     珍惜      4.067918
     未来      4.021752
     脸       4.021597
     不要      3.970891
     孤单      3.941420
     寂寞      3.883376
     一天      3.874287
     回忆      3.865685
     没       3.820393
     曾经      3.810800
     遥远      3.801887
     真的      3.800754
     dtype: float64,
     1: 再见     75.034489
     爱       8.131853
     脸       5.807741
     一天      5.187082
     相见      4.836003
     想念      4.764223
     一面      4.724080
     也许      4.652974
     时间      4.501329
     永远      4.442351
     离别      4.397146
     走       4.245773
     告别      4.049240
     思念      3.995093
     最后      3.989844
     眼泪      3.769411
     回忆      3.687805
     不要      3.625825
     一点      3.600980
     来不及     3.267590
     怀念      3.085309
     不能      3.056160
     不再      3.035461
     离开      2.936442
     从前      2.924713
     我会      2.919569
     不会      2.881693
     没       2.842105
     那天      2.767899
     眷恋      2.718044
     不想      2.713155
     明天      2.656122
     心       2.616386
     里面      2.597291
     忘       2.580796
     改变      2.577073
     一次      2.572062
     一眼      2.516135
     发现      2.513309
     远       2.512169
     世界      2.492284
     就让      2.466636
     朋友      2.466433
     是否      2.451581
     以后      2.448582
     过去      2.384240
     一声      2.375039
     已经      2.348202
     遥远      2.319712
     今天      2.299780
     dtype: float64}




```python
# Try 60 again but with max_df set at 5%
from numpy import unique
from numpy import where
from sklearn.datasets import make_classification
from sklearn.cluster import KMeans
from matplotlib import pyplot
# define dataset

# fit the tvec on the 35k rows of lyrics, will take include the 10k top features in terms of scores. 
# Higher scores go to words that have more disciminating power. I.e. those that appear frequently in some documents,
# but occur relatively less freqently overall. 
tvec = TfidfVectorizer(stop_words=updated_stopwords_3,max_df=0.05,max_features=10000,tokenizer=man_token_jie, norm='l2')
tvec.fit(lyrics)

# as we fit on all lyrics, and we are transoforming the 35k lyric rows into a dataframe, all 10k features will occur
# at least once. 
X_transformed = tvec.transform(lyrics)


# Consider X_transform each row is a vector, pointing to a position in 10k dimensional space.KMeans looks at these 
# points in 10k dimensional space and finds the 60 best centroids. 
from sklearn.cluster import KMeans
modelkmeans = KMeans(n_clusters=60, init='k-means++', n_init=100)
modelkmeans.fit(X_transformed)

# update the main dataframe so that labels can be seen
data['labels_60_splits_5%maxdf'] = modelkmeans.labels_
# for each of the 60 labels, look at the words that were given the highest score in terms of their ability to distinguish

# from each of the other 2 labels. 
topics3 = {}
for i in range(2):
    topics3[i] = pd.DataFrame(tvec.transform(data[data['labels']==i].lyrics).toarray(), columns =tvec.get_feature_names()).sum().sort_values(ascending=False)[:50]

```


```python
# from each of the other 2 labels. 
topics3 = {}
for i in range(60):
    topics3[i] = pd.DataFrame(tvec.transform(data[data['labels']==i].lyrics).toarray(), columns =tvec.get_feature_names()).sum().sort_values(ascending=False)[:50]
    
```


```python
topics3
```




    {0: 不变      7.641444
     改变      6.934038
     我俩      5.959105
     愿       5.927213
     怀念      5.845134
     不够      5.286233
     唯一      4.999453
     爱着      4.710444
     思念      4.539008
     变       4.402551
     珍惜      4.324691
     孤单      4.249079
     脸       4.208260
     深深      4.141939
     遥远      4.102088
     看见      4.014079
     这份      4.013084
     愿意      3.987224
     拥有      3.982815
     誓言      3.907941
     放弃      3.869742
     轻轻      3.659532
     身旁      3.633288
     温暖      3.564693
     消失      3.540690
     以后      3.536539
     记忆      3.509409
     回头      3.444329
     熟悉      3.379152
     相爱      3.358661
     仿佛      3.341817
     从前      3.282475
     感情      3.270246
     雨       3.164541
     告诉      3.159434
     爱上你     3.156424
     泪       3.120291
     变成      3.081297
     吻       3.039956
     情       3.021181
     太阳      3.014440
     讲       2.989975
     我要      2.940781
     在我心中    2.934544
     昨天      2.929040
     祝福      2.923087
     就让      2.915998
     风雨      2.905585
     感动      2.904022
     明天      2.903559
     dtype: float64,
     1: 再见     78.114668
     脸       6.108197
     相见      5.014206
     想念      4.995720
     一面      4.871699
     离别      4.553531
     思念      4.249403
     告别      4.185548
     来不及     3.363922
     怀念      3.347428
     从前      3.055425
     那天      2.856245
     眷恋      2.805735
     明天      2.788580
     里面      2.723983
     改变      2.697511
     就让      2.641543
     远       2.620567
     发现      2.618326
     一眼      2.605261
     以后      2.581790
     朋友      2.553730
     一声      2.478114
     遥远      2.430703
     微笑      2.373732
     抱歉      2.333454
     昨天      2.319586
     夏天      2.292531
     不见      2.278475
     一定      2.177933
     原来      2.134109
     变成      2.129706
     回头      2.055631
     看不见     2.022843
     遗憾      1.987569
     勇敢      1.975762
     一遍      1.957484
     放       1.947369
     想起      1.943075
     不说      1.927133
     难       1.910357
     晚安      1.890555
     要说      1.885459
     见       1.878786
     之前      1.871719
     那年      1.860146
     双眼      1.834161
     好好      1.808590
     实现      1.803667
     只能      1.768495
     dtype: float64,
     2: 是不是    6.874048
     告诉     6.059499
     期待     5.886506
     发现     5.473335
     变成     5.314917
     想念     5.108907
     能够     5.108578
     回来     5.035115
     昨天     4.841656
     当时     4.745096
     想起     4.732799
     从前     4.721308
     面对     4.609626
     快      4.549661
     日子     4.425404
     应该     4.392740
     爱过     4.367727
     勇敢     4.351631
     努力     4.325843
     以后     4.276644
     无奈     4.219654
     可能     4.197555
     好像     4.188498
     人生     4.142647
     我要     4.115723
     梦想     4.052603
     不用     3.949909
     有没有    3.920877
     以前     3.893291
     原来     3.874354
     每天     3.871468
     看着     3.842074
     结束     3.801694
     到底     3.726754
     遗憾     3.682204
     不敢     3.626288
     看到     3.591619
     呼吸     3.580424
     拥有     3.541969
     伤害     3.541355
     变      3.513422
     放弃     3.501043
     改变     3.469922
     路      3.462842
     分手     3.399782
     变得     3.397864
     继续     3.393708
     坏      3.366094
     美好     3.337255
     吃      3.311568
     dtype: float64,
     3: 青春    30.411048
     花     29.678306
     吹     29.578789
     相思    29.566354
     姑娘    29.067141
     少年    28.040910
     一片    27.642567
     风     27.404631
     人间    26.065115
     雨     25.963395
     人生    25.944723
     不知    25.276300
     开     24.398030
     思念    24.086459
     愿     23.823810
     难     23.084174
     路     23.063983
     轻轻    22.999612
     情     22.798503
     恨     22.737518
     欲     22.555071
     天     21.325785
     今夜    21.003773
     我心    20.720973
     英雄    20.530473
     太阳    20.460406
     春天    20.146065
     岁月    19.930591
     春风    19.517709
     似     19.384990
     风雨    19.105050
     夜     18.882541
     拢     18.760369
     天涯    18.686150
     长     18.388009
     故乡    18.016872
     家     17.790565
     飘     17.560950
     吃     17.302890
     时光    17.211964
     伊     17.139331
     真     17.099770
     老     17.010344
     一朵    16.698484
     回来    16.670045
     月光    16.660595
     送     16.647845
     梦里    16.334987
     讲     16.113257
     风吹    16.007747
     dtype: float64,
     4: 未     75.164585
     便     61.875383
     令     55.615530
     似     52.499761
     讲     50.248453
     仍然    46.679686
     没法    44.931034
     得到    37.610720
     情人    35.658969
     原来    34.271224
     恋爱    34.165775
     愿     33.193650
     不必    33.023480
     不可    32.722663
     吻     30.914629
     知     30.389871
     难道    27.906824
     继续    26.462648
     难     26.070387
     几多    25.649274
     抱     24.529998
     从前    24.325151
     极     22.507801
     当天    21.533138
     无谓    21.415923
     明日    21.291057
     变     21.280916
     忘掉    21.055635
     期望    21.022954
     开心    20.627991
     我心    20.175222
     应该    20.146647
     竟     20.017559
     完全    19.790944
     找到    19.783236
     不知    19.464199
     死     19.265353
     真     19.111619
     清楚    19.108780
     话     18.648343
     总     18.474037
     终于    18.346868
     只得    18.168870
     无奈    18.119315
     明明    17.900907
     怀念    17.608073
     始终    17.533749
     天天    17.163760
     从未    16.659231
     这刻    16.609660
     dtype: float64,
     5: 告诉    9.927796
     害怕    6.678907
     难过    6.430132
     应该    5.720381
     回头    5.630726
     到底    5.602848
     我要    5.404333
     难道    5.224239
     就让    4.914751
     不必    4.690396
     变成    4.670352
     好好    4.633137
     千万    4.460344
     沉默    4.443367
     朋友    4.424330
     自由    4.379291
     分手    4.343005
     越     4.295612
     眼睛    4.286654
     想起    4.263397
     承诺    4.237194
     看见    4.176765
     拥有    4.158634
     拒绝    4.064952
     快     4.048592
     情人    4.044969
     后悔    3.927488
     流泪    3.842974
     脸     3.825576
     日子    3.781303
     看到    3.700568
     恨     3.666249
     继续    3.657221
     不用    3.622481
     说话    3.589665
     黑夜    3.561408
     放弃    3.560048
     错     3.558134
     看着    3.524798
     惹     3.497523
     听见    3.461063
     明天    3.461060
     哭泣    3.432379
     感情    3.419150
     付出    3.412401
     从来    3.342000
     吻     3.332793
     今晚    3.291960
     担心    3.278960
     只能    3.273349
     dtype: float64,
     6: 承诺    6.777191
     以后    6.630448
     沉默    6.400044
     孤独    5.077747
     思念    4.570721
     夜     4.540483
     孤单    4.475486
     告诉    4.411704
     每个    4.406310
     难过    4.261828
     能够    4.245200
     爱过    4.185189
     回头    3.958161
     想念    3.903050
     相爱    3.860512
     留下    3.703726
     后来    3.647121
     安慰    3.576575
     不够    3.575913
     夜晚    3.422408
     了解    3.400243
     越     3.363416
     错过    3.336518
     风     3.296284
     原来    3.252009
     折磨    3.200139
     泪     3.183090
     爱得    3.161536
     变成    3.143895
     总     3.142777
     害怕    3.093944
     星空    3.091283
     原谅    3.001527
     伤痛    2.973124
     面对    2.964995
     只能    2.957968
     独自    2.945982
     太多    2.935063
     习惯    2.921647
     遗忘    2.906761
     自由    2.874961
     回来    2.792598
     默默    2.777979
     失落    2.742952
     应该    2.728595
     学会    2.722517
     看着    2.712497
     特别    2.704201
     冷漠    2.700433
     朋友    2.693246
     dtype: float64,
     7: 自由     70.909707
     飞翔      4.131765
     感受      4.086298
     理由      3.912898
     放手      3.869525
     拥有      3.776041
     孤独      3.745217
     能够      3.686512
     沉默      3.585015
     借口      3.375523
     就让      3.181815
     阳光      3.176288
     风       3.075652
     享受      2.868732
     梦想      2.853627
     终于      2.840150
     越       2.826309
     原来      2.817839
     我要      2.737962
     渴望      2.720162
     孤单      2.655341
     靠近      2.639448
     得到      2.589212
     以后      2.589174
     选择      2.512880
     痛       2.508494
     双手      2.506223
     沉重      2.467474
     头       2.448258
     告诉      2.444145
     彩虹      2.410908
     难道      2.347613
     青春      2.340092
     节奏      2.320469
     伤口      2.306075
     属于      2.225866
     地球      2.159314
     不愿      2.152744
     解脱      2.147460
     带走      2.073434
     觉得      2.063449
     身体      2.052717
     伤害      2.014542
     看着      2.003373
     美好      1.984042
     天       1.974055
     应该      1.970213
     烦恼      1.962287
     从来不     1.961406
     人生      1.956280
     dtype: float64,
     8: 重要     6.589891
     微笑     6.397184
     骄傲     6.093872
     变成     5.897755
     明      5.832815
     看到     5.669080
     味道     5.608796
     告诉     5.500715
     我要     5.156478
     想念     4.785211
     一定     4.689302
     应该     4.406977
     感情     4.332397
     是不是    4.321666
     眼睛     4.224654
     风      3.976421
     不该     3.953354
     终于     3.861831
     早      3.859209
     烦恼     3.736363
     放弃     3.691315
     不用     3.639761
     得到     3.624614
     愿意     3.615942
     找到     3.596701
     朋友     3.581977
     感受     3.575760
     依靠     3.557881
     觉得     3.555762
     只能     3.545209
     站      3.542425
     安静     3.541461
     难过     3.536430
     拥有     3.512889
     看见     3.460824
     老      3.408138
     害怕     3.368631
     很多     3.357966
     一秒     3.337126
     声音     3.327269
     以后     3.279963
     每个     3.255532
     徘徊     3.243766
     了解     3.228891
     快      3.223044
     围绕     3.221677
     听到     3.190757
     就让     3.182829
     可能     3.169181
     不愿     3.157703
     dtype: float64,
     9: 告诉     5.658191
     放弃     4.918717
     依然     4.836231
     回到     4.594773
     只能     4.589861
     倔强     4.541321
     难过     4.478843
     回头     4.106883
     愿意     3.943605
     事      3.824960
     恨      3.817823
     是不是    3.650347
     停止     3.583431
     好好     3.506723
     悲伤     3.371695
     疯狂     3.361328
     梦想     3.359482
     爱过     3.313666
     害怕     3.217350
     我要     3.208562
     伤害     3.183216
     就让     3.137250
     能够     3.053572
     分开     3.053529
     真心     3.052233
     改变     3.044255
     赤裸裸    2.947710
     天堂     2.943620
     承受     2.925865
     付出     2.924167
     相爱     2.883768
     快      2.823043
     以后     2.795001
     梦里     2.738840
     面对     2.726493
     总      2.696670
     留下     2.629762
     继续     2.615834
     留      2.611219
     了解     2.610617
     不愿     2.606991
     泪      2.585333
     接受     2.541253
     坐在     2.526707
     或许     2.525293
     翅膀     2.523580
     拥有     2.521349
     原谅     2.513573
     模样     2.512383
     停留     2.461137
     dtype: float64,
     10: 我爱你     75.122185
     月亮代表     4.054582
     一句       4.008052
     来不及      3.148450
     有多深      3.048555
     呼吸       2.522136
     对不起      2.489137
     爱着       2.391081
     几分       2.362385
     吻        2.347476
     真        2.316411
     唯一       2.127570
     愿意       2.116763
     勇气       2.095266
     决定       1.997248
     原来       1.958151
     信仰       1.917655
     我要       1.901237
     放弃       1.899212
     距离       1.769897
     这是       1.767902
     讲        1.708723
     甜蜜       1.682837
     变        1.642163
     孤单       1.602740
     早已       1.520150
     秘密       1.495029
     一万年      1.490422
     一辈子      1.467141
     爱到       1.461082
     逃避       1.454547
     方式       1.445241
     勇敢       1.426938
     想一想      1.421339
     说声       1.390861
     是不是      1.389887
     不该       1.375561
     难道       1.354501
     告诉       1.351057
     思念       1.350159
     轻轻       1.347969
     如今       1.326320
     感情       1.315604
     脸        1.310970
     改变       1.294419
     恨        1.287312
     结局       1.281279
     答答       1.275291
     我用       1.253992
     大声       1.209528
     dtype: float64,
     11: 妈妈      41.663898
     爸爸       9.113960
     长大       2.465449
     回家       2.402740
     家        2.375018
     吃        1.751542
     告诉       1.733176
     娃娃       1.703270
     第一次      1.525101
     很小       1.497099
     大汉       1.484328
     声音       1.469747
     咿        1.410395
     真        1.286888
     想起       1.277427
     一下       1.277028
     人生       1.198314
     一定       1.179482
     宝贝       1.175806
     看到       1.173792
     爸爸妈妈     1.168317
     好好       1.158013
     地方       1.146991
     孩子       1.133153
     我要       1.107516
     童年       1.076178
     温暖       1.024843
     帮        0.983772
     双手       0.959225
     害怕       0.950476
     伟大       0.933957
     小小的      0.910769
     恰恰       0.910646
     肉        0.910249
     话        0.909474
     一朵       0.900934
     小时候      0.898931
     白发       0.880255
     妈        0.877187
     说话       0.874808
     报答       0.854460
     天上       0.841123
     教        0.837520
     从来       0.837471
     天使       0.832709
     亲爱       0.829438
     家庭       0.828093
     乖乖       0.823327
     年轻       0.821699
     菜        0.815063
     dtype: float64,
     12: 室母      9.313758
     人母      6.485295
     键盘      5.929316
     合声      5.741354
     工作室     5.681895
     企划      5.124898
     小提琴     5.034492
     钢琴      4.608535
     统筹      4.111330
     演唱      4.108757
     人和声     3.679356
     木吉他     3.637745
     后期制作    3.566966
     出品      3.563672
     大提琴     3.434683
     剪辑      3.056227
     电吉他     3.041351
     小号      2.912109
     鼓和声     2.709561
     室       2.544704
     音频编辑    2.416409
     公司      2.185616
     第二      2.184855
     合音      2.181224
     主唱      2.176685
     执行      2.157945
     作词      2.114068
     作曲      2.085891
     乐器      2.073159
     选择      2.071229
     越       1.961817
     存在      1.947272
     词       1.944250
     呼吸      1.917771
     相遇      1.882731
     美术设计    1.846699
     中提琴     1.821070
     期待      1.803842
     第一      1.799185
     协力      1.719610
     雨       1.686924
     笑容      1.675075
     渴望      1.672941
     秘密      1.669429
     值得      1.632226
     至少      1.620090
     编辑      1.600921
     合成器     1.595213
     不停      1.593257
     以后      1.587450
     dtype: float64,
     13: 一场    17.427332
     风      9.147020
     匆匆     7.915928
     红尘     7.457052
     就让     6.934551
     吹      6.561020
     拥有     6.279852
     泪      6.000674
     梦里     5.740938
     雨      5.654334
     黑夜     5.453551
     如今     5.318176
     不愿     5.315202
     醒来     5.215999
     轻轻     5.066471
     现实     5.055092
     人生     5.045085
     青春     5.043440
     城市     5.032756
     往事     4.912178
     相思     4.854299
     时光     4.800854
     笑容     4.716974
     追      4.681374
     路      4.677330
     痛      4.656364
     恨      4.641260
     风雨     4.591952
     梦醒     4.399213
     夜      4.384859
     夜里     4.378355
     变成     4.345621
     人间     4.313885
     沉默     4.187951
     能够     4.186281
     空      4.183271
     昨夜     4.183151
     孤单     4.141172
     岁月     4.105267
     情      4.036890
     原来     4.024258
     告诉     4.015370
     午夜     3.983789
     醒      3.940029
     走过     3.828194
     远方     3.775241
     难      3.767792
     留在     3.751560
     思念     3.707717
     不敢     3.694985
     dtype: float64,
     14: 键盘      21.007652
     版权      20.760859
     统筹      20.271551
     出品      19.875384
     主唱      19.584449
     小提琴     17.506036
     大提琴     16.542051
     总监      15.983032
     钢琴      14.921263
     编辑      14.531049
     合声      11.918293
     演唱      11.717301
     工作室     11.304832
     乐器      10.957513
     室       10.921809
     合成器     10.528064
     出品人     10.169343
     中提琴      9.547139
     营销       8.234200
     设计       8.159815
     鼓手       7.907624
     乐团       7.888237
     人和声      7.867059
     执行       7.821855
     电吉他      7.352505
     孤独       7.265938
     合唱       7.188780
     企划       7.067825
     公司       7.065913
     木吉他      6.994798
     打击乐      6.946934
     作曲       6.928218
     缩        6.748495
     平面       6.295097
     岁月       6.268173
     弦        6.204592
     音频编辑     6.110192
     归属       6.013521
     乐队       5.987192
     指导       5.982474
     声音       5.769943
     录制       5.722590
     混        5.609327
     一片       5.531530
     第二       5.508523
     美术设计     5.456403
     第一       5.415005
     愿        5.232359
     习惯       5.190499
     一场       5.068777
     dtype: float64,
     15: 回来     9.827187
     以后     8.322964
     存在     7.520947
     能够     6.121210
     习惯     5.606459
     不愿     5.357760
     伤害     5.350289
     期待     5.324812
     应该     5.245061
     之后     5.124177
     沉默     4.960530
     依赖     4.891780
     决定     4.745478
     告诉     4.559339
     我要     4.414981
     终于     4.395874
     转身     4.201224
     看着     4.198552
     愿意     4.178273
     依然     4.162273
     留下     4.139168
     回不来    4.115940
     回到     4.081079
     悲哀     4.073202
     选择     3.986177
     难过     3.909034
     孤单     3.823458
     雨      3.807572
     是不是    3.755235
     思念     3.752650
     昨天     3.750873
     躲      3.686104
     声音     3.682295
     遗憾     3.628767
     醒来     3.579960
     之前     3.570759
     留给     3.552047
     想起     3.529601
     孤独     3.504624
     无奈     3.501259
     面对     3.500099
     明天     3.492692
     相爱     3.479610
     越      3.478139
     夜      3.461742
     留下来    3.435007
     勇敢     3.404956
     放弃     3.355102
     错      3.290094
     坏      3.289831
     dtype: float64,
     16: 轻轻     5.621876
     唱歌     5.473353
     有没有    4.484564
     跳      3.617130
     变      3.367862
     人生     3.176964
     声音     3.132536
     一段     3.062637
     太阳     3.012102
     孤单     2.917966
     大声     2.817536
     日子     2.739174
     失恋     2.728918
     走过     2.716256
     流泪     2.683584
     继续     2.661457
     一曲     2.647135
     青春     2.598357
     这歌     2.590154
     悲伤     2.577350
     歌儿     2.575554
     时代     2.571328
     眼睛     2.552931
     身旁     2.461804
     轻轻地    2.417323
     愿意     2.395457
     喉咙     2.385444
     活      2.354456
     感动     2.346661
     每天     2.334600
     愿      2.295030
     起来     2.293832
     草原     2.228986
     梦想     2.226891
     演唱会    2.213256
     跟着     2.195056
     唱出     2.127156
     就让     2.114642
     每个     2.081677
     想念     2.081545
     里面     2.059208
     煞      2.027519
     我要     2.018348
     必须     2.001692
     一声     1.998622
     老      1.982492
     温暖     1.968248
     风      1.945089
     未      1.940040
     新      1.925779
     dtype: float64,
     17: 喔       90.195305
     耶        2.385252
     心情       1.954565
     越来越      1.945045
     呜呜       1.863596
     快        1.831322
     黑暗       1.540646
     存在       1.533160
     回家       1.531723
     女生       1.446494
     路        1.428649
     是不是      1.390493
     美好       1.382488
     起来       1.372725
     灌醉       1.361549
     拥有       1.360359
     到底       1.288050
     一颗       1.253064
     心跳       1.247959
     觉得       1.238660
     只能       1.234924
     看着       1.196507
     讲        1.191742
     呦        1.164778
     整个       1.091323
     站        1.065235
     买        1.058910
     看到       1.054125
     看见       1.046436
     眼睛       1.044994
     相遇       1.037781
     花朵       1.010056
     坏        0.992029
     乡间       0.991611
     奇怪       0.989052
     吻        0.968520
     合        0.967286
     理由       0.964906
     话        0.964133
     回到       0.963710
     远        0.940836
     溶化       0.936996
     有人       0.929232
     走向       0.929075
     千山万水     0.924150
     吃        0.924001
     机会       0.922179
     谢谢       0.922122
     太阳       0.917729
     男        0.917371
     dtype: float64,
     18: 男      115.364805
     女      105.890930
     合       32.778639
     男女      11.209395
     男合      10.455940
     女合       6.233540
     合女       4.687120
     难        3.747259
     相爱       2.982007
     未        2.919363
     愿意       2.767757
     原来       2.619079
     分手       2.577426
     一场       2.432745
     思念       2.412483
     变成       2.399022
     微笑       2.340578
     遇见       2.323333
     合合       2.321298
     男合女      2.298714
     合男       2.279662
     爱女       2.264266
     温暖       2.250920
     不够       2.233521
     从前       2.197356
     一句       2.174917
     浪漫       2.173354
     想念       2.171368
     甜蜜       2.068978
     好像       2.052228
     以后       1.995852
     每天       1.985083
     愿        1.966058
     心爱       1.954556
     真        1.907484
     美        1.878338
     男人       1.874395
     恋爱       1.867340
     悲伤       1.853696
     错过       1.851686
     真心       1.839988
     时光       1.828472
     妳        1.792464
     苦涩       1.784468
     变        1.764957
     天生       1.763949
     不必       1.731871
     我合       1.725130
     两个       1.718906
     难道       1.705862
     dtype: float64,
     19: 思念     3.458838
     诗      3.344528
     唱歌     3.003719
     想起     2.972336
     流行     2.884811
     每个     2.742546
     一条     2.678673
     山川     2.554294
     夏天     2.542209
     悲伤     2.526795
     属于     2.460024
     难过     2.453208
     舞      2.421072
     一刻     2.400463
     听到     2.314701
     轻轻     2.309924
     名字     2.244443
     想念     2.217579
     声音     2.181136
     到底     2.167948
     知      2.165106
     里面     2.162233
     简单     2.151757
     唱起     2.128129
     喜爱     2.087306
     欢乐     2.080546
     安静     2.078691
     不出     1.965805
     城市     1.950611
     岁月     1.938950
     感动     1.932272
     觉得     1.918602
     看着     1.917035
     一支     1.814200
     嘲笑     1.800856
     风      1.788733
     青春     1.786605
     河      1.741262
     这歌     1.728854
     快      1.708933
     花      1.694596
     愿      1.673964
     错      1.667663
     孤独     1.620881
     不了     1.611464
     创作     1.602654
     远方     1.600223
     越来越    1.592702
     那年     1.576032
     送给     1.546667
     dtype: float64,
     20: 愿     10.899906
     一世    10.116188
     未      7.437790
     人生     7.224267
     不必     6.734895
     伴      6.724378
     真      6.624015
     命运     6.411391
     几多     6.192202
     愿意     6.184110
     岁月     5.972369
     真心     5.796330
     始终     5.757819
     一刻     5.631224
     令      5.455816
     仿佛     5.314727
     终于     5.309118
     没法     5.285545
     前路     5.073178
     拥有     5.023736
     世间     5.014982
     注定     4.917030
     改变     4.895880
     匆匆     4.867883
     以后     4.858253
     孤单     4.812410
     天天     4.669115
     不可     4.632545
     讲      4.575406
     变      4.570552
     风雨     4.558584
     珍惜     4.554250
     仍然     4.397758
     似      4.346716
     便      4.251434
     找到     4.223531
     我心     4.197700
     从前     4.178919
     盼      4.116186
     可能     4.058981
     总      4.015331
     痛苦     3.926080
     路      3.886933
     不知     3.783217
     开心     3.634574
     今生     3.624743
     知      3.593374
     我愿     3.586412
     吻      3.581283
     一句     3.540972
     dtype: float64,
     21: 爱人      52.132531
     吻        2.316009
     青春       2.202365
     眼睛       2.187061
     欺骗       2.105273
     怀念       2.077575
     原来       2.033462
     猜        1.956012
     爱到       1.937496
     如今       1.924303
     错        1.858828
     生        1.840275
     恨        1.820659
     树枝       1.817416
     娜        1.806269
     堂堂       1.770521
     想起       1.767178
     肯定       1.675795
     人生       1.649945
     讲        1.633951
     应该       1.631107
     可不可以     1.581606
     我要       1.570304
     吹        1.549399
     亲爱       1.535467
     徘徊       1.509801
     男儿       1.501089
     心肝       1.479216
     红        1.469890
     我俩       1.448478
     变        1.427487
     找到       1.382009
     灵魂       1.352682
     情书       1.322697
     日子       1.302218
     倾城       1.298226
     继续       1.294392
     明日       1.288693
     前度       1.281887
     风        1.269058
     牵引       1.257522
     知        1.243743
     再见       1.241449
     往日       1.233945
     某人       1.233771
     微风       1.233366
     亲密       1.215356
     放        1.204966
     心情       1.201521
     森林       1.179735
     dtype: float64,
     22: 孤独     9.486353
     我要     9.267036
     满足     8.146108
     路      7.974292
     清楚     7.794431
     微笑     7.716774
     祝福     7.612839
     辛苦     7.548854
     一定     7.455773
     一步     6.523476
     结束     6.340326
     简单     5.674177
     害怕     5.651300
     能够     5.576759
     原来     5.515276
     就让     5.457423
     追逐     5.320833
     变成     5.296599
     越      5.226322
     存在     5.220879
     感动     5.026536
     以后     5.022498
     守护     5.003163
     温暖     4.964093
     美好     4.886324
     错误     4.858003
     付出     4.835415
     永恒     4.735229
     不愿     4.652576
     领悟     4.545251
     值得     4.491612
     学会     4.486544
     得到     4.331940
     两个     4.318815
     孤单     4.270594
     拥有     4.263537
     不在乎    4.207529
     勇敢     4.207125
     可能     4.194907
     童话     4.148639
     温度     4.038028
     遗憾     4.004723
     地图     4.002769
     愿      3.970920
     至少     3.924098
     平凡     3.914565
     爱是     3.901019
     放弃     3.897226
     在乎     3.890536
     天使     3.864325
     dtype: float64,
     23: 月亮      37.632463
     星星      36.905498
     天上       5.700132
     月光       5.225473
     今晚       4.963628
     夜空       4.081847
     一颗       4.064755
     眼睛       3.968784
     太阳       3.904418
     夜里       3.713342
     闪烁       3.364736
     远方       3.321191
     千千阙歌     3.129371
     路上       2.924203
     千千       2.904732
     晚星亮      2.816326
     夜晚       2.730757
     比不起      2.642326
     这宵       2.642326
     亮        2.551423
     星        2.527708
     画        2.437813
     死去       2.389106
     飘于       2.379718
     光芒       2.353064
     可使       2.338634
     白天       2.306612
     堆满       2.277722
     弯弯的      2.132288
     画上       2.125267
     见        2.081134
     思念       2.072235
     变成       2.039359
     欣赏       2.019099
     城市       1.998078
     脸        1.953278
     每个       1.938460
     我要       1.929337
     看着       1.929306
     晚上       1.876290
     夜        1.871918
     想起       1.860617
     温暖       1.860362
     圆        1.854146
     影子       1.840417
     黑夜       1.802308
     闪        1.772623
     轻轻       1.768577
     孤单       1.768541
     满天       1.753981
     dtype: float64,
     24: 一颗    21.207228
     眼睛     7.081534
     泪      7.020238
     夜      6.857010
     留下     6.799899
     温暖     6.772422
     思念     6.742399
     吻      6.724811
     今夜     6.697052
     想念     6.536074
     情      6.405062
     夜里     6.362514
     痛      6.361044
     看着     6.295464
     烦恼     6.194425
     拥有     6.169035
     风      6.138260
     梦里     6.084052
     孤单     6.009257
     一片     5.870718
     真      5.824605
     放弃     5.786683
     心情     5.708088
     我要     5.685085
     就让     5.680521
     我心     5.667717
     雨      5.578952
     伤      5.484968
     感情     5.399352
     哭泣     5.195571
     恨      5.134330
     呼吸     5.125634
     令      5.076204
     声音     5.007464
     继续     4.991697
     忘掉     4.909847
     飘      4.877573
     越      4.866549
     能够     4.830524
     黑夜     4.799871
     早已     4.787079
     灵魂     4.751547
     真心     4.686473
     每个     4.587374
     想起     4.550664
     跟着     4.512849
     花      4.497948
     看见     4.486160
     不知     4.434732
     孤独     4.432533
     dtype: float64,
     25: 伤心    64.716217
     难过     3.913750
     开心     3.520425
     泪      3.472065
     不必     3.465201
     流泪     3.062231
     真心     3.049495
     痛苦     3.025328
     不该     2.952710
     决定     2.897354
     哭泣     2.895222
     痴心     2.874633
     放弃     2.857643
     分手     2.854991
     雨      2.789608
     付出     2.709931
     一步     2.679831
     继续     2.668186
     愿意     2.646954
     城市     2.590061
     想起     2.561275
     回头     2.551163
     爱过     2.519551
     感情     2.493961
     心情     2.475761
     孤单     2.473722
     原来     2.464428
     留下     2.433862
     告诉     2.402413
     关心     2.361158
     离去     2.355136
     面对     2.335381
     在意     2.330999
     无情     2.326880
     悲伤     2.314074
     思念     2.306007
     记忆     2.275516
     一场     2.254667
     容易     2.242127
     伤痕     2.231793
     难免     2.217964
     伤      2.201841
     能够     2.152797
     泪水     2.125977
     早      2.103414
     愿      2.075062
     何必     2.074212
     每天     2.060112
     如今     2.047857
     情      2.039696
     dtype: float64,
     26: 唔     30.768240
     嘅     24.322636
     冇     15.610090
     咁     15.416907
     佢     14.947092
     系     14.105388
     话      8.721053
     乜      8.612739
     啲      6.815961
     咗      5.995700
     讲      5.913302
     揾      5.818635
     咪      5.718473
     哋      5.595685
     嘢      5.392795
     嚟      4.780197
     晒      4.535650
     睇      4.470650
     喺      4.369216
     俾      3.667473
     点解     3.572358
     咩      3.478091
     谂      3.455155
     左      3.309503
     继续     3.084812
     钱      2.955242
     惊      2.907420
     住      2.871694
     啱      2.773903
     够      2.752883
     乜嘢     2.665672
     钟意     2.622618
     鬼      2.570881
     今晚     2.388287
     屋企     2.315589
     一味     2.306358
     知      2.279139
     死      2.145550
     买      2.143958
     无人     1.885601
     埋      1.874087
     我系     1.841202
     班      1.836267
     我话     1.833757
     嗌      1.822931
     好似     1.794112
     鸡      1.776939
     开心     1.771924
     实在     1.754600
     瞓      1.730718
     dtype: float64,
     27: 悲伤    5.344471
     原来    5.025516
     拥有    4.778957
     终于    4.317518
     朋友    4.147693
     快     4.108285
     开心    3.923326
     情人    3.878686
     起来    3.851093
     可能    3.680980
     笑容    3.632675
     觉得    3.490609
     烦恼    3.448555
     难过    3.407052
     一刻    3.360700
     人生    3.327311
     容易    3.291772
     选择    3.284754
     发现    3.254318
     真正    3.222663
     得到    3.185895
     事     3.150241
     角落    3.147553
     微笑    3.072660
     属于    3.024775
     跟着    2.995375
     阳光    2.995090
     决定    2.952622
     开     2.939645
     值得    2.938172
     变成    2.919378
     找到    2.878599
     留在    2.810156
     灵魂    2.801462
     不够    2.789030
     风     2.782492
     难     2.770724
     看见    2.768176
     自由    2.739654
     知足    2.730856
     情绪    2.726362
     告诉    2.720788
     每个    2.719079
     路     2.684404
     感动    2.672805
     应该    2.660465
     至少    2.655370
     渴望    2.645531
     变得    2.622423
     吹     2.609177
     dtype: float64,
     28: 害怕    4.775025
     不必    4.672401
     应该    4.648814
     突然    4.615829
     难     4.002829
     愿     3.957113
     越     3.884085
     不敢    3.655910
     未     3.613364
     原谅    3.593569
     得到    3.242902
     伤     3.242092
     孤单    3.225339
     快     3.215483
     讲     3.187031
     放弃    3.051790
     担心    3.029204
     自由    2.969647
     人人    2.944893
     眼睛    2.851483
     看到    2.807675
     恋爱    2.807442
     理想    2.779084
     开心    2.698986
     人生    2.643101
     明明    2.628053
     原来    2.604842
     似     2.604566
     习惯    2.588892
     难道    2.558304
     伤害    2.542290
     以后    2.538807
     背弃    2.514299
     觉得    2.511684
     回头    2.465770
     事     2.441892
     我怕    2.423643
     见     2.414979
     情愿    2.414875
     跌倒    2.401582
     话     2.384658
     不安    2.357959
     心酸    2.352236
     笑话    2.338382
     拥有    2.324956
     决定    2.309630
     变     2.300838
     情     2.297775
     老     2.297054
     吻     2.284770
     dtype: float64,
     29: 我要     37.204036
     觉得     36.893377
     变成     36.292250
     快      33.905938
     每个     32.413526
     城市     32.044624
     告诉     31.726652
     一定     31.626291
     习惯     31.619424
     起来     30.642263
     看见     30.630397
     可能     30.061037
     不用     29.133381
     微笑     29.063748
     发现     27.897764
     到底     27.647172
     存在     27.631328
     完美     27.575338
     期待     26.789534
     原来     26.563902
     害怕     26.424747
     越      26.396829
     应该     25.963772
     能够     25.841943
     看着     25.824355
     眼睛     25.766229
     人生     25.695359
     呼吸     25.489448
     站      25.395514
     孤独     24.914581
     阳光     24.835949
     只能     24.617106
     眼神     24.333318
     拥有     24.283562
     之间     24.282247
     是不是    24.277179
     感情     24.262312
     面对     24.017687
     孤单     23.962584
     声音     23.721743
     突然     23.622282
     变得     23.466364
     脸      23.264643
     继续     23.259469
     一场     23.158633
     出现     23.064098
     看到     23.063021
     选择     22.817091
     灵魂     22.553463
     勇敢     22.488456
     dtype: float64,
     30: 有人    66.526539
     总      5.286115
     人生     2.928063
     新      2.639132
     能够     2.396576
     靠近     2.262721
     总会     2.232633
     奇怪     2.185982
     每个     2.066672
     关心     2.043602
     一定     2.028066
     夜里     1.958390
     安慰     1.934061
     存在     1.877108
     到底     1.862076
     好像     1.860976
     渴望     1.857985
     以后     1.856986
     面对     1.850194
     太美     1.803098
     看到     1.760527
     眼睛     1.748671
     应该     1.738652
     完美     1.724304
     滋味     1.721818
     换      1.703714
     明天     1.699418
     看见     1.696063
     孤独     1.675475
     现实     1.664610
     开      1.650314
     感情     1.645926
     啾      1.638961
     呼唤     1.636035
     不用     1.599479
     抱      1.596244
     路      1.596211
     越      1.571252
     受伤     1.570268
     结局     1.552816
     放下     1.551358
     敲敲     1.547889
     事      1.545493
     我要     1.542711
     一句     1.542537
     流浪     1.536609
     忙      1.533655
     门      1.524416
     当爱     1.520649
     角落     1.504845
     dtype: float64,
     31: 依然     7.512276
     愿意     6.842221
     回到     6.476182
     能够     6.239727
     告诉     5.986066
     眼睛     5.599305
     想念     5.469960
     想起     5.422424
     思念     5.051547
     不知     4.949761
     看见     4.861236
     从前     4.785597
     拥有     4.747757
     明天     4.681815
     雨      4.312437
     应该     4.188025
     看到     4.169531
     是不是    4.112544
     出现     3.934096
     泪      3.918799
     错过     3.894592
     只能     3.829657
     还会     3.826302
     相遇     3.768548
     或许     3.636060
     分手     3.635604
     改变     3.614348
     选择     3.604634
     流泪     3.576668
     以后     3.507846
     流浪     3.438539
     人生     3.411630
     珍惜     3.375406
     到底     3.358444
     梦里     3.302550
     心情     3.294703
     重新     3.259516
     泪水     3.219913
     孤单     3.133493
     记忆     3.114628
     远方     3.108203
     如今     3.051063
     今夜     3.042567
     日子     3.040471
     灵魂     3.019818
     那天     3.013978
     不敢     3.009774
     听见     3.008021
     遥远     2.947406
     沉默     2.929901
     dtype: float64,
     32: 告诉     7.898872
     感情     7.213961
     一场     6.579813
     变成     6.532782
     我要     5.920254
     放弃     5.832271
     孤独     5.706688
     原来     5.564014
     孤单     5.486500
     是不是    5.413795
     游戏     5.399077
     眼睛     5.289071
     选择     5.146030
     面对     5.027625
     愿意     4.959672
     何必     4.938277
     承诺     4.866450
     完美     4.823430
     微笑     4.794585
     总      4.754738
     掉      4.711050
     拥有     4.649043
     心情     4.639116
     遇见     4.586673
     痛      4.563706
     不愿     4.542197
     里面     4.475418
     好像     4.407739
     听见     4.400814
     浪漫     4.343671
     心碎     4.263625
     真心     4.243328
     快      4.224024
     青春     4.145860
     应该     4.142426
     抱      4.109553
     看见     4.108867
     每个     4.108539
     以后     4.098388
     唯一     4.090564
     越      4.083869
     恋爱     4.059473
     不必     4.042250
     难道     4.034780
     有点     4.011652
     沉默     4.010483
     爱过     3.982803
     觉得     3.951477
     能够     3.895477
     继续     3.891884
     dtype: float64,
     33: 昨天    12.110443
     当天     8.788685
     明天     6.109069
     回家     5.808485
     讲      5.035779
     当初     4.747173
     我心     4.575092
     似      4.260579
     从前     4.210650
     应该     4.194550
     竟      4.124145
     每天     4.077550
     日子     3.936631
     始终     3.773197
     不必     3.698108
     抱      3.681490
     可惜     3.655200
     终于     3.627169
     说话     3.470252
     真心     3.382284
     明日     3.269834
     失意     3.251085
     意义     3.246912
     变      3.220398
     知      3.193016
     难      3.129221
     天天     3.089379
     那天     3.058165
     以后     3.056502
     开心     3.016205
     高兴     3.001143
     有点     2.982804
     晚上     2.960070
     愿      2.915850
     改变     2.884409
     面对     2.865666
     看着     2.855933
     仍然     2.819731
     不可     2.816139
     睡      2.788715
     天      2.744954
     人生     2.732820
     没法     2.717578
     旧      2.635577
     话      2.623270
     总      2.528343
     见      2.520463
     你好     2.516944
     恨      2.504980
     愿意     2.504516
     dtype: float64,
     34: 从来    11.187473
     能够     8.603051
     眼睛     7.561636
     告诉     7.553814
     想念     7.453356
     放弃     7.366958
     原来     7.080121
     想起     6.892016
     孤独     6.888692
     人生     6.814948
     习惯     6.706164
     好像     6.435060
     决定     6.427920
     理由     6.392538
     付出     6.106951
     感情     6.096315
     难过     6.023821
     难道     6.004795
     事      5.979927
     愿意     5.978103
     呼吸     5.924337
     一场     5.818335
     结局     5.794687
     再也     5.612493
     关系     5.596092
     到底     5.571275
     回头     5.510371
     觉得     5.439033
     孤单     5.404452
     发现     5.375973
     一句     5.364322
     办法     5.332963
     以后     5.317186
     看着     5.309362
     日子     5.266797
     拥有     5.226314
     反正     5.206083
     声音     5.143005
     可能     5.045476
     变      4.971096
     感动     4.960114
     沉默     4.931300
     留下     4.886050
     看见     4.883419
     改变     4.876007
     真心     4.856196
     分手     4.832619
     简单     4.800880
     害怕     4.784092
     答案     4.759317
     dtype: float64,
     35: 明天    85.760199
     昨天     8.378661
     微笑     4.700644
     面对     4.583587
     改变     4.562790
     脸      4.316910
     我要     4.222909
     勇敢     3.931206
     醒来     3.714129
     依然     3.548619
     人生     3.381970
     青春     3.356987
     更好     3.300924
     梦想     3.162559
     从前     3.150500
     思念     3.146991
     黑夜     3.070115
     今夜     2.952916
     今晚     2.909752
     此刻     2.894014
     天      2.862308
     总会     2.832345
     留给     2.806114
     拥有     2.741050
     变      2.739830
     变成     2.738403
     太阳     2.737448
     害怕     2.725618
     简单     2.712817
     一定     2.712287
     告诉     2.687353
     眼睛     2.649651
     笑容     2.649043
     看见     2.647113
     终于     2.596745
     路      2.516846
     发现     2.491836
     再见     2.387566
     出现     2.382436
     能够     2.357285
     双眼     2.355909
     听见     2.315340
     决定     2.302400
     事      2.294530
     漫长     2.291142
     充满     2.282983
     美好     2.281233
     习惯     2.277682
     期待     2.265346
     轻轻     2.231823
     dtype: float64,
     36: 得到     5.435685
     找到     4.998402
     理由     4.640300
     越      4.461137
     孤单     4.356697
     讲      4.212843
     路      4.017444
     不必     3.942467
     再也     3.850869
     方向     3.840203
     爱过     3.535033
     地方     3.482703
     从前     3.453469
     回      3.304042
     终于     3.277525
     看到     3.241096
     到底     3.200045
     寻找     3.186548
     新      3.171639
     尽头     3.171584
     痛      3.090702
     害怕     3.090014
     眼睛     3.021781
     城市     3.018548
     可惜     2.955737
     难道     2.921885
     放弃     2.903200
     习惯     2.891576
     变得     2.866411
     假装     2.858754
     呼吸     2.851659
     知      2.828635
     觉得     2.792537
     看不到    2.791972
     见      2.775066
     拥有     2.753833
     想象     2.743730
     回头     2.709805
     有没有    2.704192
     我要     2.697569
     答案     2.666226
     快要     2.656577
     不了     2.645299
     看着     2.638203
     天      2.635248
     孤独     2.614795
     存在     2.587739
     出现     2.572464
     一句     2.566275
     老      2.522270
     dtype: float64,
     37: 身旁     5.835663
     看着     4.962973
     孤单     4.241171
     每个     4.056549
     温暖     3.952326
     青春     3.799780
     愿意     3.762123
     我要     3.749366
     感谢     3.734754
     陪你走    3.607626
     恋爱     3.475244
     天      3.388533
     心情     3.298826
     走过     3.291545
     继续     3.211456
     尽头     3.200871
     轻轻     3.177354
     好好     3.122419
     原来     3.063565
     发现     2.945489
     路上     2.913301
     睡      2.871207
     一辈子    2.857754
     度过     2.836735
     愿      2.788562
     孤独     2.767354
     拥有     2.744452
     陪伴     2.743407
     理由     2.735127
     以后     2.730978
     有时候    2.725995
     笑容     2.699794
     可能     2.677132
     雨      2.672419
     遇见     2.668974
     路      2.660128
     改变     2.615825
     微笑     2.589419
     留下来    2.585056
     看见     2.528662
     天涯     2.524408
     星光     2.517138
     朋友     2.513659
     牵      2.510231
     遗憾     2.470423
     承诺     2.447122
     人生     2.442212
     找到     2.437605
     属于     2.405311
     像是     2.391763
     dtype: float64,
     38: 女人     65.244576
     男人     20.507383
     感情      2.792701
     每个      2.791551
     真       2.780954
     花       2.737799
     坏       2.544666
     容易      2.431208
     付出      2.405023
     天真      2.331196
     难道      2.115199
     认真      2.112972
     青春      2.103875
     傻       2.099166
     在乎      1.997328
     不该      1.984886
     吻       1.983437
     可爱      1.911472
     流泪      1.900307
     女女      1.856820
     渴望      1.856239
     灵魂      1.769289
     选择      1.739660
     伤痕      1.734470
     告诉      1.698774
     受伤      1.678234
     小心      1.668766
     恋爱      1.659522
     伤心      1.599615
     眼神      1.596288
     仍然      1.542977
     快       1.532805
     女孩      1.495126
     人生      1.474836
     伤       1.472264
     越       1.467119
     一吻      1.437022
     勇敢      1.426497
     责任      1.424978
     不在乎     1.416059
     恨       1.414676
     缘分      1.407184
     有人      1.384069
     夜       1.359056
     不必      1.359039
     可能      1.311493
     明明      1.311123
     回头      1.301918
     是不是     1.292539
     好好      1.272597
     dtype: float64,
     39: 错      74.181866
     难过      4.556760
     当初      4.211908
     痛       3.771147
     恨       3.632400
     以后      3.412777
     爱上你     3.404555
     改变      3.393985
     后悔      3.290174
     承诺      3.278070
     不必      3.170348
     真       3.165393
     应该      3.087699
     放手      3.086639
     真心      2.942026
     爱过      2.878463
     到底      2.809280
     感动      2.772766
     折磨      2.684206
     不够      2.662024
     讲       2.646338
     难道      2.619861
     清楚      2.614499
     惹       2.593085
     不该      2.500519
     越       2.488317
     对错      2.482335
     自由      2.470882
     沉默      2.440125
     终于      2.393446
     从来      2.383506
     坏       2.368153
     是不是     2.353131
     回头      2.352911
     解脱      2.337450
     傻       2.269927
     伤害      2.221752
     祸       2.200967
     不了      2.188305
     日子      2.182622
     妳       2.155519
     自我      2.155474
     原谅      2.146997
     继续      2.131410
     面对      2.102948
     执着      2.088854
     分开      2.082186
     放弃      2.074855
     错过      2.066605
     怪       2.043224
     dtype: float64,
     40: 路      14.240504
     回头     12.495238
     尽头      7.464355
     远       7.058062
     以后      6.965563
     留       6.170107
     拥有      6.003717
     雨       5.716625
     理由      5.707786
     就让      5.415856
     跟着      5.305129
     自由      5.285326
     路上      5.040216
     痛       4.715219
     一步      4.649822
     带走      4.630688
     告诉      4.605834
     悲伤      4.602604
     继续      4.525742
     停留      4.515779
     见       4.476488
     不停      4.417860
     分手      4.411857
     变成      4.327893
     放手      4.317931
     是不是     4.189051
     能够      4.156008
     牵       4.096603
     街头      4.032066
     留下      3.977575
     我别      3.869423
     头       3.845430
     挽留      3.795520
     走走      3.746482
     一片      3.730878
     我要      3.704634
     不必      3.680374
     接受      3.678428
     泪       3.674734
     相爱      3.656811
     听见      3.595525
     爱过      3.576957
     恨       3.555982
     背后      3.548935
     梦想      3.491217
     风       3.484409
     看见      3.442542
     发现      3.441249
     难过      3.436869
     转身      3.411758
     dtype: float64,
     41: 甜蜜     46.010830
     想起      3.418315
     浪漫      3.131230
     分手      3.125765
     恋爱      2.918529
     呼吸      2.869498
     变成      2.770712
     哭泣      2.703848
     是不是     2.689191
     愿意      2.688242
     珍惜      2.583544
     微笑      2.536928
     想念      2.465177
     眼睛      2.412758
     容易      2.329601
     心底      2.319276
     记忆      2.275529
     留下      2.197198
     雨       2.116635
     害怕      2.096583
     一定      2.092988
     糖       2.089146
     谎言      2.082088
     痛苦      2.051942
     温暖      2.033456
     笑容      2.015855
     不必      1.996481
     全世界     1.994252
     相爱      1.986831
     怀里      1.942124
     游戏      1.910895
     慢慢      1.905619
     应该      1.903960
     拥有      1.897432
     越       1.884791
     相聚      1.877438
     叹息      1.857045
     真心      1.855857
     吻       1.847910
     小小的     1.811784
     夜里      1.799105
     爱上你     1.787858
     融化      1.776187
     靠近      1.763914
     怀疑      1.753944
     人生      1.751984
     好像      1.737562
     我俩      1.731291
     阳光      1.720996
     美好      1.715548
     dtype: float64,
     42: 梦想    6.960416
     能够    5.860290
     力量    4.813391
     路     4.303380
     阳光    4.284613
     身旁    4.050364
     拥有    4.002583
     远方    3.874439
     方向    3.848834
     温暖    3.764417
     新     3.711532
     看见    3.583495
     发现    3.468497
     以后    3.409502
     太阳    3.393123
     每个    3.345286
     看到    3.325717
     黑夜    3.264635
     遇见    3.180430
     种     3.150472
     看着    3.126335
     飞翔    3.027931
     失望    3.006703
     女郎    2.997705
     真     2.992607
     勇敢    2.989057
     放弃    2.898965
     一片    2.893714
     流浪    2.845108
     坚持    2.821229
     感情    2.801318
     不敢    2.797756
     选择    2.790131
     依然    2.783407
     黑暗    2.690697
     决定    2.685410
     我要    2.678190
     走过    2.651955
     了解    2.631282
     告诉    2.625693
     悲伤    2.569723
     害怕    2.562856
     一定    2.542308
     笑容    2.533473
     应该    2.521807
     翅膀    2.511248
     理想    2.490187
     心情    2.480632
     回到    2.476764
     好好    2.462006
     dtype: float64,
     43: 爱上你     4.108979
     起来      3.152817
     不用      3.139460
     有点      3.115095
     开心      3.081028
     讨厌      3.063548
     愿意      3.035480
     习惯      3.006939
     恋爱      2.908174
     深深地     2.879251
     很多      2.804018
     怎会      2.752442
     坏       2.725670
     偏偏      2.652873
     吃       2.607902
     浪漫      2.548973
     讲       2.527311
     不必      2.519759
     改变      2.509659
     我爱你     2.506527
     莫名      2.497681
     继续      2.489228
     相爱      2.481848
     微笑      2.479270
     看着      2.451445
     每天      2.389886
     原来      2.388875
     话       2.361098
     在乎      2.357216
     看见      2.351743
     可不可以    2.348891
     想着      2.329365
     仿佛      2.326559
     一句      2.295342
     简单      2.228385
     靠近      2.227851
     告诉      2.210857
     觉得      2.183135
     怀念      2.077797
     我要      2.057330
     得到      2.056966
     变成      2.055202
     样子      2.053154
     刚好      2.047971
     声音      2.037696
     自由      2.031728
     期待      2.023131
     呼吸      2.004549
     天天      1.995169
     这种      1.980897
     dtype: float64,
     44: 地方     79.268008
     流浪      9.574816
     天堂      7.821941
     身旁      7.491252
     模样      6.898113
     遗忘      6.194302
     遥远      5.713208
     家       5.618583
     梦想      5.361045
     方向      5.342270
     阳光      5.134564
     温暖      4.937443
     翅膀      4.639821
     飞翔      4.448507
     时光      4.227100
     悲伤      4.179089
     月光      4.170578
     站       4.146904
     远       4.089932
     力量      3.828148
     老       3.796205
     远方      3.675941
     坚强      3.662173
     忧伤      3.639189
     思念      3.589408
     回到      3.562160
     风       3.096155
     姑娘      3.091780
     渴望      3.068735
     孤单      3.044419
     太阳      3.030451
     一场      2.993049
     海洋      2.990285
     依然      2.973930
     每个      2.889396
     走过      2.888087
     受伤      2.877554
     是不是     2.864598
     飞到      2.846251
     来到      2.791610
     天亮      2.777816
     寻找      2.766128
     故乡      2.752604
     安静      2.729811
     带你去     2.674501
     夜晚      2.633176
     孩子      2.580812
     勇敢      2.580722
     能够      2.536860
     心上      2.526721
     dtype: float64,
     45: 阮     88.193400
     甲     12.716380
     拢     12.577562
     讲     10.287394
     袂      9.993118
     搁      9.619976
     伊      9.398733
     暝      8.430699
     不知     8.240725
     心肝     7.662090
     孤单     7.338528
     欲      7.332470
     惦      6.621143
     心情     6.366986
     呒      6.341781
     犹原     5.591898
     青春     5.529049
     亲像     5.482023
     心内     5.191401
     人生     5.189771
     目屎     5.112589
     知影     5.104088
     心爱     4.797159
     感情     4.784964
     无人     4.753849
     茫茫     4.751217
     故乡     4.629270
     返来     4.455202
     屎      4.426063
     阮是     4.400879
     惊      4.389765
     牵      4.239552
     敢      4.181786
     不通     4.064648
     日头     4.050827
     愈      4.001141
     路      3.886844
     阮心     3.818535
     吹      3.576483
     思念     3.566273
     无情     3.410568
     怨叹     3.385077
     仔      3.365734
     放      3.343133
     块      3.341548
     甘愿     3.268226
     娘      3.211699
     真正     3.207577
     真      3.149261
     想起     3.136666
     dtype: float64,
     46: 飞       74.079560
     起来       5.566130
     美        4.235451
     我要       4.047410
     飞过       3.810291
     风筝       3.598085
     翅膀       3.569523
     追        3.453965
     一只       3.345312
     蝴蝶       3.188545
     自由       3.078622
     远        3.054412
     流星       2.910344
     吹        2.692229
     疯狂       2.689900
     泪        2.660232
     看着       2.584267
     远方       2.496854
     寻寻觅觅     2.404025
     飘        2.353724
     蓝天       2.285315
     鸟儿       2.220396
     后悔       2.191422
     向前       2.173252
     告诉       2.148555
     是不是      2.123902
     飞飞       2.114967
     逃离       2.111367
     上天       2.092938
     每个       2.086642
     天上       2.064983
     雪花       2.041746
     风        2.039499
     天使       2.030225
     天        2.004865
     看到       1.990517
     拥有       1.977774
     彩虹       1.954170
     放心       1.945518
     回        1.898752
     找到       1.884985
     轻轻       1.867693
     勇敢       1.860474
     白云       1.820693
     完美       1.819533
     到处       1.813809
     云        1.801368
     大地       1.795749
     往前       1.786721
     梦想       1.776294
     dtype: float64,
     47: 朋友     80.305831
     以后      3.912619
     左右      3.538529
     情人      3.373391
     理由      3.312006
     人生      3.270102
     能够      3.150569
     变成      3.138881
     分手      2.950975
     干杯      2.940648
     一句      2.767040
     一定      2.558193
     杯酒      2.505496
     感情      2.494435
     酒       2.486054
     问候      2.378004
     不必      2.315525
     话       2.303864
     只能      2.288263
     自由      2.251830
     告诉      2.229893
     拥有      2.228199
     最好      2.222479
     有人      2.172154
     有没有     2.163567
     分担      2.157591
     事       2.142571
     岁月      2.137208
     回头      2.136818
     放手      2.134368
     变       2.128454
     兄弟      2.106866
     知己      2.101099
     之后      2.065661
     忧愁      1.984320
     欢笑      1.958312
     普通      1.957552
     梦想      1.901189
     难过      1.899246
     跟着      1.876312
     很多      1.874386
     知       1.867215
     男人      1.855635
     不用      1.848070
     举起      1.846919
     难得      1.840530
     多久      1.825595
     路       1.752927
     苦衷      1.743357
     每次      1.681735
     dtype: float64,
     48: 眼睛    7.156978
     呼吸    6.626246
     夜     5.880820
     身体    5.751482
     抗拒    5.643390
     愿意    5.462793
     停止    5.398068
     改变    5.318069
     只能    5.286266
     告诉    4.958256
     风     4.759601
     存在    4.758521
     发现    4.727047
     能够    4.671459
     思念    4.551710
     脸     4.541437
     期待    4.531228
     看见    4.420173
     不停    4.389078
     坚强    4.251826
     记忆    4.144315
     依然    4.115080
     勇敢    4.037669
     难道    3.996099
     代替    3.961889
     痛     3.950804
     日子    3.927348
     变     3.920698
     继续    3.913222
     得到    3.857298
     黑夜    3.718608
     遗憾    3.633648
     觉得    3.609270
     真     3.595279
     消失    3.586380
     青春    3.553193
     悲伤    3.508234
     害怕    3.495178
     唯一    3.440658
     声音    3.427322
     完美    3.426061
     留下    3.382048
     想着    3.355851
     看着    3.340768
     时光    3.339709
     微笑    3.323255
     孤独    3.314137
     仿佛    3.313283
     最美    3.311950
     温暖    3.306598
     dtype: float64,
     49: 男女     76.021918
     女      21.030841
     男      17.814617
     男合     17.812601
     合      13.334416
     男合女     6.305505
     合女      6.132223
     女合      3.871805
     合合      2.524521
     想念      1.116121
     浪漫      1.086876
     变       1.082276
     真       1.032451
     合男      0.967765
     变改      0.957782
     女女      0.909477
     看见      0.888467
     真心      0.877150
     相爱      0.824223
     受伤      0.815999
     期待      0.798217
     人生      0.789391
     舞台      0.789226
     未       0.781455
     最美      0.752294
     噢       0.737077
     话       0.714136
     一句      0.692758
     一夜      0.686832
     一杯      0.685894
     愿意      0.682764
     爱过      0.654230
     佢       0.649589
     珍贵      0.647893
     思念      0.647193
     相约      0.630105
     存在      0.629786
     明天      0.628805
     情人      0.623452
     解释      0.619637
     重逢      0.616712
     谢谢      0.614061
     知       0.610310
     同心      0.610136
     证明      0.609951
     感情      0.606888
     如今      0.606339
     留下      0.605695
     只爱      0.603606
     分开      0.601050
     dtype: float64,
     50: 恭喜     17.953293
     以后      6.573489
     想起      5.021626
     想念      4.023664
     那年      3.851054
     话       3.720841
     未       3.596146
     当初      3.561096
     每个      3.509050
     依然      3.267892
     一句      3.181941
     如今      3.156902
     当时      3.101788
     青春      3.100953
     怀念      3.099030
     后来      2.963824
     一年      2.921777
     雨       2.920356
     约定      2.844833
     美好      2.826630
     拥有      2.813016
     遗忘      2.741654
     恋爱      2.718238
     是不是     2.709858
     属于      2.663449
     岁月      2.657987
     微笑      2.646774
     回来      2.618777
     伤口      2.588429
     记忆      2.580358
     梦里      2.578170
     最好      2.522328
     分开      2.521445
     承诺      2.478760
     时光      2.442809
     可能      2.431238
     觉得      2.380453
     放弃      2.352931
     吻       2.342186
     从前      2.321693
     思念      2.296871
     以前      2.272026
     名字      2.270597
     变成      2.270177
     真       2.253038
     有没有     2.250598
     遥远      2.246173
     不知      2.245669
     多久      2.236284
     那天      2.222707
     dtype: float64,
     51: 噢      49.233485
     嫂子      3.623965
     狗       1.701983
     凉       1.320610
     喔       1.220124
     爸爸      1.175423
     故乡      1.156683
     耶       0.994176
     原谅      0.911563
     蛋       0.889808
     丢失      0.886251
     伟大      0.882989
     能够      0.882235
     呼唤      0.876153
     家       0.864813
     天       0.850597
     觉悟      0.815474
     不知      0.794080
     兄弟      0.792894
     泪滴      0.790215
     被迫      0.772870
     虚构      0.759889
     有人      0.754298
     公平      0.728612
     电话      0.708555
     好像      0.688038
     山       0.677605
     哥哥      0.673284
     应该      0.654334
     唔       0.648892
     滴答      0.643796
     鲜血      0.637692
     喧嚣      0.630955
     孤单      0.624663
     想法      0.611954
     灿烂      0.611471
     路       0.609601
     诶       0.608658
     终究会     0.605912
     飞       0.601450
     上天      0.599558
     快       0.599087
     一句      0.593971
     外面      0.589179
     妈妈      0.585948
     太久      0.583613
     叮叮      0.580867
     可能      0.578354
     烦恼      0.573855
     哭泣      0.572459
     dtype: float64,
     52: 改变    11.322013
     整个    11.086610
     宇宙     8.930888
     每个     7.830226
     梦想     7.391282
     告诉     6.917451
     看见     6.163709
     我要     5.939627
     完美     5.891774
     里面     5.607754
     了解     5.556008
     快      5.533108
     存在     5.499744
     地球     5.467092
     眼睛     5.362861
     拥有     5.332328
     就让     5.283460
     人生     5.181664
     两个     5.137939
     外面     4.971652
     面对     4.906602
     期待     4.847219
     变      4.800626
     新      4.739970
     瞬间     4.730136
     放弃     4.695433
     不同     4.690474
     黑暗     4.656042
     声音     4.628547
     勇敢     4.620186
     充满     4.601549
     属于     4.581122
     愿意     4.572884
     可能     4.556571
     悲伤     4.525270
     现实     4.499516
     温暖     4.474186
     消失     4.357662
     继续     4.335495
     终于     4.309108
     城市     4.240872
     变得     4.233127
     原来     4.216138
     天      4.176271
     看着     4.130059
     越      4.102007
     孤单     4.093977
     风      4.076053
     之间     4.029302
     雨      3.971895
     dtype: float64,
     53: 我心    9.105583
     愿     7.499263
     轻轻    7.254169
     风     7.031647
     令     6.428617
     泪     6.283093
     似     5.887762
     愿意    5.701127
     雨     5.578599
     始终    5.540050
     不知    5.521872
     永不    5.473024
     不可    5.269728
     改变    5.242300
     爱意    5.144654
     记忆    5.129607
     默默    5.090709
     痴心    4.919705
     未     4.766781
     知     4.719016
     想起    4.717929
     深深    4.717340
     留下    4.577054
     匆匆    4.490053
     此刻    4.472432
     仿佛    4.384707
     一片    4.374012
     讲     4.355059
     怀念    4.300507
     爱念    4.250761
     这刻    4.177508
     情     4.172121
     告诉    4.129208
     没法    4.107205
     伴     4.068993
     找到    4.051870
     温暖    4.041467
     柔情    4.000392
     从前    3.972655
     一刻    3.967054
     人生    3.941114
     原来    3.936276
     梦想    3.931940
     苦痛    3.876110
     只想    3.835497
     暖     3.818023
     无奈    3.778353
     今夜    3.772981
     不愿    3.769256
     思念    3.756514
     dtype: float64,
     54: 亲爱     64.469311
     宝贝      2.539877
     告诉      2.321022
     朋友      2.272703
     小孩      2.216495
     姑娘      2.021883
     日子      2.005542
     哭泣      1.932058
     愿意      1.870067
     拥有      1.865329
     害怕      1.775706
     爱过      1.738280
     好好      1.713559
     孤独      1.712470
     风       1.665058
     一声      1.635436
     一定      1.581928
     春天      1.563774
     微笑      1.507611
     美好      1.498956
     原谅      1.495412
     慢慢      1.490307
     想起      1.481941
     沉默      1.468598
     勇敢      1.437704
     电话      1.431797
     风中      1.399756
     变成      1.370444
     眼睛      1.343425
     可知      1.307449
     心底      1.304171
     秋天      1.271268
     错       1.255347
     雨       1.239083
     我要      1.231954
     阳光      1.203517
     人住      1.189481
     人生      1.151542
     睡       1.094178
     就让      1.090485
     站       1.088026
     老       1.075422
     像是      1.075162
     祝福      1.067033
     路       1.063026
     在乎      1.052911
     眼神      1.051577
     分外      1.044330
     我爱你     1.033936
     先       1.031451
     dtype: float64,
     55: 哒       25.377967
     嘟        1.001153
     流沙       0.621431
     滴答       0.612872
     听到       0.594949
     喔        0.582258
     哔        0.570277
     说谎       0.555437
     重新       0.540401
     唔        0.472061
     滴哒       0.451003
     绿洲       0.448864
     笑笑       0.448796
     装作       0.444934
     夏夜       0.431072
     反复       0.429076
     透露       0.426547
     表情       0.413994
     蜜糖       0.395947
     北京       0.379031
     麻雀       0.372328
     失落       0.367929
     逃离       0.366977
     回音       0.363571
     派        0.358151
     看着       0.356911
     如常       0.353263
     吹拂       0.347455
     不可思议     0.345851
     上天       0.344043
     末日       0.339233
     只想       0.331288
     左右       0.331110
     阿兄       0.323298
     直接       0.322672
     遮掩       0.319120
     心疼       0.310373
     冷静       0.306622
     沙漠       0.303493
     好像       0.303448
     噜        0.303268
     一步       0.301871
     一定       0.297796
     红灯       0.293542
     得到       0.287748
     一边       0.287189
     唯一       0.286295
     意外       0.285717
     番        0.284672
     屋企       0.281317
     dtype: float64,
     56: 慢慢     72.215704
     习惯      5.665809
     变成      4.164511
     轻轻      3.961540
     遗忘      3.644459
     拼凑      3.092094
     时光      3.041911
     想念      2.852083
     只能      2.851253
     靠近      2.804675
     记忆      2.783081
     感动      2.736225
     熟悉      2.579921
     思念      2.571918
     呼吸      2.557660
     学会      2.518914
     渐渐      2.496924
     原来      2.481553
     声音      2.441362
     看着      2.438391
     留不住     2.380356
     心跳      2.369641
     改变      2.358098
     不停      2.335727
     听见      2.270688
     想起      2.265605
     能够      2.248916
     微笑      2.202035
     愿意      2.175767
     流       2.142403
     阳光      2.133190
     悲伤      2.126277
     可能      2.109037
     不必      2.069543
     夜晚      2.068278
     轻轻地     2.022570
     岁月      2.012325
     偶尔      2.001600
     浪漫      1.998772
     或许      1.984666
     总       1.956669
     决定      1.955970
     告诉      1.944538
     之后      1.941067
     青春      1.926435
     越来越     1.891441
     慢       1.890196
     温暖      1.868487
     画面      1.866543
     浮现      1.848909
     dtype: float64,
     57: 发现    8.026298
     一年    7.139609
     终于    6.301622
     一遍    5.943889
     改变    5.786240
     脸     5.651706
     出现    5.504022
     每天    4.807395
     怀念    4.760982
     变     4.617957
     看见    4.575083
     温暖    4.356648
     期待    4.325292
     原来    4.288339
     思念    4.104059
     里面    4.099381
     慢慢    4.047864
     新     4.029713
     美好    3.974034
     我要    3.972216
     天     3.792671
     告诉    3.705490
     一夜    3.630000
     长大    3.629885
     风     3.613417
     童年    3.590219
     拥有    3.498577
     从前    3.471706
     事     3.419466
     孤单    3.389112
     回到    3.378214
     始终    3.360324
     此刻    3.307911
     或许    3.284586
     留下    3.264040
     看着    3.254432
     想起    3.249125
     阳光    3.182736
     活     3.163853
     明天    3.126378
     之间    3.122330
     一定    3.107084
     想念    3.091666
     命运    3.081552
     爱恋    3.072146
     总会    3.060696
     梦想    3.050530
     昨天    3.034181
     愿意    3.019364
     不停    3.018953
     dtype: float64,
     58: 分离     7.074152
     两个     6.664924
     分享     5.765449
     每天     5.360570
     能够     4.973649
     愿意     4.791156
     走过     4.711195
     勇气     4.633255
     放弃     4.291845
     跳      4.101517
     只想     4.010078
     不用     3.990734
     珍惜     3.843723
     烦恼     3.821425
     梦想     3.630660
     人生     3.572066
     距离     3.514404
     天地     3.499449
     抱      3.460268
     跟着     3.372188
     跳舞     3.335433
     朋友     3.322321
     或许     3.245234
     遇见     3.185587
     原来     3.154688
     心情     3.113617
     呼吸     3.111039
     继续     3.109301
     容易     3.098481
     我要     3.096998
     阳光     3.051645
     日子     3.010777
     玩      2.982766
     节奏     2.924404
     有时候    2.902578
     死      2.901939
     寻找     2.875440
     摇摆     2.858611
     习惯     2.858376
     假装     2.821833
     浪漫     2.816015
     快      2.806982
     眼睛     2.783325
     心跳     2.771570
     大声     2.749684
     好好     2.742247
     害怕     2.728501
     拥有     2.692152
     一秒     2.687873
     勇敢     2.627427
     dtype: float64,
     59: 見     21.085379
     気     21.006190
     涙     18.494890
     誰     15.375556
     変     14.391032
     私     13.337556
     帰     12.168946
     君     11.577525
     顔     10.961352
     愛     10.515542
     夢      9.912823
     今日     7.877223
     時      7.740229
     僕      7.596425
     明日     6.717802
     二人     6.510249
     言      6.479835
     作曲     6.420904
     遠      5.591139
     胸      5.561437
     泣      5.529763
     無      5.016646
     抱      4.257253
     行      3.975586
     後      3.834447
     夜      3.608418
     昨日     3.592332
     目      3.551500
     道      3.521117
     眠      3.119028
     止      3.093580
     生      2.866820
     元      2.700882
     感      2.586062
     追      2.544486
     一度     2.512244
     街      2.353219
     逢      2.292993
     雨      2.241837
     空      2.229665
     悲      2.218110
     暮      2.110226
     切      2.096691
     知      2.064584
     探      2.038433
     前      2.034902
     一人     1.968473
     迷      1.920943
     少      1.915614
     寄      1.873457
     dtype: float64}




```python
# Try 300 again but with max_df set at 5%
from numpy import unique
from numpy import where
from sklearn.datasets import make_classification
from sklearn.cluster import KMeans
from matplotlib import pyplot
# define dataset

# fit the tvec on the 35k rows of lyrics, will take include the 10k top features in terms of scores. 
# Higher scores go to words that have more disciminating power. I.e. those that appear frequently in some documents,
# but occur relatively less freqently overall. 
tvec = TfidfVectorizer(stop_words=updated_stopwords_3,max_df=0.3,max_features=10000,tokenizer=man_token_jie, norm='l2')
tvec.fit(lyrics)

# as we fit on all lyrics, and we are transoforming the 35k lyric rows into a dataframe, all 10k features will occur
# at least once. 
X_transformed = tvec.transform(lyrics)


# Consider X_transform each row is a vector, pointing to a position in 10k dimensional space.KMeans looks at these 
# points in 10k dimensional space and finds the 60 best centroids. 
from sklearn.cluster import KMeans
modelkmeans = KMeans(n_clusters=300, init='k-means++', n_init=100)
modelkmeans.fit(X_transformed)

# update the main dataframe so that labels can be seen
data['labels_300_splits_30%maxdf'] = modelkmeans.labels_
# for each of the 60 labels, look at the words that were given the highest score in terms of their ability to distinguish


```


```python
# from each of the other 2 labels. 
topics4 = {}
for i in range(300):
    topics4[i] = pd.DataFrame(tvec.transform(data[data['labels_300_splits_30%maxdf']==i].lyrics).toarray(), columns =tvec.get_feature_names()).sum().sort_values(ascending=False)[:10]
    
```


```python
topics4
```




    {0: 手中     11.699240
     紧握      2.997353
     名字      2.907741
     走       1.355694
     几公里     1.320932
     双手      1.317350
     手       1.246790
     飞驰      1.188928
     相爱      1.057249
     杯       0.997125
     dtype: float64,
     1: 相信    42.250376
     爱情     3.248130
     不会     2.692172
     幸福     2.227495
     我会     2.159298
     不要     2.143100
     世界     2.119697
     知道     2.016600
     不信     1.963493
     直觉     1.874105
     dtype: float64,
     2: 吃      29.848988
     早餐      5.023519
     钱       2.416164
     不吃      2.375211
     了不起     2.287201
     一口      2.275391
     菜       1.992932
     不要      1.884285
     现在      1.805504
     贪       1.776901
     dtype: float64,
     3: 听见    17.875504
     声音     2.446248
     不要     1.534376
     走      1.465714
     心里     1.449667
     一点     1.433982
     我别     1.433129
     出口     1.413452
     爱情     1.357273
     真的     1.342828
     dtype: float64,
     4: 想起    30.370237
     从前     3.024906
     心里     2.789818
     喜爱     2.499201
     不知     2.191333
     是否     2.054288
     总是     1.967856
     时间     1.950020
     回忆     1.894797
     曾经     1.866993
     dtype: float64,
     5: 一闪     7.235514
     亮晶晶    1.791516
     永远     1.069549
     知足     0.826387
     天上     0.686384
     星星     0.671769
     一阵风    0.642605
     目的地    0.615893
     小星星    0.604995
     身影     0.527720
     dtype: float64,
     6: 飞鸟    4.983814
     看到    3.077678
     向前    2.650793
     黑白    2.074552
     骤然    2.068424
     天边    1.673451
     一线    1.598784
     翻     1.497192
     一只    1.308555
     往返    1.231883
     dtype: float64,
     7: 男女     69.806180
     女      18.052720
     男合     16.810450
     男      15.373227
     合      10.927231
     男合女     6.247706
     合女      4.910365
     女合      2.998868
     合合      2.488471
     一生      1.490144
     dtype: float64,
     8: 别哭    9.402345
     最美    0.873683
     不老    0.865516
     昙花    0.821286
     凋落    0.810585
     挽     0.732155
     红颜    0.713655
     是否    0.708034
     男孩    0.648479
     叹息    0.635449
     dtype: float64,
     9: 跑       14.570826
     跑跑       2.079044
     轻轻       1.954487
     向前       1.856972
     勇敢       1.574901
     笑        1.282163
     快        1.165698
     天涯海角     0.969155
     往前       0.842497
     快点       0.829546
     dtype: float64,
     10: 鼓励     3.764300
     成全     3.250605
     远      2.248018
     幸福     1.719113
     高      1.547702
     确定     1.542747
     追求     1.402374
     一朵花    1.151054
     渴求     0.961115
     星球     0.818977
     dtype: float64,
     11: 吻     29.798105
     抱      2.593830
     便      2.122802
     心      2.120298
     震撼     1.978435
     唇      1.947895
     需要     1.717028
     就让     1.708745
     一吻     1.570013
     兴奋     1.529348
     dtype: float64,
     12: 里面     18.145868
     草原      2.380543
     离不开     1.651287
     如火      1.522638
     脸       1.463010
     世界      1.393143
     一天      1.352073
     看不见     1.296190
     出现      1.279851
     躺       1.261164
     dtype: float64,
     13: 干     7.225059
     往事    5.109021
     干杯    3.807142
     朋友    2.309271
     举起    2.125709
     一杯    2.023999
     拎     1.749912
     杯酒    1.499390
     倘卖    1.431616
     酒     1.243329
     dtype: float64,
     14: 曾经    42.750876
     回忆     4.194256
     现在     4.016104
     拥有     3.384405
     已经     2.982406
     过去     2.885693
     失去     2.850450
     不会     2.692711
     走      2.530033
     走过     2.261344
     dtype: float64,
     15: 故事    31.450685
     意义     5.392125
     美丽     3.063192
     结局     2.285008
     梦      2.259795
     最后     2.229014
     名字     2.154838
     太      2.136872
     遮住     2.061917
     未      2.058752
     dtype: float64,
     16: 有没有    25.304911
     到底      1.425169
     日子      1.400487
     走       1.364613
     过去      1.326840
     世界      1.266032
     现在      1.217337
     不会      1.199680
     越来越     1.187604
     忧       1.175501
     dtype: float64,
     17: 女人    52.641935
     男人     9.630162
     心      2.873136
     花      2.691771
     爱情     2.576831
     太      2.228224
     美丽     2.218742
     每个     2.133754
     难道     1.996963
     一生     1.992386
     dtype: float64,
     18: 怕     31.029919
     不要     2.768043
     突然     2.656876
     没      2.011496
     伤      1.936187
     我怕     1.719547
     太      1.563258
     知道     1.455966
     需要     1.408220
     害怕     1.385974
     dtype: float64,
     19: 留给      18.022758
     走        1.836773
     悲伤       1.826460
     可不可以     1.642737
     带走       1.574353
     是否       1.411908
     离开       1.393998
     明天       1.386043
     思念       1.292590
     回忆       1.199581
     dtype: float64,
     20: 不该    29.302193
     知道     2.766428
     伤心     2.230545
     爱情     2.010182
     离开     1.938595
     总是     1.929684
     回忆     1.838814
     是否     1.775092
     难免     1.721108
     不会     1.694305
     dtype: float64,
     21: 变     31.163956
     世界     2.676548
     改变     2.522590
     一天     2.387646
     不会     2.010489
     永远     1.976822
     慢慢     1.771435
     心      1.748116
     幸福     1.522899
     未      1.485008
     dtype: float64,
     22: 回来    39.031968
     离开     4.554685
     等待     4.053910
     明白     2.551722
     一定     2.205086
     开      2.189573
     期待     2.134004
     未来     1.884423
     现在     1.876663
     不再     1.871077
     dtype: float64,
     23: 天空     30.696413
     彩虹      6.607731
     梦       3.565290
     天黑黑     2.851447
     走       2.392934
     世界      2.308052
     笑容      2.148406
     感动      2.078034
     思念      1.971002
     云       1.926756
     dtype: float64,
     24: 春天    28.428941
     冬天     2.320862
     一年     1.988048
     红      1.796773
     岁      1.724743
     春风     1.550722
     永远     1.444804
     爱情     1.388101
     脸      1.336520
     心      1.325801
     dtype: float64,
     25: 无所谓    21.673896
     狼狈      1.902618
     没       1.491428
     安慰      1.239239
     吾       1.231039
     笑       1.225999
     已经      1.138411
     爱情      1.103515
     乡       1.040440
     不在乎     1.034869
     dtype: float64,
     26: 知道    83.906535
     真的     4.636226
     心里     4.497507
     不会     4.314491
     已经     4.215843
     心      4.080495
     走      3.879024
     我会     3.814037
     不要     3.718957
     太      3.589062
     dtype: float64,
     27: 放      11.850171
     出品      5.158817
     火       3.870150
     小提琴     3.215530
     大提琴     2.047583
     轻松      1.629145
     低       1.352232
     野狼      1.285960
     飞机      1.085297
     送给      1.071497
     dtype: float64,
     28: 得到    34.902180
     没      3.195212
     便      3.134373
     令      2.997439
     美丽     2.974633
     知道     2.932267
     不到     2.877801
     期望     2.864914
     怕      2.816670
     失去     2.716300
     dtype: float64,
     29: 纯净      4.078475
     无比      3.677591
     偶尔      2.464910
     世界      1.265066
     一天      1.037828
     低       0.892292
     最好      0.813827
     永远      0.806013
     无所不在    0.755052
     送       0.719359
     dtype: float64,
     30: 时代     22.932078
     崇拜      1.922872
     跟我来     1.684732
     现在      1.549926
     精彩      1.432930
     世界      1.427522
     新       1.339045
     时间      1.293858
     心       1.241187
     生活      1.223877
     dtype: float64,
     31: 之间    22.750070
     距离     1.452847
     脸      1.305941
     时间     1.250672
     回忆     1.196804
     再见     1.174136
     心      1.170205
     需要     1.150811
     世界     1.138985
     空气     1.133466
     dtype: float64,
     32: 此刻    23.555313
     撩动     1.941838
     心      1.772971
     过去     1.727768
     世界     1.610292
     敲打     1.594157
     时光     1.537044
     琴弦     1.522465
     欢乐     1.459992
     记忆     1.378866
     dtype: float64,
     33: 真的    51.126544
     知道     4.426376
     是否     4.005833
     不会     3.404417
     拥有     3.333167
     心      3.140206
     不想     3.100704
     假      3.047059
     走      2.729245
     感觉     2.405058
     dtype: float64,
     34: 完美    32.787852
     世界     3.381065
     美丽     2.540405
     爱情     2.439103
     需要     2.200066
     太      2.066797
     追求     1.940304
     不会     1.812988
     时间     1.622993
     也许     1.557007
     dtype: float64,
     35: 夜来香    5.995984
     月色     5.051705
     听听     2.716692
     明月     1.268598
     思量     1.172036
     树影     1.091304
     千里     1.024737
     夜莺     0.896090
     摇晃     0.891127
     夜色     0.889432
     dtype: float64,
     36: 笑     28.195502
     美丽    26.857357
     世界    25.033953
     心里    24.905236
     没     23.618500
     走     22.122639
     需要    21.658690
     知道    21.040710
     问     20.831282
     不会    20.445583
     dtype: float64,
     37: 风吹    13.079166
     飘      3.014608
     砂      2.239672
     夜      1.621059
     吹      1.145410
     幸福     1.007213
     走      0.939332
     落      0.918789
     间      0.858691
     难眠     0.818433
     dtype: float64,
     38: 呜呜     16.063113
     车站      0.813913
     喔       0.798932
     弯弯的     0.797440
     真的      0.752280
     先知      0.721789
     健忘      0.646676
     玩玩      0.618430
     时间      0.606828
     还好      0.568675
     dtype: float64,
     39: 阮     61.341463
     甲      7.606463
     讲      5.299961
     不知     4.664718
     心      4.240084
     拢      4.181887
     心肝     4.170568
     孤单     4.146727
     暝      4.105144
     心情     3.789667
     dtype: float64,
     40: 想到    16.364259
     梅兰     1.831136
     苦衷     1.342327
     最后     1.327974
     走      1.270961
     没      1.187872
     丢      1.044643
     心里     1.041664
     时间     1.015919
     寂寞     1.006480
     dtype: float64,
     41: 没法    30.219580
     得到     3.369724
     令      3.353154
     似      3.089323
     没      3.046677
     太      2.881517
     未      2.617804
     便      2.414253
     难      2.412960
     永远     2.364721
     dtype: float64,
     42: 一杯     19.860293
     咖啡      3.229431
     敬       2.149000
     酒       2.007854
     喝       1.865246
     干       1.675886
     酒杯      1.631279
     喝一杯     1.027312
     我敬      1.018688
     不醉      0.972126
     dtype: float64,
     43: 喜欢     69.663774
     太       3.108206
     感觉      2.971207
     想要      2.940961
     爱情      2.828679
     知道      2.619267
     真的      2.564413
     偏偏      2.259416
     我爱你     2.209826
     没       2.136364
     dtype: float64,
     44: 感受    23.148059
     温柔     3.142513
     左右     2.853041
     享受     2.555425
     以后     2.392852
     走      2.009314
     手      1.902322
     真正     1.819534
     需要     1.818815
     世界     1.659448
     dtype: float64,
     45: 明天    35.419449
     今天     4.130301
     昨天     3.321139
     希望     2.456097
     微笑     2.166916
     不要     2.105533
     面对     1.879519
     脸      1.705524
     知道     1.675604
     心      1.597722
     dtype: float64,
     46: 兄弟    16.256765
     拎      1.343704
     北      1.278315
     大道     1.269558
     朋友     1.217119
     平安     1.208161
     干掉     1.202210
     没      1.153840
     知道     1.151791
     玛莉     0.932587
     dtype: float64,
     47: 很久很久    3.056823
     孤寂      2.670181
     角落      2.209583
     外面      1.627930
     拥挤      1.483205
     以前      1.401827
     世界      1.212695
     喔       1.181550
     哭泣      1.051313
     伤心      0.767615
     dtype: float64,
     48: 力量    20.999409
     勇敢     2.615824
     希望     2.492533
     坚强     2.446417
     飞翔     2.402272
     身旁     2.388453
     方向     2.290543
     世界     1.986445
     信仰     1.971078
     翅膀     1.966100
     dtype: float64,
     49: 路     29.408235
     走      4.894044
     幸福     2.861112
     一条     2.312207
     这条     2.254906
     梦      2.240842
     茫茫     2.197617
     穿过     2.124033
     走过     2.068441
     踏上     1.818006
     dtype: float64,
     50: 今晚      33.974048
     这宵       3.157046
     千千阙歌     3.101079
     千千       2.878154
     晚星亮      2.790729
     比不起      2.618193
     夜        2.372517
     飘于       2.358023
     可使       2.317595
     月亮       2.290600
     dtype: float64,
     51: 便     32.437057
     未      3.675600
     令      3.302903
     没      2.506545
     不到     2.409457
     难道     2.265051
     似      2.251969
     有着     1.956090
     需要     1.866784
     拥抱     1.833773
     dtype: float64,
     52: 不要      58.885642
     问        5.201781
     可不可以     4.857743
     知道       4.728916
     太        4.579475
     心        4.248545
     没        4.149332
     多一点点     3.704290
     动次       3.677728
     时间       3.498437
     dtype: float64,
     53: 男     36.959096
     女     34.513058
     合     19.914848
     男女     8.953150
     男合     6.772262
     女合     5.761998
     合女     4.257792
     爱情     2.804669
     没      2.611036
     令      2.596552
     dtype: float64,
     54: 讲     33.534266
     话      4.915005
     知      4.006581
     未      2.799835
     其实     2.655138
     说话     2.434163
     一句     2.163788
     太      2.083642
     无谓     2.044910
     问      1.958514
     dtype: float64,
     55: 中国     16.567757
     新       1.296899
     五千年     1.223202
     梦       0.938966
     艳阳天     0.926004
     长城      0.909628
     国       0.832739
     姑娘      0.756606
     父亲      0.722501
     一百年     0.719792
     dtype: float64,
     56: 不愿     26.081664
     梦       3.025449
     潮水      2.333446
     深夜里     2.079480
     承受      1.869429
     爱如      1.782801
     心       1.621579
     爱情      1.613371
     心碎      1.611397
     我俩      1.571543
     dtype: float64,
     57: 一点    41.035456
     靠近     3.671201
     需要     2.820166
     世界     2.744026
     感觉     2.641723
     没      2.440492
     一遍     2.369464
     快      1.993075
     时间     1.909171
     不够     1.893517
     dtype: float64,
     58: 愿     30.342351
     永远     2.749596
     一生     2.605242
     游      1.882258
     欢笑     1.825936
     世间     1.535219
     怎能     1.445629
     失去     1.395961
     一天     1.341863
     不再     1.266083
     dtype: float64,
     59: 愿意    38.107927
     美丽     2.940807
     相信     2.522356
     世界     2.297099
     是否     2.153627
     可惜     2.120638
     梦      2.070934
     真的     1.908739
     牺牲     1.899035
     永远     1.825754
     dtype: float64,
     60: 朋友    49.334187
     走      3.288020
     不要     3.216599
     理由     2.388229
     不会     2.311304
     不能     2.139510
     陪      2.129732
     知道     1.899722
     能够     1.868213
     情人     1.815828
     dtype: float64,
     61: 思念      20.466312
     悲伤       2.221917
     一千遍      1.291938
     一条       1.233961
     走        1.202935
     知不知道     1.140320
     一万遍      1.132348
     亲像       1.129809
     蔓延       1.060646
     慢慢       1.050393
     dtype: float64,
     62: 眼泪     34.863642
     掉       4.825626
     掉下来     3.554117
     流下      3.345922
     不要      3.084471
     心里      2.525882
     心碎      2.372761
     爱情      2.316890
     笑       2.179362
     擦干      2.063813
     dtype: float64,
     63: 爱上你    30.259088
     真的      3.038934
     喜欢      2.694439
     深深地     2.622723
     怎会      2.360239
     莫名      2.169187
     知道      2.125703
     在乎      2.013003
     错       1.913700
     我会      1.893587
     dtype: float64,
     64: 娱乐    9.781268
     站     6.536587
     开心    5.330167
     骄傲    2.908005
     笑     1.436783
     太     1.214463
     继续    1.121756
     只得    1.079024
     走     1.030376
     一秒    0.981643
     dtype: float64,
     65: 放弃    40.244300
     真的     3.737304
     心      3.595891
     不能     3.402520
     爱情     3.296121
     勇气     3.124370
     最后     2.970123
     失去     2.916240
     幸福     2.911846
     轻易     2.906170
     dtype: float64,
     66: 也许    64.090348
     以后     3.712331
     一天     3.347574
     世界     3.280684
     爱情     3.272059
     真的     3.159417
     已经     3.147876
     走      3.145522
     曾经     3.112400
     回忆     2.991138
     dtype: float64,
     67: 总是    28.304182
     太软     3.297199
     心      2.690907
     不会     2.000331
     寂寞     1.993771
     走      1.893682
     练习     1.760439
     知道     1.714315
     回忆     1.686316
     发现     1.553845
     dtype: float64,
     68: 忧伤    21.878869
     劝      1.837460
     带来     1.641633
     离开     1.513175
     地方     1.425381
     梦      1.400754
     世界     1.356988
     走      1.297773
     城市     1.259142
     天亮     1.251901
     dtype: float64,
     69: 等待    39.125071
     未来     3.235644
     爱情     2.632984
     期待     2.519055
     一生     2.473453
     徘徊     2.417203
     时间     2.326737
     现在     2.083583
     只能     2.050146
     存在     2.007993
     dtype: float64,
     70: 没     50.555829
     办法     4.582324
     知道     3.564175
     发现     3.418345
     现在     3.264377
     从来     2.989941
     过去     2.972305
     世界     2.929935
     太      2.823106
     时间     2.678420
     dtype: float64,
     71: 爱过    31.834514
     曾经     3.818977
     懂      2.571190
     爱情     2.524158
     不会     2.485797
     知道     2.315838
     忘记     2.212857
     变成     2.153784
     走      2.127378
     回忆     2.083628
     dtype: float64,
     72: 走     68.009061
     回头     6.515123
     手      4.927418
     路      3.862808
     不要     3.499650
     没      3.485336
     梦      3.427945
     世界     3.384484
     不能     3.246246
     远      3.230512
     dtype: float64,
     73: 是不是    36.638793
     不能      2.739367
     爱情      2.625272
     知道      2.613543
     多久      2.346344
     离开      2.259427
     现在      2.186756
     太       2.053205
     走开      1.923985
     真的      1.884030
     dtype: float64,
     74: 回家    32.222596
     路      2.609279
     今天     2.413229
     不要     1.937658
     爱带     1.733459
     我要     1.696923
     路上     1.566686
     家      1.426341
     不想     1.395123
     乡      1.359233
     dtype: float64,
     75: 人间    32.348401
     笑      3.239189
     红尘     2.915857
     梦      1.978776
     烟火     1.934369
     世界     1.714858
     岁月     1.669926
     叹      1.580869
     梦里     1.559910
     满      1.530687
     dtype: float64,
     76: 君    15.397959
     気    15.152196
     見    10.332479
     変     9.966795
     僕     6.967799
     顔     5.822122
     時     5.190621
     言     3.290018
     夢     3.235620
     愛     2.495428
     dtype: float64,
     77: 难      26.652760
     忘不了     7.444291
     缘       3.523913
     相思      2.386271
     生还      2.289468
     情难      2.178847
     今夜      2.131181
     暮       2.121877
     明       1.999133
     过去      1.987299
     dtype: float64,
     78: 越      58.939708
     世界      3.591282
     幸福      3.516473
     走       2.927287
     越来越     2.828050
     心       2.620717
     寂寞      2.506236
     怕       2.368882
     不会      2.351496
     没       2.324545
     dtype: float64,
     79: 回不来    3.840068
     看开     2.549385
     终于     1.330067
     求      1.305952
     失败     1.139295
     放不开    1.135473
     没出息    1.071123
     离开     1.057841
     我别     1.043952
     最后     0.921298
     dtype: float64,
     80: 孤单    37.187312
     习惯     3.152355
     心      2.943562
     不再     2.795183
     浪漫     2.484035
     永远     2.278723
     遗憾     2.184501
     现在     2.156354
     世界     2.150010
     爱情     2.127711
     dtype: float64,
     81: 时间    40.781336
     世界     2.896394
     没      2.798930
     走      2.488902
     不会     2.429225
     需要     2.276735
     过去     2.001076
     回到     1.873961
     感觉     1.762040
     变成     1.751157
     dtype: float64,
     82: 情     27.210648
     心      3.290252
     当年     2.834609
     轻轻     2.594650
     往日     2.428286
     这段     2.371706
     心里     2.135255
     我心     2.035334
     梦里     1.959127
     知      1.934539
     dtype: float64,
     83: 今天    49.910077
     昨天     7.847664
     明天     5.319435
     当天     4.382886
     太      2.889021
     不要     2.865430
     一天     2.790826
     心      2.449906
     没      2.294601
     走      2.289234
     dtype: float64,
     84: 家      24.201041
     洗       1.818727
     地方      1.754315
     留在      1.681827
     温暖      1.648861
     远方      1.578488
     长大      1.530259
     发展      1.282162
     电视剧     1.271855
     娃娃      1.161898
     dtype: float64,
     85: 一天    78.972573
     身边     5.469165
     永远     4.773289
     一年     4.286105
     知道     4.104991
     一点     3.847154
     时间     3.637921
     没      3.611523
     世界     3.581992
     终于     3.479066
     dtype: float64,
     86: 爱着     20.515171
     永远      1.881404
     我会      1.422533
     我爱你     1.164989
     相爱      1.014267
     无奈      0.960705
     疯狂      0.946150
     生命      0.876424
     浪花      0.820664
     起来      0.809596
     dtype: float64,
     87: 不要    82.340509
     问      5.583004
     太      5.049169
     知道     4.051989
     爱情     4.018636
     走      3.716858
     千万     3.428918
     世界     2.807493
     寂寞     2.750672
     不会     2.724738
     dtype: float64,
     88: 人生    35.579571
     呐      7.690440
     拥有     2.218181
     呒      2.118729
     朋友     2.022707
     不必     2.010059
     每个     1.863395
     世界     1.823764
     幸福     1.727403
     走      1.720491
     dtype: float64,
     89: 飞       42.970381
     世界       3.647861
     天空       3.102231
     一只       2.530343
     寻寻觅觅     2.098748
     风筝       2.062667
     美        1.941471
     温柔       1.886589
     疯狂       1.802074
     想要       1.760853
     dtype: float64,
     90: 很多     12.673781
     理由      1.221950
     受够了     0.786965
     需要      0.746360
     美梦      0.721167
     看到      0.720076
     没       0.690361
     多想      0.667631
     走走      0.624007
     觉得      0.615392
     dtype: float64,
     91: 之后    12.263116
     离开     1.548767
     真的     1.385600
     忙      1.319422
     确定     1.141860
     发现     1.141432
     爱情     1.101759
     转      1.096074
     角落     1.090335
     转身     1.051188
     dtype: float64,
     92: 瞬间    19.388312
     身边    15.358117
     世界     3.952877
     时间     3.593527
     远      2.905744
     永远     2.507213
     每个     2.083532
     留在     2.040310
     发现     2.015186
     幸福     1.926966
     dtype: float64,
     93: 见     22.541621
     一面     3.113227
     走      2.211913
     心里     2.041003
     没      1.848529
     围住     1.736322
     似      1.687255
     不再     1.663480
     每次     1.644012
     想见     1.621566
     dtype: float64,
     94: 妹妹     15.843577
     哥哥      2.741025
     美       1.410710
     往前走     1.395627
     干       1.145238
     天空      0.904101
     哭泣      0.827141
     俏       0.819818
     大胆      0.749348
     阿哥      0.676744
     dtype: float64,
     95: 夕阳    19.517611
     小溪     2.224795
     黄昏     2.205631
     美丽     1.698855
     问君     1.623479
     世界     1.566540
     飞      1.559461
     无限     1.553206
     远方     1.512016
     西沉     1.503025
     dtype: float64,
     96: 阳光     24.852096
     摩天轮     2.753512
     黑夜      2.500280
     溶化      2.368597
     灿烂      1.937141
     走       1.439333
     走进      1.414959
     花都      1.370435
     大道      1.308125
     小雨      1.303214
     dtype: float64,
     97: 青春    45.659857
     少年     3.215283
     梦      3.098154
     美丽     2.923149
     生命     2.673672
     时间     2.418155
     明天     2.344286
     世界     2.323930
     爱情     2.246466
     走过     2.170710
     dtype: float64,
     98: 等候     13.743139
     多久     12.791186
     地下铁     2.025421
     梦       1.887583
     走       1.861535
     承诺      1.830253
     温柔      1.771551
     伤口      1.663993
     不要      1.446030
     寂寞      1.444902
     dtype: float64,
     99: 跳      26.142897
     舞      10.421926
     一支      2.229328
     舞步      1.996172
     笑       1.577955
     快       1.401650
     恰恰      1.399056
     热辣辣     1.348085
     起舞      1.341437
     舞蹈      1.203261
     dtype: float64,
     100: 今夜    38.336613
     搁      2.962073
     心      2.645473
     不会     2.599562
     离开     2.166467
     梦      2.137128
     暝      1.743463
     温柔     1.690602
     知道     1.636582
     已经     1.579698
     dtype: float64,
     101: 啊啊啊    18.232444
     成全      1.064268
     愿意      1.026041
     一步      0.940285
     冤家      0.767121
     离开      0.723936
     抓       0.698536
     玩       0.675132
     听说      0.628357
     阮       0.594851
     dtype: float64,
     102: 小姐     12.027761
     先生      1.661040
     口音      1.093697
     希望      1.078302
     顽童      0.785857
     帮       0.767138
     讲       0.751178
     绿灯      0.710098
     女朋友     0.668724
     逗       0.662031
     dtype: float64,
     103: 孩子    23.485461
     长大     2.654641
     城市     1.903796
     样子     1.676340
     世界     1.389713
     走      1.294544
     母亲     1.251847
     秋天     1.176379
     容易     1.071947
     梦想     1.062672
     dtype: float64,
     104: 懂     43.296954
     温柔     2.954688
     太      2.803783
     梦      2.784012
     知道     2.673991
     世界     2.574730
     时间     2.329808
     感动     2.281786
     寂寞     2.220199
     心痛     2.132046
     dtype: float64,
     105: 傻     22.387366
     傻瓜     4.221562
     太      2.595957
     真的     2.219322
     没      1.872781
     笑      1.569062
     傻傻     1.531670
     笨      1.420609
     男人     1.370859
     一句     1.256760
     dtype: float64,
     106: 夏天    23.395806
     那年     2.051635
     一天     1.860659
     过去     1.707323
     永远     1.635967
     时间     1.540958
     去年     1.338135
     昨天     1.286902
     阳光     1.263470
     脸      1.196722
     dtype: float64,
     107: 地球    31.724065
     烦恼     5.076545
     噗通     4.393496
     世界     3.632110
     月球     3.484460
     甩掉     3.044614
     火星     2.528648
     宇宙     2.513683
     跳      2.370291
     珍惜     2.245648
     dtype: float64,
     108: 男     75.071101
     女     70.148367
     合     17.333139
     男女     6.440573
     男合     4.510050
     爱情     2.983164
     女合     2.770254
     幸福     2.621557
     一生     2.193522
     永远     2.127194
     dtype: float64,
     109: 晚安    16.478459
     告别     1.980108
     早安     1.625284
     再见     1.184936
     每天     1.017765
     明天     0.960310
     不安     0.933123
     小熊     0.870872
     晚餐     0.844552
     难堪     0.803983
     dtype: float64,
     110: 对不起    17.114356
     我爱你     1.815311
     无法      1.375738
     没       1.289696
     当作      1.287097
     太       1.236259
     一句      1.148597
     不要      1.139376
     只能      1.074908
     爱情      1.014326
     dtype: float64,
     111: 弯弯    8.487044
     月儿    2.208661
     月光    1.999999
     溜溜    1.813845
     小河    0.882049
     悲     0.687745
     路     0.675119
     快     0.616768
     眨     0.605200
     幸福    0.595464
     dtype: float64,
     112: 一步     16.589964
     意外     15.497065
     未来      2.196866
     现在      2.043209
     走       1.942752
     手牵手     1.855279
     爱情      1.612996
     更改      1.502316
     幸福      1.434569
     一口      1.395639
     dtype: float64,
     113: 妳     24.802008
     我們     0.905580
     愛      0.755924
     知道     0.720606
     笑      0.684664
     风来     0.662025
     女      0.639900
     错      0.606801
     為      0.584892
     說      0.571368
     dtype: float64,
     114: 梦想    34.691914
     世界     3.794498
     翅膀     3.052120
     勇敢     2.955201
     实现     2.647987
     未来     2.282607
     天空     2.202944
     现在     2.091154
     心里     2.084563
     相信     2.046043
     dtype: float64,
     115: 未来    31.825381
     现在     4.447701
     过去     3.588480
     世界     3.018581
     空白     1.923765
     我要     1.721457
     心      1.462559
     希望     1.452118
     无奈     1.448624
     爱情     1.434804
     dtype: float64,
     116: 依依      4.278954
     可能      4.276504
     深深      3.828115
     杜鹃      2.894386
     几许      2.684766
     天上人间    2.542692
     高呼      2.230397
     恩怨      1.849672
     聚       1.690031
     往事      1.532779
     dtype: float64,
     117: 跳舞     18.867215
     舞步      1.768030
     节奏      1.415684
     孤独      1.361116
     继续      1.314569
     想象      1.129903
     独自      1.065726
     路       1.027429
     需要      1.007806
     带我去     0.853835
     dtype: float64,
     118: 爱爱爱    7.596367
     准备     5.479248
     机会     4.583109
     全力     4.187328
     欧欧     3.187940
     战斗     1.858061
     一脚     1.343212
     爱看     1.325019
     潮流     1.207643
     没      1.200202
     dtype: float64,
     119: 仍然    29.510897
     未      4.694357
     似      2.519063
     无奈     2.506550
     永远     2.201341
     继续     2.151748
     不可     2.127494
     从前     2.075051
     过去     2.034834
     完全     2.014157
     dtype: float64,
     120: 昨夜    16.725508
     梦中     2.402343
     梦      1.971912
     记得     1.387546
     真      1.291971
     风雨     1.115024
     依旧     1.057145
     回首     0.950331
     夜半     0.916174
     依然     0.913967
     dtype: float64,
     121: 相思    38.139623
     悠悠     3.021122
     落花     2.783714
     梦      2.763599
     愁      2.719569
     恨      2.606521
     长江     2.485519
     一曲     2.470363
     长流     2.349047
     泪      2.190654
     dtype: float64,
     122: 心中    38.664408
     爱念     3.420182
     令      3.361406
     心      3.008881
     身边     2.789056
     怀念     2.749532
     知道     2.558586
     轻轻     2.546294
     我心     2.409421
     感觉     2.362258
     dtype: float64,
     123: 花      21.442910
     转      10.251134
     开       4.000991
     田里      1.801422
     犯了错     1.729544
     离开      1.565541
     桃花      1.555970
     救赎      1.411175
     过路      1.396471
     遍地      1.345706
     dtype: float64,
     124: 或许    34.328339
     已经     2.073213
     选择     2.072009
     是否     2.046009
     也许     1.998006
     太      1.940488
     一天     1.894006
     生命     1.765499
     没      1.735561
     永远     1.702839
     dtype: float64,
     125: 毋     9.331466
     伫     9.262947
     阁     4.727300
     袂     3.980927
     拢     3.496618
     欲     3.052420
     佮     2.741494
     仔     2.330938
     讲     1.958260
     毋知    1.812489
     dtype: float64,
     126: 宝贝    21.183644
     世界     1.924136
     不要     1.590610
     陪      1.320950
     亲爱     1.169840
     老爸     1.085792
     爸爸     1.055838
     不会     1.031242
     一天     0.990918
     眼泪     0.929908
     dtype: float64,
     127: 不会      44.357946
     知道       4.564184
     现在       3.563742
     三天三夜     2.968865
     走        2.964473
     永远       2.827280
     心        2.785336
     难过       2.658569
     害怕       2.466785
     太        2.433026
     dtype: float64,
     128: 流浪    27.370627
     远方     3.463983
     地方     3.192349
     身旁     1.905923
     故乡     1.776121
     时光     1.612247
     悲伤     1.600977
     独自     1.585676
     心      1.584173
     梦      1.515114
     dtype: float64,
     129: 我爱你     55.562424
     月亮代表     3.884280
     心        3.014540
     有多深      2.909176
     知道       2.704097
     一句       2.394005
     几分       2.269803
     永远       2.140506
     吻        2.080755
     问        1.957658
     dtype: float64,
     130: 旷野    8.392468
     知否    1.935036
     此心    1.032202
     车     0.983257
     天空    0.946017
     彩虹    0.944366
     空旷    0.944270
     怒放    0.886700
     生命    0.882814
     野花    0.882090
     dtype: float64,
     131: 永远    47.709902
     不够     3.268775
     不会     2.893418
     怀念     2.698285
     世界     2.626647
     忘记     2.504490
     遥远     2.154791
     需要     2.117486
     消失     2.066478
     身边     2.063921
     dtype: float64,
     132: 恋爱    43.234959
     感觉     2.694838
     爱情     2.542133
     不会     2.452659
     没      2.433782
     喜欢     2.300772
     不要     2.151756
     心情     2.151119
     浪漫     2.139661
     怕      2.074770
     dtype: float64,
     133: 星星    24.628856
     天上     2.889205
     一颗     2.561652
     夜里     2.344565
     星      2.221075
     眼睛     2.104308
     闪烁     2.068374
     夜空     1.796085
     美丽     1.457936
     知道     1.386551
     dtype: float64,
     134: 亲爱    51.364371
     永远     3.067822
     朋友     2.403604
     不要     2.360889
     想要     2.189542
     小孩     2.038317
     世界     2.014140
     时间     2.002348
     是否     1.830886
     走      1.785024
     dtype: float64,
     135: 热爱     18.837551
     温馨      2.442692
     天际      2.110967
     始终      1.603274
     身体      1.488875
     愿       1.438860
     一生      1.408582
     讲不出     1.371344
     怀念      1.370226
     美丽      1.235675
     dtype: float64,
     136: 快      32.768899
     疯       3.011006
     急       2.299368
     世界      1.997353
     双截棍     1.817472
     赶走      1.745791
     知道      1.690723
     寂寞      1.635760
     干什么     1.477628
     呼吸      1.439372
     dtype: float64,
     137: 起点    11.823679
     回到     3.219375
     终点     2.774722
     明天     1.442705
     世界     1.418481
     学会     1.369415
     不见     1.193581
     走      1.174147
     幸福     1.135318
     永远     1.111153
     dtype: float64,
     138: 不想    46.993016
     走      2.610720
     没      2.515803
     现在     2.437528
     伤      2.298728
     乱      2.071518
     知道     2.030410
     怀疑     2.002103
     世界     1.954191
     失去     1.914535
     dtype: float64,
     139: 改变    18.419437
     相见    12.564716
     不变    10.611781
     远      3.149085
     一年     3.119869
     心中     2.682965
     永远     2.558997
     一遍     2.552538
     世界     2.427206
     脸      2.292487
     dtype: float64,
     140: 吹     38.503784
     风      6.981441
     春风     5.301237
     轻轻     3.571310
     风吹     2.758719
     海风     2.349478
     梦      2.254927
     吹风     2.078274
     飞      1.967257
     走      1.910607
     dtype: float64,
     141: 梦     48.641058
     一场     6.157833
     就让     2.851371
     心里     2.647167
     走      2.619491
     寂寞     2.492407
     红尘     2.222056
     世界     2.180789
     心      2.137950
     洞      2.001598
     dtype: float64,
     142: 地方    44.098548
     遥远     3.694642
     遗忘     3.321057
     模样     3.033310
     方向     2.833280
     远      2.622177
     知道     2.611858
     离开     2.446465
     希望     2.307782
     悲伤     2.257670
     dtype: float64,
     143: 唔    25.020325
     佢     9.890327
     系     7.034459
     咁     5.165911
     话     4.838403
     乜     3.861381
     揾     2.973581
     咪     2.463526
     惊     2.414874
     讲     2.233063
     dtype: float64,
     144: 暖     15.789971
     心中     1.300538
     围巾     1.036598
     阳光     1.019651
     默默     1.017350
     日记     0.994392
     心      0.977050
     透      0.968213
     吻      0.919019
     爱意     0.901786
     dtype: float64,
     145: 昨日     16.664782
     记       3.107176
     揭谛      2.864594
     一如      1.693001
     够不够     1.679501
     记忆      1.546630
     走       1.308592
     清醒      1.292288
     回望      1.250261
     光阴      1.218395
     dtype: float64,
     146: 哒     24.692464
     滴答     0.606091
     流沙     0.595147
     听到     0.591287
     嘟      0.573405
     喔      0.569725
     说谎     0.523848
     真的     0.511048
     唔      0.472061
     爱情     0.453859
     dtype: float64,
     147: 哭     40.405429
     笑      6.662619
     不要     3.811727
     幸福     3.264838
     清楚     2.777370
     眼泪     2.361806
     怀里     2.282433
     走      2.270422
     不会     2.251855
     爱情     2.186476
     dtype: float64,
     148: 原来    58.203477
     幸福     3.753984
     需要     3.561694
     曾经     3.257654
     相信     3.256662
     女孩     3.213491
     世界     3.171283
     爱情     3.069846
     一天     2.999630
     明白     2.904499
     dtype: float64,
     149: 从来不    18.082141
     心里      1.918884
     不会      1.911433
     懂       1.576430
     片刻      1.532754
     知道      1.331228
     走       1.261481
     情意      1.180966
     一天      1.176971
     相信      1.156571
     dtype: float64,
     150: 唷     12.590229
     打拼     0.668382
     伊      0.649543
     晃      0.594976
     甲      0.576462
     阿哥     0.572145
     墓      0.559507
     云端     0.549755
     班      0.544740
     娜      0.544333
     dtype: float64,
     151: 嗨      19.741333
     呆       0.974423
     咿呀      0.898346
     喊       0.889232
     嗒       0.794202
     星期六     0.789431
     烧       0.772958
     抛开      0.752352
     烦恼      0.687250
     欧       0.682462
     dtype: float64,
     152: 恭喜      21.833431
     新年       7.533205
     发财       2.193852
     家家户户     2.177658
     拜年       2.121422
     祝        2.015863
     新年好      1.971862
     财神       1.580889
     一年       1.529007
     今年       1.378426
     dtype: float64,
     153: 告诉    53.469151
     不要     3.506947
     爱情     2.942615
     世界     2.833390
     不能     2.830002
     寂寞     2.611673
     心里     2.503053
     不会     2.350457
     温柔     2.345198
     知道     2.121177
     dtype: float64,
     154: 欲     27.176289
     讲      3.358302
     阮      3.084882
     甲      2.418725
     梦      2.006224
     拢      1.875477
     无人     1.560421
     人生     1.436260
     佗      1.427158
     暝      1.389905
     dtype: float64,
     155: 陪     45.608544
     走      5.853952
     身边     4.305911
     世界     3.207514
     幸福     3.053779
     时间     2.896524
     梦      2.884751
     身旁     2.642987
     最后     2.543359
     笑      2.541799
     dtype: float64,
     156: 生命    32.086866
     世界     3.722800
     风景     3.302278
     意义     2.162727
     永远     1.971135
     岁月     1.961345
     美丽     1.750340
     时间     1.730257
     幸福     1.721700
     需要     1.652474
     dtype: float64,
     157: 我会    42.318418
     知道     3.717790
     也许     2.839623
     相信     2.657021
     笑      2.622323
     努力     2.611369
     世界     2.601824
     永远     2.593569
     不会     2.559216
     离开     2.514212
     dtype: float64,
     158: 看着      25.940275
     眼睛       2.069623
     想要       2.015238
     像是       1.973937
     世界       1.886467
     喜欢       1.778769
     笑        1.675228
     可有可无     1.433029
     心        1.432701
     脸        1.430610
     dtype: float64,
     159: 忘     44.368676
     曾经     2.790315
     记得     2.772552
     忘记     2.667920
     问      2.262847
     付出     2.010397
     是否     2.004305
     幸福     1.959373
     爱情     1.937531
     不会     1.919350
     dtype: float64,
     160: 天使    25.073863
     翅膀     3.357504
     魔鬼     3.022133
     天堂     1.878254
     受伤     1.696251
     世界     1.593016
     太      1.578314
     美丽     1.487949
     飞      1.400648
     故事     1.395147
     dtype: float64,
     161: 噢     40.308463
     嫂子     3.623965
     凉      1.320424
     想要     1.251583
     喔      0.926745
     不要     0.891795
     呼唤     0.872858
     丢失     0.855281
     一直     0.826474
     天      0.807966
     dtype: float64,
     162: 忘记    51.297475
     心里     3.845387
     回忆     3.547622
     不能     3.270019
     记得     3.093819
     不会     3.053340
     过去     3.039067
     难      3.031258
     曾经     2.910766
     已经     2.650610
     dtype: float64,
     163: 誰     12.070074
     今日     2.279144
     私      1.817179
     気      1.408513
     夜      1.171877
     胸      1.019797
     說      1.008003
     君      0.930442
     行      0.927913
     堕      0.901246
     dtype: float64,
     164: 完整    12.473957
     温柔     2.205407
     疼      1.863975
     责任     1.693534
     永恒     1.546970
     可能     1.484258
     不能     1.473041
     心      1.466925
     幸福     1.191436
     世界     1.173533
     dtype: float64,
     165: 呦      17.873936
     嘿嘿嘿     1.293200
     咿       1.142850
     诶       0.848420
     丢       0.770682
     米       0.659775
     害羞      0.639417
     魔鬼      0.534877
     朋友      0.514647
     气球      0.510429
     dtype: float64,
     166: 不能    62.910479
     爱情     5.327297
     心      4.838530
     寂寞     4.281892
     心里     3.931865
     知道     3.805191
     世界     3.482033
     温柔     3.416488
     回到     3.303337
     无法     3.013700
     dtype: float64,
     167: 李      12.620961
     合       1.341487
     导演      1.046554
     陈       0.866415
     比较烦     0.806859
     天然      0.805600
     谭       0.794355
     太       0.595288
     古       0.586335
     是否      0.582171
     dtype: float64,
     168: 可爱     20.118221
     真       1.531395
     玫瑰花     1.461996
     美丽      1.299170
     爱是      1.147076
     一张      1.029995
     需要      1.009382
     小花      0.936890
     现在      0.917430
     温暖      0.917163
     dtype: float64,
     169: 不再    54.550868
     过去     4.013064
     不会     3.474433
     已经     3.400666
     是否     3.026851
     永远     3.012604
     记忆     2.996650
     心      2.981155
     生命     2.868835
     感觉     2.855224
     dtype: float64,
     170: 一直    43.940449
     走      3.400354
     世界     2.929367
     我会     2.439478
     永远     2.303054
     陪      2.279329
     心      2.172083
     不会     2.135830
     离开     2.089726
     时间     2.086879
     dtype: float64,
     171: 最后    33.725889
     一次     2.949449
     走      2.653335
     爱情     2.331895
     不要     2.129443
     武装     1.769005
     不到     1.728536
     太      1.693442
     一秒     1.598415
     温柔     1.557965
     dtype: float64,
     172: 马     4.620629
     光辉    2.418639
     一次    2.102088
     欢呼    1.669536
     爱心    1.461056
     山坡    1.376802
     党     1.372748
     黄     1.310782
     凭着    1.246050
     白马    1.144177
     dtype: float64,
     173: 一片    25.776223
     枫叶     4.448471
     找      1.957190
     爷爷     1.746908
     云      1.539394
     心      1.505184
     不见     1.429738
     一棵     1.384334
     海上     1.301141
     水      1.221520
     dtype: float64,
     174: 南无      6.111289
     阿弥陀佛    5.451813
     观世音     3.298129
     菩萨      3.229685
     摩诃      1.086842
     玲珑      0.987308
     众生      0.931223
     清净      0.751210
     怛       0.702379
     娑婆      0.605246
     dtype: float64,
     175: 缩      5.255886
     混      4.303812
     主唱     3.823154
     打击乐    2.347415
     键盘     2.286007
     参与     2.229323
     合成器    1.351050
     生长     1.110118
     合唱     1.091054
     黑暗     1.026407
     dtype: float64,
     176: 太     51.905462
     不想     4.398206
     不要     3.849239
     不会     3.805687
     认真     3.719055
     现在     3.391161
     没      3.310606
     需要     3.257390
     世界     3.237915
     怕      3.217919
     dtype: float64,
     177: 怪     28.119913
     太      3.138826
     不到     2.885641
     没      2.235137
     感情     1.997615
     找      1.887715
     可能     1.747249
     温柔     1.727531
     心      1.681261
     问      1.560075
     dtype: float64,
     178: 感谢      19.415847
     幸福       2.025641
     相遇       1.729965
     生命       1.446107
     人间烟火     1.361101
     世界       1.334789
     陪        1.293597
     变成       1.219996
     朋友       1.081433
     身边       1.053344
     dtype: float64,
     179: 一夜     15.677341
     冷冷的     1.900465
     最后      1.550940
     一天      1.195735
     哭       1.096653
     不会      1.067556
     心碎      1.033348
     知道      0.919952
     心       0.849995
     睡       0.826551
     dtype: float64,
     180: 流泪      16.781864
     心碎       4.676813
     伤悲       3.341810
     心醉       1.910940
     知道       1.851842
     演唱会      1.801304
     伤痕累累     1.789579
     无所谓      1.758719
     不会       1.547339
     后悔       1.506363
     dtype: float64,
     181: 喔     81.000109
     耶      2.318677
     世界     2.317961
     知道     2.122536
     走      2.033658
     心情     1.829833
     不能     1.807122
     没      1.729655
     不要     1.696905
     美丽     1.659407
     dtype: float64,
     182: 匆匆    22.639181
     相逢     2.867158
     梦      2.179120
     夜空     1.809332
     留下     1.740212
     是否     1.461172
     美梦     1.338794
     朦胧     1.338159
     似      1.283688
     流水     1.274533
     dtype: float64,
     183: 存在    36.730179
     期待     3.915779
     未来     3.902549
     明白     3.298236
     离开     2.492682
     依赖     2.161631
     证明     2.156079
     永远     1.845065
     尘埃     1.775409
     看见     1.746303
     dtype: float64,
     184: 太阳    31.807684
     阳光     2.669944
     升起     2.274357
     天亮     2.215581
     月亮     2.169675
     光芒     2.125427
     燃烧     1.744433
     世界     1.726350
     走      1.528213
     心中     1.454071
     dtype: float64,
     185: 爱情    90.771522
     幸福     5.481051
     心      5.266029
     不要     4.809926
     美丽     4.789193
     世界     4.668528
     没      4.608760
     回忆     4.061328
     温柔     4.025867
     找      3.973843
     dtype: float64,
     186: 生日快乐    10.000506
     祝        4.788122
     生日       0.714180
     蜡烛       0.628619
     若无其事     0.620776
     猪        0.593732
     平衡       0.473418
     我来       0.396977
     重生       0.392700
     幻灭       0.390405
     dtype: float64,
     187: 一点点     15.900919
     靠近       1.867953
     多一点点     1.285387
     动心       1.060263
     房间       0.992563
     缺        0.913576
     害怕       0.884628
     无尽       0.869869
     差        0.814028
     渐渐       0.764560
     dtype: float64,
     188: 雨     42.324762
     一场     3.460641
     回忆     2.827641
     伞      2.368542
     落下     2.120486
     想起     2.017080
     窗外     1.997785
     走      1.927352
     雨中     1.885753
     晴      1.853740
     dtype: float64,
     189: 心     70.358362
     一颗    10.155103
     爱情     4.340958
     是否     4.258582
     梦      4.244686
     过去     4.110234
     心里     4.058766
     知道     3.841794
     夜      3.685877
     眼睛     3.667660
     dtype: float64,
     190: 感觉    38.764144
     需要     2.988634
     这种     2.699669
     心      2.656307
     世界     2.356350
     知道     2.337124
     呼吸     2.200564
     空间     2.170166
     笑      2.043730
     寂寞     1.927168
     dtype: float64,
     191: 蝴蝶    10.951333
     开     10.769651
     花儿    10.002922
     玫瑰     9.137263
     阿飞     2.042722
     天涯     1.702489
     花      1.454410
     难得     1.375365
     风吹     1.336722
     已经     1.320947
     dtype: float64,
     192: 寂寞    52.255913
     心      3.378365
     承诺     2.779277
     不能     2.717610
     走      2.711996
     爱情     2.401886
     不要     2.262246
     心里     2.162002
     温柔     2.094849
     感觉     2.050972
     dtype: float64,
     193: 一生    54.450184
     一世     6.633160
     伴      4.253226
     从前     3.559132
     未      3.486552
     心      3.333638
     心中     3.243606
     令      3.209088
     永远     3.141407
     命运     2.986058
     dtype: float64,
     194: 英雄    31.831232
     江湖     3.304592
     梦      2.164105
     笑      1.787640
     心中     1.587690
     热血     1.465661
     天地     1.422554
     红颜     1.396303
     逍遥     1.373092
     知道     1.348227
     dtype: float64,
     195: 一朵    19.213932
     花      3.538365
     雅琪     3.188304
     哈萨     3.093719
     送      2.796920
     盛开     1.665875
     摘下     1.482719
     送给     1.304356
     粉红     1.261726
     小花     1.231139
     dtype: float64,
     196: 明日    21.852801
     几多    18.165945
     似      2.472850
     前面     2.013550
     昨天     1.996261
     愉快     1.842067
     今日     1.821107
     愿      1.774858
     今天     1.744045
     世间     1.562926
     dtype: float64,
     197: 伊     19.529795
     拢      2.390803
     伊讲     2.268041
     阮      1.964394
     惦      1.922238
     欲      1.709471
     搁      1.595974
     伊是     1.575802
     槟榔     1.543801
     暝      1.472545
     dtype: float64,
     198: 秋天    23.951973
     落叶     4.683174
     舍得     2.839661
     春天     2.556024
     难过     2.378872
     冬天     2.041344
     去年     1.685369
     秋风     1.659592
     走      1.606665
     单身     1.499323
     dtype: float64,
     199: 大海    20.987089
     滴答     6.094266
     女郎     2.957655
     滴      2.836570
     浪花     1.701335
     海      1.666806
     答      1.554960
     表现     1.530360
     世界     1.444547
     风浪     1.343178
     dtype: float64,
     200: 明明    22.205034
     听说     2.136535
     梦回     1.811622
     爱恨     1.676689
     台      1.541177
     两      1.475681
     偏偏     1.442365
     茫茫     1.281017
     心      1.266455
     问君     1.259401
     dtype: float64,
     201: 那天    25.008945
     这天     2.296615
     今天     1.792516
     相见     1.462051
     一天     1.426593
     你好     1.422666
     出现     1.347575
     相信     1.261850
     晚上     1.232521
     想起     1.229731
     dtype: float64,
     202: 错     43.751249
     不要     3.030273
     没      2.856919
     寂寞     2.511285
     痛      2.271017
     走      2.213743
     到底     2.187411
     恨      2.149281
     爱情     2.017887
     祸      2.005989
     dtype: float64,
     203: 面对     24.137992
     勇敢      2.422691
     现在      2.260209
     世界      2.229527
     太       1.830740
     过去      1.750859
     心酸      1.712429
     害怕      1.640581
     未来      1.612355
     白日梦     1.608331
     dtype: float64,
     204: 失恋    16.027244
     搬      1.083105
     可能     1.024627
     进入     0.927598
     眼泪     0.923473
     惨      0.901505
     不必     0.891723
     开心     0.874901
     一于     0.859774
     似      0.855790
     dtype: float64,
     205: 一定    25.298418
     要死     2.130626
     手里     1.834970
     保重     1.724766
     不会     1.517848
     保护     1.480513
     认识     1.397947
     走      1.178045
     成功     1.143869
     记得     1.095092
     dtype: float64,
     206: 想念    41.872647
     味道     2.947495
     回忆     2.884521
     知道     2.531117
     是否     2.160651
     世界     2.149256
     慢慢     2.144262
     思念     2.090027
     现在     2.057309
     真的     2.051430
     dtype: float64,
     207: 离开    39.541291
     不要     3.743896
     以后     3.374433
     不能     2.430011
     不会     2.204745
     回忆     1.948780
     走      1.853142
     不再     1.825115
     决定     1.666297
     无法     1.642880
     dtype: float64,
     208: 生活    37.093808
     幸福     3.052478
     城市     2.749469
     世界     2.684317
     时间     2.494105
     不要     1.998586
     喜欢     1.931624
     现在     1.931616
     永远     1.898742
     拥有     1.768658
     dtype: float64,
     209: 午夜    16.653079
     女子     2.194753
     城门     1.670373
     梦      1.432268
     百花     1.413684
     不敢     1.319290
     想着     1.242477
     留下     1.175047
     寂寞     1.173516
     深处     1.125081
     dtype: float64,
     210: 夜晚    27.688605
     漫长     1.929738
     寂寞     1.827470
     每个     1.781377
     时间     1.689090
     世界     1.575495
     今天     1.511717
     星星     1.407080
     温暖     1.387920
     需要     1.375710
     dtype: float64,
     211: 深爱    14.239185
     如今     1.313695
     曾经     1.091623
     幸福     1.015883
     没      1.004631
     世界     0.964447
     毕竟     0.945708
     现在     0.795385
     一生     0.783545
     伤心     0.777879
     dtype: float64,
     212: 盼望    18.459599
     墙      1.649468
     淡忘     1.649342
     吹干     1.572171
     身边     1.554674
     夜      1.486465
     缘      1.407632
     有泪     1.364975
     寒冬     1.355630
     幻想     1.346540
     dtype: float64,
     213: 道     13.969160
     涙      9.589055
     変      1.603247
     作曲     1.560719
     見      1.403816
     梨花     1.398156
     手      1.380664
     忘      1.344710
     胸      1.305998
     夢      1.206810
     dtype: float64,
     214: 我要    49.145658
     世界     3.162346
     不要     2.873962
     幸福     2.721975
     不能     2.148846
     走      2.085972
     现在     2.025692
     太      2.007243
     心      1.983437
     永远     1.928901
     dtype: float64,
     215: 爱人    35.070149
     欺骗     1.831393
     树枝     1.802683
     堂堂     1.754794
     生      1.703353
     眼睛     1.688818
     爱到     1.680466
     吻      1.679133
     肯定     1.580385
     男儿     1.487755
     dtype: float64,
     216: 记得    39.390927
     以后     3.911505
     是否     2.856785
     一天     2.674360
     永远     2.491545
     没      2.440851
     走      2.287309
     爱错     2.229917
     那年     2.119099
     找      2.046644
     dtype: float64,
     217: 孤独     36.973054
     幸福      4.285004
     寂寞      3.750431
     爱情      3.250883
     不在乎     2.407202
     没       2.377010
     走       2.305759
     不会      2.091136
     世界      2.038910
     自由      2.032721
     dtype: float64,
     218: 一次    56.147390
     最后     5.111548
     爱个     3.760704
     倔强     3.468235
     不能     3.239503
     心      2.871589
     爱情     2.616510
     够      2.607591
     寂寞     2.581976
     明白     2.579997
     dtype: float64,
     219: 简单    26.971154
     幸福     2.481258
     没      2.125324
     爱情     1.992927
     需要     1.854163
     希望     1.700336
     其实     1.653006
     平凡     1.528236
     生活     1.504757
     原来     1.498681
     dtype: float64,
     220: 月亮     26.434700
     星星      2.550289
     画上      2.110971
     亮       1.781175
     天上      1.716176
     圆       1.580755
     画       1.379717
     心上人     1.246187
     月光      1.227370
     世界      1.186803
     dtype: float64,
     221: 好好    36.385374
     不要     2.606369
     我会     2.587864
     世界     2.567390
     不能     2.108103
     时间     2.041107
     没      2.025569
     离开     1.963449
     知道     1.886937
     需要     1.819989
     dtype: float64,
     222: 美好    29.806709
     微笑     3.006497
     回忆     2.323059
     拥抱     2.169475
     世界     2.072267
     一刻     1.987852
     时间     1.980336
     一天     1.917619
     知道     1.833461
     笑      1.709789
     dtype: float64,
     223: 哎呀呀    13.850416
     呀呀      2.399765
     哗啦啦     1.288044
     宝贝      1.273302
     三人      0.989375
     最美      0.954114
     姐妹      0.846961
     一场      0.734576
     睡着      0.704499
     整个      0.699824
     dtype: float64,
     224: 一声    29.255814
     讲      3.876611
     问      2.125316
     轻轻     1.923827
     心      1.819269
     我心     1.668649
     难道     1.652879
     一句     1.599334
     叹息     1.497338
     钟情     1.459501
     dtype: float64,
     225: 白云     19.804699
     蓝天      2.868586
     奉献给     1.698757
     看见      1.578095
     天空      1.527752
     真有      1.427391
     问       1.347673
     伴       1.307785
     天上      1.307222
     眼前      1.279558
     dtype: float64,
     226: 明白    42.495046
     无奈     3.385702
     未来     2.923281
     终于     2.596576
     需要     2.593920
     太      2.550942
     等待     2.502052
     想要     2.479749
     分开     2.456536
     走      2.368325
     dtype: float64,
     227: 小小的    21.866467
     小小      1.857636
     城里      1.428782
     眼睛      1.415220
     地方      1.399257
     心       1.336754
     世界      1.331811
     吹进      1.199460
     很小      1.179761
     心愿      1.108284
     dtype: float64,
     228: 心跳    15.039540
     裙摆     2.359440
     呼吸     1.603843
     心动     1.525020
     摇摇     1.495493
     现在     1.253389
     一秒     1.230986
     合      1.178962
     味道     1.168311
     烦恼     1.159925
     dtype: float64,
     229: 舞台      9.413005
     总监      7.827457
     歌手      3.407991
     总导演     3.020161
     乐队      2.581560
     音响      2.306562
     声乐      2.303991
     盖世英雄    2.067459
     执行      2.037047
     指导      1.943917
     dtype: float64,
     230: 天天    29.605664
     身边     1.979339
     路上     1.806118
     永远     1.532533
     心      1.521531
     一生     1.482422
     日子     1.480328
     每天     1.414510
     今天     1.398627
     心里     1.390572
     dtype: float64,
     231: 后来    22.552467
     可惜     1.968923
     没      1.689509
     回忆     1.568209
     记得     1.557677
     明白     1.544434
     遇见     1.402334
     笑      1.395647
     错过     1.392717
     终于     1.299349
     dtype: float64,
     232: 世界    63.041576
     整个     4.712784
     美丽     3.631470
     无法     3.386602
     不会     3.371972
     改变     3.368424
     时间     3.133577
     需要     3.085302
     就让     3.030044
     心      2.954121
     dtype: float64,
     233: 来不及    26.688527
     一波      2.105482
     失去      1.745348
     回忆      1.551237
     泪       1.547720
     后悔      1.480710
     一次      1.450688
     眼泪      1.405607
     是否      1.369260
     事情      1.324620
     dtype: float64,
     234: 唯一    31.252851
     世界     2.903024
     明白     2.211306
     怀疑     2.205861
     永远     2.188565
     决定     2.093158
     心      1.831315
     不曾     1.739733
     不会     1.667583
     相信     1.627809
     dtype: float64,
     235: 冇     16.185964
     咁      4.404550
     乜      3.120944
     钱      2.651822
     晒      2.493356
     咪      2.084552
     啲      2.001300
     唔      1.953592
     佢      1.935586
     点解     1.697017
     dtype: float64,
     236: 放开    27.221589
     手      4.919294
     心      2.292626
     走      1.828087
     头脑     1.752199
     最后     1.715246
     记得     1.551455
     心中     1.486697
     主义     1.459590
     不朽     1.455771
     dtype: float64,
     237: 如果说    13.237715
     不会      1.792000
     谁又能     1.525855
     走       1.444973
     心里      1.443712
     十八岁     1.208021
     应该      1.196665
     相信      1.162497
     追着      1.110941
     男孩      1.103855
     dtype: float64,
     238: 追     32.099228
     重要     2.414466
     追赶     2.101868
     梦      1.719460
     目标     1.602847
     原来     1.506976
     飞      1.462706
     不要     1.380764
     不到     1.333566
     我会     1.308492
     dtype: float64,
     239: 故乡    18.277937
     仔      9.958187
     厝      2.718325
     拢      2.514451
     阮      2.024485
     西风     1.891704
     这多     1.798232
     搁      1.794207
     爸爸     1.709614
     楼      1.616611
     dtype: float64,
     240: 妈妈    35.865391
     爸爸     6.509377
     长大     2.227646
     不要     1.835429
     家      1.818598
     回家     1.751363
     大汉     1.466050
     娃娃     1.459626
     声音     1.422260
     咿      1.391958
     dtype: float64,
     241: 日子    27.202027
     冬季     2.571474
     未来     2.260741
     乐趣     1.975984
     岁月     1.950159
     美梦     1.766331
     相信     1.664046
     问      1.633008
     现在     1.625851
     风雨     1.556938
     dtype: float64,
     242: 摇      11.952921
     摇摇      1.952830
     摆       1.158851
     轻轻      1.111422
     飘       0.840141
     跳       0.835091
     娇       0.831266
     笑       0.794752
     梦乡      0.731441
     睡不着     0.697358
     dtype: float64,
     243: 主唱      11.875557
     统筹       7.140323
     键盘       6.973785
     企划       5.388097
     合成器      4.980718
     编辑       3.483344
     混母       3.180078
     声音       3.079959
     音频编辑     2.909028
     钢琴       2.470574
     dtype: float64,
     244: 敢     22.435586
     心肝     1.701934
     道歉     1.602677
     花谢     1.527285
     阮      1.452616
     惦      1.430627
     问      1.386583
     非要     1.339791
     呒      1.272841
     我敢     1.203003
     dtype: float64,
     245: 起来    33.104005
     站      2.226043
     飞      2.182987
     笑      2.085438
     掌      1.970164
     动      1.965886
     声响     1.812206
     跳      1.737789
     舞台     1.627573
     加大     1.619517
     dtype: float64,
     246: 遇见    23.297555
     爱情     2.203235
     听见     1.892790
     未来     1.679856
     美丽     1.504156
     从前     1.436894
     意外     1.403331
     人海     1.393775
     新      1.381572
     世界     1.379594
     dtype: float64,
     247: 原谅     29.539791
     不能      2.203690
     请原谅     1.891382
     太       1.657738
     笑       1.512889
     不会      1.472533
     承诺      1.439646
     不要      1.356093
     以后      1.300909
     记得      1.285167
     dtype: float64,
     248: 月光    32.256067
     身旁     2.606215
     海洋     2.180543
     轻轻     2.052461
     思念     1.989166
     梦      1.964628
     照亮     1.943872
     地方     1.870538
     变成     1.849320
     心      1.756499
     dtype: float64,
     249: 代价    8.818463
     走     2.489644
     痴心    2.486775
     黯然    2.200676
     长大    1.839281
     心碎    1.721947
     伤心    1.618760
     付出    1.533410
     挣扎    1.448886
     家     1.445035
     dtype: float64,
     250: 想要     47.663897
     知道      3.291107
     太       3.178012
     现在      2.697738
     问问      2.382229
     问       2.317124
     不要      2.294353
     没       2.158709
     飞       2.151625
     敢不敢     1.921831
     dtype: float64,
     251: 摇摆     17.764678
     不想      1.378216
     爱不爱     1.226647
     唱起歌     1.156670
     尽情      1.080646
     烦恼      1.079879
     不要      1.055893
     心里话     1.054282
     等待      1.038399
     身体      1.024717
     dtype: float64,
     252: 事      33.949269
     重要      2.537140
     没       2.257990
     一辈子     1.961296
     名字      1.783932
     解释      1.721098
     不要      1.669117
     每天      1.654070
     一次      1.635085
     无法      1.581255
     dtype: float64,
     253: 著     16.550929
     我們     7.143889
     讓      4.287219
     為      4.112749
     來      2.781340
     說      2.753327
     沒      2.545391
     對      1.882445
     後      1.631579
     會      1.369486
     dtype: float64,
     254: 不停     17.410773
     东西     11.335958
     旋转      2.168567
     背包      1.619343
     男人      1.530661
     美丽      1.518023
     世界      1.139053
     了不起     1.075503
     冰雪      1.071663
     身体      1.058210
     dtype: float64,
     255: 海边    14.663970
     海浪     1.747893
     沙滩     1.218817
     天边     1.145411
     无人     1.089090
     小岛     1.065646
     相交     1.040263
     海      1.034663
     一片     1.034580
     无聊     1.033631
     dtype: float64,
     256: 曰     15.271818
     何谓     0.840423
     君子     0.695688
     号      0.544492
     尝      0.509242
     三年     0.469300
     皆      0.453634
     名      0.437070
     吾      0.437031
     罪人     0.432034
     dtype: float64,
     257: 嘅     12.651823
     等待     1.926758
     永远     1.762932
     愿望     0.717582
     系      0.703842
     一刻     0.689194
     面上     0.656286
     喺      0.603387
     我话     0.593825
     爆发     0.586217
     dtype: float64,
     258: 痛快      16.928541
     值得       1.454667
     伤害       1.443801
     重来       1.367835
     心        1.233722
     需要       1.231520
     存在       1.195185
     痛        1.080580
     人间烟火     1.062041
     哭        1.006857
     dtype: float64,
     259: 情人    45.532854
     没      3.150173
     未      2.916080
     心      2.796669
     问      2.371996
     爱情     2.203092
     老      2.147088
     不要     2.090938
     不想     2.082791
     令      2.045492
     dtype: float64,
     260: 天堂    21.505826
     希望    17.712283
     地狱     4.270211
     地方     3.694027
     梦想     3.568496
     世界     2.351478
     幸福     2.028337
     翅膀     1.989771
     模样     1.895975
     飞翔     1.874104
     dtype: float64,
     261: 同一个    7.064265
     世界     3.313138
     姐妹     1.908400
     安静     1.602752
     走向     1.361248
     我敬     1.258895
     跟我来    1.153382
     谭      1.131996
     勇士     1.040722
     环游     1.016879
     dtype: float64,
     262: 拥抱    35.824740
     世界     3.496797
     一次     3.165183
     微笑     2.857420
     温柔     2.822313
     心跳     2.681595
     幸福     2.653582
     爱情     2.647912
     失去     2.642988
     以后     2.587913
     dtype: float64,
     263: 眼     17.088782
     脸      1.579259
     守候     1.396433
     谎言     1.362853
     双眼     1.283914
     巨龙     1.209579
     嘴      1.208352
     心      1.203208
     永远     1.190485
     不要     1.100319
     dtype: float64,
     264: 不必    40.931704
     太      2.705430
     问      2.579667
     不要     2.477302
     一生     2.364742
     何必     2.234754
     始终     2.188759
     怕      2.083543
     相信     2.055275
     世上     1.914458
     dtype: float64,
     265: 左手    9.602025
     右手    8.709231
     眠     1.056135
     纸     1.044407
     牵     0.700414
     师傅    0.696918
     举起    0.696702
     世界    0.693547
     悲     0.688888
     抓     0.676617
     dtype: float64,
     266: 依然    22.686200
     相信     2.383888
     明天     2.280864
     是否     1.632102
     继续     1.586258
     知道     1.405143
     记得     1.401524
     走      1.367118
     无法     1.349923
     早已     1.258632
     dtype: float64,
     267: 版权      14.274155
     归属       5.580785
     美术设计     4.267033
     平面       4.107692
     合唱       2.727601
     吉他手      2.702856
     封面       2.330794
     鼓手       2.176871
     键盘       2.149598
     摄影       1.575422
     dtype: float64,
     268: 习惯    35.285580
     慢慢     3.084148
     没      2.810014
     每天     2.147334
     知道     1.981175
     喜欢     1.971827
     夜      1.820879
     已经     1.774691
     世界     1.725625
     改变     1.710798
     dtype: float64,
     269: 跟着    24.534353
     节奏     4.199173
     走      2.122179
     感觉     1.632618
     我别     1.628894
     梦      1.302824
     不要     1.252332
     梦想     0.994633
     尽情     0.937978
     周末     0.920110
     dtype: float64,
     270: 干脆     6.140978
     多一些    5.695490
     呼呼     4.214983
     茧      2.325484
     怪兽     1.808655
     拉      1.112082
     蔓延     0.999522
     一点     0.890752
     难道     0.882584
     老爹     0.865902
     dtype: float64,
     271: 答案    17.031915
     问题    14.887801
     时间     1.326181
     需要     1.304111
     没      1.296676
     知道     1.220897
     今日     1.199144
     问      1.138964
     头壳     1.135378
     世界     1.127304
     dtype: float64,
     272: 其实    38.898421
     知道     3.316147
     心      2.295668
     已经     2.217981
     害怕     2.142761
     没      2.056067
     不要     2.009212
     懂      1.959058
     需要     1.916318
     不到     1.728414
     dtype: float64,
     273: 一半     24.657476
     另一半     1.983281
     留恋      1.544486
     畔       1.485113
     夜半      1.428063
     失去      1.351595
     勇敢      1.273276
     世界      1.084213
     有时候     1.044503
     故事      0.993239
     dtype: float64,
     274: 透明    18.679407
     心      1.937588
     眼睛     1.916943
     颜色     1.879723
     空气     1.750897
     看见     1.636752
     声音     1.617499
     世界     1.494024
     少女     1.449895
     雨      1.366257
     dtype: float64,
     275: 风     31.097038
     吻别     3.130549
     梦      2.911016
     心      2.683627
     冷得     2.079007
     风中     1.994353
     夏      1.841986
     心中     1.781226
     世界     1.732047
     等待     1.706997
     dtype: float64,
     276: 痛     21.934269
     懂      3.219337
     梦      2.379695
     不会     2.137190
     放手     2.076821
     走      2.042517
     累      1.938978
     心      1.839444
     痛痛     1.666605
     哭      1.638752
     dtype: float64,
     277: 踩      13.641184
     澎湖湾     2.981666
     旋转      2.422463
     碎       1.754085
     年岁      1.451793
     白雪      1.243752
     看不见     1.188939
     外婆      1.147014
     沙滩      1.103211
     一层      1.086238
     dtype: float64,
     278: 能够    37.964009
     心      3.093838
     回忆     3.031157
     希望     2.781534
     世界     2.698936
     梦      2.406098
     是否     2.396141
     温柔     2.247485
     到底     2.231139
     需要     2.113934
     dtype: float64,
     279: 自由    42.687877
     温柔     7.049833
     走      4.241473
     爱情     3.108798
     世界     2.937416
     不要     2.932731
     理由     2.927354
     寂寞     2.874528
     懂      2.831176
     太      2.821326
     dtype: float64,
     280: 姑娘    34.051794
     美丽     2.276266
     草原     2.052497
     少年     2.004663
     高山     1.998133
     摇      1.639319
     南方     1.615688
     一位     1.421331
     情郎     1.401847
     围着     1.395075
     dtype: float64,
     281: 烊      3.557464
     千玺     3.327799
     王源     3.176818
     王俊凯    3.046785
     易      2.195327
     合      0.921500
     朋友     0.543068
     告白     0.308434
     冒险     0.284614
     真心话    0.278327
     dtype: float64,
     282: 应该    29.991152
     明白     1.670827
     太      1.663356
     明知     1.576789
     世界     1.439973
     没      1.372413
     知道     1.294134
     突然     1.288396
     不应     1.209359
     放在     1.205184
     dtype: float64,
     283: 再见    48.488711
     一面     4.189772
     想念     2.676511
     离别     2.668216
     一天     2.312920
     永远     2.144375
     脸      2.110747
     时间     2.098106
     思念     1.998904
     不再     1.975783
     dtype: float64,
     284: 真心    29.103456
     真      2.480528
     不必     1.900466
     今天     1.803468
     心      1.739447
     回忆     1.682630
     心坎     1.592639
     永远     1.559398
     幸福     1.555754
     太      1.553728
     dtype: float64,
     285: 每个    30.440735
     夜      5.222456
     声音     2.883266
     夜里     2.863498
     世界     2.301274
     角落     2.133560
     心里     2.095137
     心      2.078008
     迷宫     1.935236
     梦      1.932909
     dtype: float64,
     286: 演唱    10.834656
     原唱     7.848085
     原      2.675129
     作曲     2.211849
     作词     2.061511
     键盘     1.531725
     歌曲     0.940317
     合音     0.928450
     小号     0.848318
     合唱     0.833954
     dtype: float64,
     287: 大桥    3.170069
     伦敦    3.048088
     誓言    2.074245
     旅客    1.110664
     西风    0.939674
     锋芒    0.939674
     在手    0.897847
     消磨    0.834776
     死     0.679436
     谎言    0.669651
     dtype: float64,
     288: 分手    37.447046
     走      4.466276
     难过     2.982247
     失去     2.946943
     也许     2.720329
     以后     2.519388
     是否     2.476755
     太      2.351375
     心      2.319610
     最后     2.301551
     dtype: float64,
     289: 有人    42.502774
     总      2.919039
     世界     1.885270
     真的     1.855803
     太美     1.788493
     陪      1.751969
     新      1.683951
     呼唤     1.625490
     啾      1.624913
     寂寞     1.509435
     dtype: float64,
     290: 拢     20.814536
     袂     13.329895
     阮      7.336187
     搁      5.422573
     甲      4.661754
     恁      4.127842
     讲      3.954647
     犹原     3.377370
     亲像     3.070869
     暝      2.991441
     dtype: float64,
     291: 至少    29.189293
     拥有     2.336992
     不能     2.213541
     真的     2.117095
     幸福     2.089863
     世界     1.980501
     不会     1.936635
     知道     1.890546
     感觉     1.830640
     失去     1.829134
     dtype: float64,
     292: 嘅    9.149443
     唔    6.127605
     咁    4.163893
     喺    3.252243
     嘢    3.185927
     哋    3.062762
     系    3.056125
     咗    2.581242
     佢    2.496563
     啲    2.025159
     dtype: float64,
     293: 帰     10.882836
     私      9.693531
     涙      4.809843
     愛      2.981203
     誰      2.123066
     作曲     1.840752
     遠      1.782388
     夢      1.764346
     顔      1.763283
     暮      1.593552
     dtype: float64,
     294: 狗     13.909876
     一只    12.064912
     走      1.947758
     咬      1.656266
     猫      1.635716
     鱼      1.463965
     牛      1.351987
     有人     1.142381
     鸟      1.118953
     变成     1.098095
     dtype: float64,
     295: 祝福    20.171337
     流水     5.893780
     默默     1.688269
     值得     1.656009
     回头     1.557613
     永远     1.525297
     手      1.504650
     幸福     1.498263
     回忆     1.376806
     心底     1.339276
     dtype: float64,
     296: 睡      32.647301
     不想      2.589121
     梦       2.204264
     睡不着     2.063274
     一觉      1.967528
     没       1.908421
     身边      1.891689
     明天      1.608442
     天黑      1.607841
     黑夜      1.599116
     dtype: float64,
     297: 秋雨    6.535382
     伊人    4.600149
     滴滴    3.755937
     春风    3.396383
     寻觅    2.507920
     丝丝    2.426814
     此情    1.535954
     飘飘    1.390593
     北     1.331733
     秋风    1.194954
     dtype: float64,
     298: 已经    29.297256
     伤心     9.763406
     过去     2.770225
     真正     2.474871
     回忆     2.405527
     无法     2.274590
     太      2.267165
     走      2.218199
     不会     2.097154
     决定     2.058172
     dtype: float64,
     299: 喵     12.159107
     猫      1.295642
     咩      0.608296
     听到     0.588836
     老鼠     0.542291
     爬      0.460422
     奇妙     0.446353
     放掉     0.427580
     学      0.412699
     有人     0.395504
     dtype: float64}




```python
### 60 seems like a good starting point. Rerun on 60 at 30% as before but with additional stop words

from numpy import unique
from numpy import where
from sklearn.datasets import make_classification
from sklearn.cluster import KMeans
from matplotlib import pyplot
# define dataset

# fit the tvec on the 35k rows of lyrics, will take include the 10k top features in terms of scores. 
# Higher scores go to words that have more disciminating power. I.e. those that appear frequently in some documents,
# but occur relatively less freqently overall. 
tvec = TfidfVectorizer(stop_words=updated_stopwords_3,max_df=0.6,max_features=10000,tokenizer=man_token_jie, norm='l2')
tvec.fit(lyrics)

# as we fit on all lyrics, and we are transoforming the 35k lyric rows into a dataframe, all 10k features will occur
# at least once. 
X_transformed = tvec.transform(lyrics)


# Consider X_transform each row is a vector, pointing to a position in 10k dimensional space.KMeans looks at these 
# points in 10k dimensional space and finds the 60 best centroids. 
from sklearn.cluster import KMeans
modelkmeans = KMeans(n_clusters=60, init='k-means++', n_init=100)
modelkmeans.fit(X_transformed)

# update the main dataframe so that labels can be seen
data['labels_60_splits_30%maxdf_v2'] = modelkmeans.labels_
# for each of the 60 labels, look at the words that were given the highest score in terms of their ability to distinguish


# from each of the other 2 labels. 
topics5 = {}
for i in range(60):
    topics5[i] = pd.DataFrame(tvec.transform(data[data['labels_60_splits_30%maxdf_v2']==i].lyrics).toarray(), columns =tvec.get_feature_names()).sum().sort_values(ascending=False)[:10]
    
    
```


```python
topics5
```




    {0: 就让    57.922905
     需要    47.027987
     爱     12.541702
     走      6.594882
     感觉     6.428905
     时间     6.057105
     世界     6.016837
     心      6.001608
     知道     5.368626
     不要     5.354108
     dtype: float64,
     1: 爱     55.883207
     美丽    50.192413
     太阳    36.147004
     生命    35.831202
     快     34.358965
     跳     34.189928
     人生    33.587352
     走     32.350160
     世界    32.245762
     吃     31.679490
     dtype: float64,
     2: 心     116.723732
     爱      23.842606
     一颗     21.163851
     是否      9.180640
     不会      8.164903
     走       8.087759
     眼睛      7.696285
     寂寞      7.652282
     知道      7.057573
     感觉      6.912302
     dtype: float64,
     3: 再见    74.536962
     爱      7.405750
     脸      6.494899
     一天     5.076347
     走      4.838324
     想念     4.661172
     一面     4.644668
     告别     4.595723
     相见     4.497887
     永远     4.473849
     dtype: float64,
     4: 忘记    78.826656
     爱      9.753412
     回忆     7.777946
     忘      7.068006
     不会     6.647859
     已经     6.197136
     心里     5.807480
     放弃     5.717349
     没      5.625655
     过去     5.595723
     dtype: float64,
     5: 我爱你     69.065770
     爱       14.561455
     知道       4.548720
     月亮代表     3.872034
     心        3.830186
     一句       3.374348
     对不起      3.153177
     有多深      2.902883
     世界       2.791020
     爱情       2.714843
     dtype: float64,
     6: 记得    84.398915
     爱     12.760570
     是否    10.110355
     忘      7.185694
     不会     6.853699
     以后     6.565027
     走      6.038615
     笑      5.622654
     永远     5.551600
     世界     5.401324
     dtype: float64,
     7: 喔     94.594928
     爱      7.104956
     知道     2.853963
     走      2.843362
     世界     2.694669
     现在     2.662905
     耶      2.624578
     不能     2.355904
     太      2.283168
     不要     2.252834
     dtype: float64,
     8: 今天    102.498004
     明天     12.597309
     昨天     12.327820
     爱      10.898647
     当天      8.946303
     走       7.179747
     从前      7.164821
     一天      6.949236
     没       6.763781
     心中      6.745929
     dtype: float64,
     9: 生活    80.005321
     世界     8.081912
     爱      7.205163
     总是     7.126427
     走      6.868274
     时间     6.816299
     寂寞     6.062722
     生命     5.758829
     未来     5.462477
     城市     5.413453
     dtype: float64,
     10: 一次    95.210580
     爱     14.036744
     最后    12.023972
     心      6.500236
     爱情     5.487171
     不会     5.224805
     不能     5.157287
     拥抱     4.759299
     温柔     4.694264
     明白     4.596819
     dtype: float64,
     11: 走     92.841197
     爱     88.351083
     没     87.855069
     时间    66.961287
     不会    63.707338
     不想    51.629172
     寂寞    51.010934
     离开    48.419061
     现在    47.457151
     最后    42.773106
     dtype: float64,
     12: 爱情    152.590932
     爱      25.388984
     幸福     11.071396
     心       9.932775
     美丽      9.231297
     不要      9.082923
     相信      8.707650
     时间      8.436541
     没       8.394747
     世界      8.142820
     dtype: float64,
     13: 亲爱    64.168818
     爱      7.015293
     永远     3.456767
     走      3.090472
     不要     2.974703
     陪      2.851380
     宝贝     2.706187
     时间     2.661840
     爱情     2.640877
     世界     2.636382
     dtype: float64,
     14: 越     81.010686
     爱     11.755226
     世界     5.377812
     寂寞     4.712883
     走      4.649538
     幸福     4.228498
     没      4.079428
     不会     4.078388
     笑      3.856373
     哭      3.725191
     dtype: float64,
     15: 一天    127.951824
     爱      16.060774
     身边     11.570484
     世界      8.552196
     永远      7.779560
     时间      7.444257
     我会      7.394356
     一点      7.342725
     没       6.857680
     终于      6.624611
     dtype: float64,
     16: 玫瑰     30.684209
     爱       3.160934
     盛开      2.931177
     蝴蝶      2.892590
     开       2.643571
     一朵      2.570854
     朵玫瑰     2.158975
     阿飞      2.032229
     依偎      1.750418
     花       1.619130
     dtype: float64,
     17: 自由    79.777670
     爱     13.286828
     走     11.796332
     温柔     9.351154
     寂寞     7.208765
     天空     6.700091
     世界     6.519033
     懂      5.743958
     放手     5.664873
     心      5.571466
     dtype: float64,
     18: 未     76.257014
     爱     72.621536
     便     60.886266
     令     58.762100
     似     52.665909
     没     48.172978
     讲     47.216184
     其实    45.909701
     没法    45.836855
     仍然    44.821380
     dtype: float64,
     19: 慢慢    74.428451
     爱      8.393695
     时间     6.298224
     走      5.849957
     温柔     5.471010
     习惯     5.389954
     回忆     4.886651
     现在     4.852401
     心      4.829643
     过去     4.535404
     dtype: float64,
     20: 女人    62.179193
     男人    20.359047
     爱     10.163105
     爱情     4.261426
     心      3.374303
     太      2.965516
     美丽     2.897497
     一生     2.765721
     每个     2.592003
     总是     2.559970
     dtype: float64,
     21: 恭喜      22.793824
     新年       7.873174
     拜年       2.579150
     发财       2.196952
     家家户户     2.187053
     祝        2.017646
     新年好      1.979036
     财神       1.580850
     一年       1.531417
     今年       1.454276
     dtype: float64,
     22: 太     116.483909
     爱      27.448614
     不会     10.832705
     没      10.519021
     不想      8.686518
     世界      8.595606
     知道      8.304731
     相信      8.156484
     明白      7.889601
     笑       7.875988
     dtype: float64,
     23: 愿     85.661046
     爱     11.606516
     一生     7.866645
     心中     6.556150
     永远     5.967540
     愿意     5.695646
     我愿     5.484014
     似      5.277849
     美丽     5.211294
     笑      5.025165
     dtype: float64,
     24: 一片    60.698850
     天空     6.478605
     爱      6.456521
     走      5.306383
     世界     4.679446
     心      4.335876
     云      4.189343
     枫叶     4.013882
     眼前     3.915864
     心中     3.700309
     dtype: float64,
     25: 阮     78.995871
     甲     10.489511
     拢      9.052297
     讲      8.176479
     搁      7.429034
     袂      6.412803
     不知     6.258212
     孤单     6.155270
     心情     5.952653
     心肝     5.718505
     dtype: float64,
     26: 家       47.519695
     爱        4.716990
     走        4.630056
     远方       3.971207
     长大       3.757835
     路上       3.732010
     温暖       3.350177
     花        3.186603
     回家       3.114440
     千千阙歌     3.101079
     dtype: float64,
     27: 噢     45.939095
     嫂子     3.623965
     爱      2.490439
     狗      1.685838
     不要     1.433798
     凉      1.320389
     想要     1.319103
     喔      1.210031
     故乡     1.153501
     爸爸     1.148988
     dtype: float64,
     28: 爱     388.197979
     幸福     25.134617
     不会     24.097599
     心      23.305893
     明白     22.940179
     爱情     21.128512
     相信     20.622599
     离开     20.597619
     走      19.990396
     寂寞     19.978553
     dtype: float64,
     29: 我要    84.790501
     爱     13.826785
     世界     5.579619
     不要     4.970436
     心      4.970128
     走      4.833605
     幸福     4.685681
     飞      4.517534
     回来     4.108005
     永远     3.699420
     dtype: float64,
     30: 梦     132.112831
     爱      16.885680
     一场     15.378360
     走      12.495916
     寂寞     11.211547
     心      10.819795
     天空     10.807445
     心中     10.577987
     懂       9.876324
     世界      9.225981
     dtype: float64,
     31: 知道    150.296016
     爱      25.087192
     不会     13.988246
     已经      9.978151
     真的      9.329916
     走       9.226613
     不要      8.878325
     心里      8.637764
     其实      8.411694
     相信      8.042167
     dtype: float64,
     32: 喜欢    96.037021
     爱      9.050218
     感觉     5.755118
     太      4.282681
     世界     4.227840
     知道     3.814244
     没      3.785034
     爱情     3.771488
     其实     3.671089
     习惯     3.540737
     dtype: float64,
     33: 中国     19.261541
     女孩      7.283983
     过来      1.941464
     男孩      1.822725
     新       1.818631
     对面      1.438747
     左       1.361678
     爱       1.350090
     五千年     1.231297
     右       1.161244
     dtype: float64,
     34: 夏天    31.327417
     秋天    30.192756
     冬天     7.600852
     春天     7.267738
     落叶     5.695787
     爱      3.874082
     时间     3.041226
     那年     2.846474
     一直     2.661057
     季节     2.643259
     dtype: float64,
     35: 不再    86.817925
     爱     15.623718
     过去     7.876327
     不会     6.467756
     是否     5.896065
     已经     5.662063
     回忆     5.618501
     离开     5.160779
     走      4.770720
     想起     4.636244
     dtype: float64,
     36: 不要    175.552398
     爱      27.328850
     问      14.278554
     走      11.453085
     太      10.498640
     知道      9.704964
     离开      9.077062
     没       8.355154
     告诉      7.946533
     心       7.826966
     dtype: float64,
     37: 不能    105.435816
     爱      19.858946
     寂寞      9.320111
     无法      8.271802
     走       7.908535
     离开      7.857461
     温柔      6.889172
     爱情      6.861191
     知道      6.758698
     心       6.597110
     dtype: float64,
     38: 拢     30.705206
     伊     27.165424
     欲     21.732792
     讲     19.802629
     袂     17.406093
     搁     17.283572
     仔     16.905608
     暝     15.857018
     心肝    13.530286
     伫     12.392756
     dtype: float64,
     39: 曾经    83.848211
     爱     13.865129
     回忆    10.542888
     现在     9.301826
     过去     8.789634
     爱过     8.603535
     如今     8.345214
     走      8.263095
     已经     7.880656
     忘      7.145185
     dtype: float64,
     40: 一生    103.668055
     爱      22.875645
     心中      9.368857
     一世      9.238908
     心       8.089215
     永远      6.685931
     陪       6.551015
     走       6.369757
     伴       6.323459
     没       6.257629
     dtype: float64,
     41: 吹     34.996925
     花     31.029166
     相思    29.337114
     笑     23.534022
     人间    23.073764
     春风    21.560902
     风     20.942439
     天涯    18.572458
     开     18.555959
     问     17.774677
     dtype: float64,
     42: 哒       25.574190
     版权      11.807231
     归属       4.562249
     美术设计     3.315615
     平面       3.191802
     吉他手      2.702856
     鼓手       2.176871
     键盘       2.149598
     合唱       1.910006
     手        1.675084
     dtype: float64,
     43: 地方    81.041626
     流浪     8.459547
     天堂     7.403089
     希望     7.330107
     走      7.275763
     爱      7.054633
     梦想     6.921236
     模样     6.786944
     身旁     6.519559
     离开     5.998968
     dtype: float64,
     44: 妈妈    41.443787
     回家    25.020876
     爸爸    14.006681
     今天     3.349476
     长大     3.141395
     我会     2.963987
     不要     2.944530
     家      2.850002
     爱      2.212869
     娃娃     2.211763
     dtype: float64,
     45: 想要    94.584210
     爱     14.222683
     世界     6.915539
     不要     6.657292
     知道     6.615353
     现在     6.596894
     走      6.541811
     飞      6.450189
     太      5.937274
     没      5.705293
     dtype: float64,
     46: 难     67.773338
     爱     12.283605
     忘记     5.106837
     明白     4.733772
     相思     4.325267
     走      4.253706
     心      4.230040
     未      4.140865
     恨      4.078322
     原来     4.050353
     dtype: float64,
     47: 眼泪    84.075931
     爱     15.226998
     掉      9.365199
     哭      8.025640
     心里     7.279558
     幸福     7.194872
     不会     6.618006
     心碎     6.586973
     流下     6.338061
     爱情     6.198360
     dtype: float64,
     48: 呀呀     14.428658
     哎呀呀     2.406493
     哗啦啦     1.288044
     宝贝      1.158901
     爱       1.148618
     最美      0.954114
     唉呀      0.739853
     个人      0.649167
     知道      0.642551
     情人      0.613483
     dtype: float64,
     49: 気    20.511183
     見    20.273477
     涙    18.088321
     誰    16.526170
     変    14.206112
     君    13.296198
     私    13.192918
     帰    11.547181
     曰    11.469442
     顔    10.723375
     dtype: float64,
     50: 男     110.705863
     女     102.555691
     合      31.237493
     爱      13.053767
     男女     11.962738
     男合     10.006109
     女合      5.825438
     爱情      5.474891
     没       4.725868
     合女      4.383824
     dtype: float64,
     51: 真的    92.789070
     爱     23.014330
     走      8.485898
     不会     7.695262
     是否     7.648292
     知道     7.310219
     心      6.740324
     没      6.015655
     不想     5.970698
     懂      5.687608
     dtype: float64,
     52: 朋友    82.427390
     走      7.095932
     爱      6.055834
     不要     5.446618
     寂寞     4.236339
     陪      4.145531
     不会     3.990901
     情人     3.820984
     没      3.742980
     人生     3.706635
     dtype: float64,
     53: 也许    100.849713
     爱      11.788352
     不会      7.186707
     时间      6.332524
     走       6.180032
     懂       6.149746
     感觉      5.832321
     寂寞      5.681870
     以后      5.630736
     回忆      5.570378
     dtype: float64,
     54: 男女     74.535872
     女      20.537276
     男合     17.485863
     男      17.430761
     合      13.004519
     男合女     6.245699
     合女      6.017177
     女合      3.805053
     爱       3.351751
     合合      2.486575
     dtype: float64,
     55: 一辈子    33.316710
     爱       6.665763
     事       3.000619
     不会      2.757468
     时间      2.519499
     承诺      2.473200
     日子      2.317017
     世界      2.288307
     陪       2.124617
     幸福      2.007836
     dtype: float64,
     56: 雨     69.649490
     爱      6.406302
     一场     5.606706
     走      5.361992
     回忆     4.769989
     心      4.766158
     风      4.625915
     天空     4.001361
     停      3.672274
     窗外     3.453795
     dtype: float64,
     57: 永远    103.329331
     爱      21.279566
     不会     10.077325
     我会      8.320391
     世界      8.020677
     身边      7.393432
     美丽      6.305999
     等待      6.170917
     时间      6.161894
     心       6.125826
     dtype: float64,
     58: 世界    152.441254
     爱      19.391252
     时间     13.158823
     整个     11.468951
     改变     10.873239
     未来     10.810576
     美丽     10.757365
     宇宙      9.787239
     生命      9.701327
     陪       9.647946
     dtype: float64,
     59: 姑娘    38.551257
     美丽     2.702274
     高山     2.271211
     少年     2.227699
     情郎     2.189468
     草原     2.049045
     回头     1.815144
     一位     1.701776
     摇      1.639319
     南方     1.615688
     dtype: float64}




```python
data3 = df_tracks_master_clean3_lyrics_genre_hsk_v2[df_tracks_master_clean3_lyrics_genre_hsk_v2.hsk3 >= 0.9]
lyrics3 = data3.lyrics
```


```python
lyrics3.shape
```




    (97,)




```python
### 60 seems like a good starting point. Rerun on 60 at 30% as before but with additional stop words

from numpy import unique
from numpy import where
from sklearn.datasets import make_classification
from sklearn.cluster import KMeans
from matplotlib import pyplot
# define dataset

# fit the tvec on the 35k rows of lyrics, will take include the 10k top features in terms of scores. 
# Higher scores go to words that have more disciminating power. I.e. those that appear frequently in some documents,
# but occur relatively less freqently overall. 
cvec = CountVectorizer(stop_words=updated_stopwords_3,max_features=50000,tokenizer=man_token_jie)
cvec.fit(lyrics3)


```




    CountVectorizer(max_features=50000,
                    stop_words={'、', '。', '〈', '〉', '《', '》', '一', '一个', '一些', '一何',
                                '一切', '一则', '一方面', '一旦', '一来', '一样', '一种', '一般',
                                '一起', '一转眼', '一首', '一首歌', '七', '万一', '三', '上', '上下',
                                '下', '不', '不仅', ...},
                    tokenizer=<function man_token_jie at 0x7fdb0b4cc940>)




```python

HSK3_lyrics_matrix = pd.DataFrame(cvec.transform(lyrics3).toarray(),columns =cvec.get_feature_names() )
```


```python
pd.options.display.max_rows = None
HSK3_lyrics_matrix.sum().sort_values(ascending=True)
```




    特别        1
    台上        1
    右手        1
    吃         1
    这太远       1
    同年        1
    教         1
    穿过        1
    过生日       1
    放在        1
    听歌        1
    放下        1
    突然        1
    边缘        1
    整个        1
    换         1
    哭泣        1
    抱起        1
    唱祝        1
    啤酒        1
    找到        1
    笑笑        1
    路         1
    足够        1
    打来        1
    四季        1
    四次        1
    手机        1
    回不去       1
    站         1
    种树        1
    叫做        1
    只能        1
    几公斤       1
    春花秋月      1
    春有        1
    遮掩        1
    星星        1
    别去        1
    睡         1
    别问        1
    副         1
    选         1
    送走        1
    努力        1
    睡眠        1
    开口        1
    早教        1
    十二点       1
    种         1
    卖         1
    原唱        1
    参加        1
    远在天边      1
    旁边        1
    发起人       1
    发黄        1
    变         1
    变了样       1
    变得        1
    口         1
    只想        1
    回到        1
    凌晨        1
    回去        1
    回来        1
    忘不了       1
    寻找        1
    小宝宝       1
    小小        1
    必须        1
    小树        1
    脸         1
    心能        1
    心碎        1
    心理        1
    山丘        1
    自信        1
    工作室       1
    快点        1
    已干        1
    帅气        1
    菜         1
    带你去       1
    带给        1
    干些        1
    花儿        1
    并不知道      1
    幽暗        1
    很多歌       1
    自私        1
    废墟        1
    往常        1
    开         1
    巴掌        1
    思念        1
    孤独        1
    思量        1
    回答        1
    国家        1
    地图        1
    我来        1
    地问        1
    地面        1
    谁给我       1
    城         1
    说声        1
    外号        1
    感谢        1
    大峡谷       1
    感觉        1
    大衣        1
    大部分       1
    感应        1
    感动        1
    天天        1
    天气        1
    累         1
    愁         1
    约         1
    想太多       1
    好不好       1
    想像        1
    好好        1
    好开心       1
    姑娘        1
    背后        1
    回家        1
    凉         1
    早餐        1
    冷冷的       1
    飞雨        1
    飞机        1
    留下来       1
    不假        1
    风雨        1
    痛         1
    面是        1
    不可        1
    没想        1
    不宜        1
    不怪        1
    沙发        1
    洗手台       1
    静静的       1
    不点        1
    雨后        1
    每朵        1
    不知所措      1
    阵风        1
    间         1
    歌树        1
    丢         1
    歌曲        1
    两口        1
    两次        1
    严肃        1
    水有        1
    中间        1
    洗衣        1
    三点        1
    一下        1
    一件        1
    爸爸        1
    爱过        1
    生         1
    一场        1
    一天天       1
    照片        1
    一把        1
    热情        1
    烧饭        1
    一棵        1
    浪费        1
    点名        1
    饭         1
    演唱        1
    生日派对      1
    生气        1
    满天飞       1
    一遍        1
    一道        1
    湿湿的       1
    渡口        1
    七点        1
    三五个       1
    三次        1
    生命        1
    举起        1
    民谣        1
    你流        1
    有春有       1
    人一        1
    先生        1
    长高        1
    先唱        1
    长路        1
    今晚        1
    来晃        1
    有太多       1
    来时        1
    机会        1
    期待        1
    有如        1
    省         1
    错别        1
    钥匙        1
    像是        1
    依偎        1
    真人        1
    假         1
    侵袭        1
    关         1
    云         1
    五次        1
    酒瓶        1
    树         1
    二十几岁      1
    来种        1
    样子        1
    晚霞        1
    醒来        1
    买醉        1
    写歌        1
    事情        1
    早上        2
    能别        2
    早         2
    眼前        2
    爱上你       2
    背影        2
    满脸        2
    忘记        2
    开花        2
    晚         2
    忽然        2
    眼睁睁       2
    明天        2
    心酸        2
    时光        2
    熟悉        2
    照相        2
    很想        2
    自由        2
    最近        2
    真挚        2
    心爱        2
    热恋        2
    有天        2
    心声        2
    点点        2
    微风        2
    睁开        2
    有所        2
    笑一笑       2
    有点儿       2
    毕竟        2
    相逢        2
    第一次       2
    每夜        2
    放不下       2
    相见        2
    每一分       2
    段时间       2
    歪打正着      2
    树上        2
    笑脸        2
    收         2
    接过        2
    次         2
    承诺        2
    掌声        2
    梦里会       2
    眼眶        2
    相         2
    我尝        2
    我千纷       2
    总会        2
    生还        2
    无奈        2
    悲伤        2
    情景        2
    梦         2
    纵有        2
    想着        2
    深情        2
    画面        2
    意义        2
    相遇        2
    没见        2
    慢慢        2
    算了吧       2
    筷子        2
    我俩        2
    白桦林       2
    望穿        2
    开着        2
    一万次       2
    座位        2
    准备        2
    写给        2
    再见面       2
    醒         2
    里面        2
    先         2
    分不清       2
    信件        2
    保留        2
    他来        2
    从前        2
    闭起        2
    也许        2
    乞求        2
    信         2
    久         2
    别为        2
    别离        2
    周末        2
    过来        2
    听到        2
    向往        2
    吐露        2
    这是        2
    开得        2
    可不可以      2
    午夜        2
    千言        2
    千         2
    送给        2
    勇气        2
    刺不刺       2
    发现        2
    车流        2
    为啥        2
    两个        2
    一趟        2
    一言不发      2
    一秒钟       2
    一眼        2
    一百次       2
    香         2
    一辈子       2
    一杯        2
    高兴        2
    高高的       2
    鱼儿        2
    黑夜        2
    一双双       2
    一千次       2
    一条街       2
    中诉        2
    一面        2
    上帝        2
    问路        2
    不需        2
    不说        2
    不舍        2
    不知不觉      2
    不用说       2
    万语        2
    不敢        2
    非常        2
    不好意思      2
    风吹        2
    下雨        2
    下午        2
    上拿份       2
    不放        2
    哪一天       2
    遇到        2
    唱歌        2
    外面        2
    大事        2
    诉         2
    天知道       2
    讲情        2
    太坏        2
    太快        2
    好久        2
    记         2
    牵         2
    字         2
    说些        2
    孩子        2
    认识        2
    家         2
    要说        2
    小时候       2
    表示        2
    街角        2
    小说        2
    街上        2
    帮助        2
    床前        2
    废话        2
    安静        2
    说说话       2
    妈妈        2
    路途遥远      2
    谢花        2
    坚强        2
    越         2
    坐         2
    唱片        2
    越深        2
    见         3
    忘川        3
    飞别        3
    没错        3
    四天        3
    面前        3
    我别        3
    白         3
    小时        3
    斜阳        3
    季节        3
    手         3
    总         3
    相思        3
    不让        3
    同情        3
    白云        3
    飞飞飞       3
    恨         3
    立地成佛      3
    把头        3
    独自        3
    爱人        3
    花有        3
    轻摇        3
    鱼         3
    照顾        3
    深         3
    微微        3
    轻轻        3
    心         3
    山岗        3
    屋顶        3
    小鸟        3
    小路        3
    喔         3
    听见        3
    一定        3
    这能        3
    相知        3
    问题        3
    日落        3
    相许        3
    有没有       3
    坏         3
    做错        3
    送         3
    关系        3
    真心        3
    北斗星       3
    真诚        3
    天上飞       3
    诉说        3
    大街        3
    冷         3
    那日        3
    愿意        3
    离去        3
    你别        3
    以后        3
    真         3
    想成        3
    绑架        3
    好久不见      3
    根         3
    想换        3
    买         3
    记得        3
    之前        3
    介绍        3
    今生今世      3
    村庄        3
    头发        3
    长大        3
    终于        4
    老         4
    继续        4
    那忘川       4
    珍重再见      4
    甜         4
    路上        4
    眼泪        4
    看看        4
    眼望        4
    等于        4
    金钱        4
    眼         4
    高雄妹       4
    难         4
    自命不凡      4
    街灯        4
    需要        4
    身旁        4
    飞飞        4
    鼓楼        4
    再也        4
    段         4
    情场        4
    情怀        4
    想到        4
    想念        4
    吃糖        4
    有点        4
    有情        4
    双眼        4
    不愿        4
    有个        4
    友情        4
    最好        4
    北方        4
    挑战        4
    放心        4
    世上        4
    今夜        4
    新鲜        4
    出现        4
    不用        4
    每到        4
    其实        4
    不变        4
    天         4
    爱是        4
    一朵        4
    一条        4
    漂着        4
    漂亮        4
    一直        4
    小河        4
    川水        4
    爱着        4
    没试        4
    没人        4
    常见        4
    喝一杯       4
    得来不易      4
    黑空        5
    走进        5
    心头        5
    冬天        5
    回         5
    笑容        5
    告诉        5
    年轻        5
    容易        5
    心要        5
    等待        5
    改变        5
    坐在        5
    献         5
    难过        5
    真情        5
    看到        5
    下山        5
    看着        5
    月光        5
    下去        5
    不到        5
    男朋友       5
    三百块       5
    一颗        5
    一声        5
    现在        5
    身边        6
    相爱        6
    服务员       6
    来到        6
    明         6
    跳         6
    不再        6
    泪         6
    喝咖啡       6
    小熊        6
    心坎        6
    记川        6
    美丽        6
    过去        6
    时间        6
    话         6
    出去        6
    从今以后      6
    陪         6
    是否        6
    拥抱        6
    看不见       6
    找         6
    天亮        6
    不爱        6
    有人        6
    花         7
    好像        7
    听说        7
    来来去去      7
    你好        7
    要来        7
    看见        7
    表白        7
    是不是       7
    跳舞        7
    城市        7
    雨         7
    别以为       7
    水         7
    太阳        7
    那种        8
    远         8
    分开        8
    一句        8
    咖啡店       8
    说话        8
    懂         8
    夏天        8
    太         8
    记起        8
    眼看        8
    秋天        8
    生活        8
    电到        8
    害怕        8
    眼睛        8
    今天        9
    你家        9
    动物园       9
    面         9
    跟着        9
    情意        9
    相信        9
    觉得        9
    泰国        9
    北京        9
    不好        9
    昨天        9
    离开       10
    声音       10
    睡着       10
    晚安       10
    不知       10
    怪        10
    真的       10
    朋友       10
    每天       10
    一口       10
    不能       10
    晚上       11
    演算法      11
    日子       11
    那天       11
    再见       11
    担心       11
    打电话      12
    每个       12
    美好       12
    海边       12
    喝        12
    充满       12
    地方       13
    喜欢       13
    春天       13
    味道       14
    希望       14
    小小的      14
    没        14
    意中人      14
    转        14
    水仙花      15
    已经       15
    谢谢       15
    爱有       15
    走过       16
    不想       16
    我要       16
    明白       16
    回头       17
    笑        17
    心里       18
    应该       18
    忘        18
    祝        19
    我爱你      20
    了解       20
    不会       20
    向前走      21
    我会       21
    一天       22
    生日快乐     22
    一次       22
    不要       23
    错        23
    飞        24
    可爱       26
    世界       28
    哭        28
    咖啡       30
    加糖       35
    想要       36
    问        36
    走        43
    知道       80
    爱       109
    dtype: int64




```python
#### TO DO FIND CHARACTERS THAT ARENT IN HSK3,
#### THEN KEEP THESE AS FEATURES
#### THEN RUN CLASSIFIER BASED ON THIS SO THAT OUT OF HSK WORDS THAT OCCURS ACROSS MULTIPLE SONGS ARE GROUPED
for i in HSK3_lyrics_matrix.sum().sort_values(ascending=True):
    print(i)
```

    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    1
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    2
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    3
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    4
    5
    5
    5
    5
    5
    5
    5
    5
    5
    5
    5
    5
    5
    5
    5
    5
    5
    5
    5
    5
    5
    5
    5
    5
    5
    5
    5
    6
    6
    6
    6
    6
    6
    6
    6
    6
    6
    6
    6
    6
    6
    6
    6
    6
    6
    6
    6
    6
    6
    6
    6
    6
    6
    7
    7
    7
    7
    7
    7
    7
    7
    7
    7
    7
    7
    7
    7
    7
    8
    8
    8
    8
    8
    8
    8
    8
    8
    8
    8
    8
    8
    8
    8
    8
    9
    9
    9
    9
    9
    9
    9
    9
    9
    9
    9
    9
    10
    10
    10
    10
    10
    10
    10
    10
    10
    10
    10
    11
    11
    11
    11
    11
    11
    12
    12
    12
    12
    12
    12
    13
    13
    13
    14
    14
    14
    14
    14
    14
    15
    15
    15
    15
    16
    16
    16
    16
    17
    17
    18
    18
    18
    19
    20
    20
    20
    21
    21
    22
    22
    22
    23
    23
    24
    26
    28
    28
    30
    35
    36
    36
    43
    80
    109



```python
# as we fit on all lyrics, and we are transoforming the 35k lyric rows into a dataframe, all 10k features will occur
# at least once. 
X_transformed = cvec.transform(lyrics3)


# Consider X_transform each row is a vector, pointing to a position in 10k dimensional space.KMeans looks at these 
# points in 10k dimensional space and finds the 60 best centroids. 
from sklearn.cluster import KMeans
modelkmeans = KMeans(n_clusters=10, init='k-means++', n_init=100)
modelkmeans.fit(X_transformed)

# update the main dataframe so that labels can be seen
data3['labels'] = modelkmeans.labels_
# for each of the 60 labels, look at the words that were given the highest score in terms of their ability to distinguish


# from each of the other 2 labels. 
topics6 = {}
for i in range(10):
    topics6[i] = pd.DataFrame(cvec.transform(data3[data3['labels']==i].lyrics).toarray(), columns =cvec.get_feature_names()).sum().sort_values(ascending=False)[:40]
    
    
```




    {0: 走      20
     希望      5
     双眼      4
     你别      3
     远       2
     一次      2
     过去      2
     背影      2
     能别      2
     睁开      2
     闭起      2
     发现      2
     出现      2
     情景      2
     洗手台     0
     泰国      0
     洗衣      0
     一万次     0
     浪费      0
     泪       0
     深       0
     深情      0
     渡口      0
     湿湿的     0
     满天飞     0
     满脸      0
     漂亮      0
     漂着      0
     海边      0
     没错      0
     演算法     0
     没试      0
     段时间     0
     每一分     0
     每个      0
     每到      0
     每夜      0
     每天      0
     每朵      0
     毕竟      0
     dtype: int64,
     1: 生日快乐    22
     祝       19
     现在       2
     今天       1
     唱祝       1
     演唱       1
     最好       1
     笑        1
     过生日      1
     爸爸       1
     先唱       1
     朋友       1
     生日派对     1
     好开心      1
     参加       1
     我来       1
     沙发       0
     渡口       0
     浪费       0
     海边       0
     深        0
     每到       0
     深情       0
     每个       0
     湿湿的      0
     洗衣       0
     满天飞      0
     满脸       0
     漂亮       0
     漂着       0
     每一分      0
     每夜       0
     洗手台      0
     没        0
     没试       0
     没人       0
     水有       0
     没想       0
     没见       0
     水仙花      0
     dtype: int64,
     2: 加糖     35
     咖啡     30
     咖啡店     6
     服务员     6
     喝咖啡     6
     不想      4
     喝一杯     4
     吃糖      4
     甜       4
     一杯      2
     上拿份     2
     来到      2
     为啥      2
     坐       2
     之前      2
     非常      2
     座位      2
     他来      2
     我尝      2
     一口      2
     一句      2
     漂着      0
     点名      0
     深       0
     烧饭      0
     点点      0
     演算法     0
     演唱      0
     深情      0
     渡口      0
     湿湿的     0
     满天飞     0
     满脸      0
     漂亮      0
     洗手台     0
     海边      0
     没       0
     每天      0
     每朵      0
     毕竟      0
     dtype: int64,
     3: 知道      22
     想要      20
     谢谢      11
     演算法     11
     哭       10
     晚上       9
     那天       9
     声音       8
     那种       8
     地方       6
     陪        6
     真的       6
     美丽       5
     年轻       5
     害怕       5
     月光       5
     了解       4
     走        4
     路上       4
     身旁       4
     世上       4
     笑        3
     不让       3
     面        3
     心里       3
     我会       3
     想换       3
     微微       3
     鱼        3
     想成       3
     怪        2
     不要       2
     路途遥远     2
     总        2
     坚强       2
     安静       2
     说些       2
     觉得       2
     有人       2
     废话       2
     dtype: int64,
     4: 爱       16
     爱有      15
     知道      12
     美好      12
     不好       9
     拥抱       6
     爱是       4
     我爱你      3
     想要       2
     有所       2
     长大       2
     保留       2
     可不可以     2
     听到       2
     以后       2
     不好意思     2
     漂亮       0
     满天飞      0
     深        0
     深情       0
     渡口       0
     湿湿的      0
     点名       0
     点点       0
     漂着       0
     满脸       0
     演算法      0
     演唱       0
     海边       0
     一万次      0
     泰国       0
     浪费       0
     洗衣       0
     每到       0
     每夜       0
     每天       0
     每朵       0
     毕竟       0
     民谣       0
     水        0
     dtype: int64,
     5: 可爱     26
     我要      3
     帮助      2
     认识      2
     街上      2
     很想      2
     你好      2
     介绍      2
     愿意      2
     每个      2
     演算法     0
     深情      0
     洗手台     0
     洗衣      0
     浪费      0
     海边      0
     点名      0
     深       0
     湿湿的     0
     渡口      0
     演唱      0
     满天飞     0
     满脸      0
     漂亮      0
     泪       0
     漂着      0
     泰国      0
     一万次     0
     没错      0
     水       0
     每一分     0
     每到      0
     每夜      0
     每天      0
     每朵      0
     毕竟      0
     民谣      0
     水仙花     0
     没试      0
     水有      0
     dtype: int64,
     6: 爱      47
     知道     37
     飞      24
     一天     22
     向前走    21
     错      21
     走      19
     应该     18
     哭      18
     我会     17
     明白     16
     走过     16
     了解     16
     水仙花    15
     不要     15
     回头     15
     心里     15
     转      14
     意中人    14
     小小的    14
     想要     14
     味道     14
     春天     13
     我要     13
     笑      13
     世界     13
     打电话    12
     海边     12
     没      12
     充满     12
     不想     12
     再见     11
     日子     11
     担心     11
     离开     10
     睡着     10
     每天     10
     不知     10
     晚安     10
     朋友      9
     dtype: int64,
     7: 喝       12
     忘       10
     记起       8
     一口       8
     水        7
     记川       6
     小河       4
     川水       4
     一条       4
     那忘川      4
     忘川       3
     记        2
     水有       1
     漂亮       0
     泰国       0
     洗手台      0
     洗衣       0
     浪费       0
     海边       0
     深        0
     演唱       0
     深情       0
     渡口       0
     漂着       0
     没错       0
     湿湿的      0
     满天飞      0
     满脸       0
     泪        0
     没        0
     没试       0
     每天       0
     歪打正着     0
     段        0
     段时间      0
     每一分      0
     每个       0
     每到       0
     每夜       0
     每朵       0
     dtype: int64,
     8: 问       30
     一次      18
     不会      14
     从今以后     6
     不要       6
     明        6
     知道       6
     爱        5
     不愿       2
     承诺       2
     再也       2
     回头       2
     回        2
     错        2
     改变       1
     我会       1
     相        0
     浪费       0
     海边       0
     深        0
     深情       0
     湿湿的      0
     渡口       0
     洗手台      0
     相见       0
     满天飞      0
     满脸       0
     漂亮       0
     漂着       0
     演唱       0
     演算法      0
     点名       0
     洗衣       0
     没错       0
     泰国       0
     水        0
     每一分      0
     每个       0
     每到       0
     每夜       0
     dtype: int64,
     9: 爱       41
     世界      15
     我爱你      8
     已经       6
     心坎       6
     每个       6
     不爱       6
     相爱       6
     喜欢       5
     下去       4
     友情       4
     得来不易     4
     段        4
     继续       4
     知道       3
     今生今世     3
     真心       3
     不能       3
     改变       2
     时间       2
     手        1
     脸        1
     眼        1
     口        1
     漂着       0
     漂亮       0
     满脸       0
     满天飞      0
     湿湿的      0
     演唱       0
     每夜       0
     渡口       0
     深情       0
     演算法      0
     点名       0
     每天       0
     深        0
     海边       0
     毕竟       0
     民谣       0
     dtype: int64}




```python
# Check elbow up to k = 10
tvec = TfidfVectorizer(stop_words=updated_stopwords_2,max_df=0.3,max_features=10000,tokenizer=man_token_jie, norm='l2')
tvec.fit(lyrics)
X_transformed = tvec.transform(lyrics)


modelkmeans = KMeans(n_clusters=60, init='k-means++', n_init=100)




distortions = []
K = range(1,10)
for k in K:
    modelkmeans = KMeans(n_clusters=k, init='k-means++', n_init=100)
    modelkmeans.fit(X_transformed)
    distortions.append(modelkmeans.inertia_)
```


```python
range(len(K)+1)
```




    range(0, 10)




```python
old = distortions
old
```




    [35678.663398590696,
     35624.219684741896,
     35583.18153644596,
     35528.92516467828,
     35496.229220625784,
     35466.25213154638,
     35421.64830444519,
     35390.88946916553,
     35356.269004933085,
     35356.269004933085,
     35356.269004933085,
     35356.269004933085,
     35356.269004933085,
     35356.269004933085,
     35356.269004933085]




```python
old.pop
```




    <function list.pop(index=-1, /)>




```python
distortions = old[:9]
```


```python
test = [x for x in range(10)]
```


```python

plt.figure(figsize=(16,8))
plt.plot(K, distortions, 'bx-')
plt.xlabel('k')
plt.ylabel('Distortion')
plt.title('The Elbow Method showing the optimal k')
plt.show()
```


    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_363_0.png)
    



```python
# Check elbow for up to k = 100, stopwords 2
tvec = TfidfVectorizer(stop_words=updated_stopwords_2,max_df=0.3,max_features=10000,tokenizer=man_token_jie, norm='l2')
tvec.fit(lyrics)
X_transformed = tvec.transform(lyrics)



distortions = []
K = range(1,100)
for k in tqdm(K):
    modelkmeans = KMeans(n_clusters=k, init='k-means++', n_init=100)
    modelkmeans.fit(X_transformed)
    distortions.append(modelkmeans.inertia_)
    
plt.figure(figsize=(16,8))
plt.plot(K, distortions, 'bx-')
plt.xlabel('k')
plt.ylabel('Distortion')
plt.title('The Elbow Method showing the optimal k')
plt.show()
```

    100%|████████████████████████████████████████| 99/99 [7:52:40<00:00, 286.47s/it]



    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_364_1.png)
    


#### 9.4.1. DBScan TfidfVectorizer


```python
# try DBScan
from sklearn import metrics
from sklearn.cluster import DBSCAN
data = df_tracks_master_clean3_lyrics_genre_hsk_v2
lyrics = data.lyrics



tvec = TfidfVectorizer(stop_words=updated_stopwords_2,max_df=0.3,max_features=10000,tokenizer=man_token_jie, norm='l2')
tvec.fit(lyrics)
X_transformed = tvec.transform(lyrics)



modeldbscan = DBSCAN(eps=0.05, min_samples=5)
modeldbscan.fit(X_transformed)


```




    DBSCAN(eps=0.05)




```python
from sklearn.neighbors import NearestNeighbors
neigh = NearestNeighbors(n_neighbors=2)
nbrs = neigh.fit(X_transformed)
distances, indices = nbrs.kneighbors(X_transformed)

distances = np.sort(distances, axis=0)
distances = distances[:,1]
plt.plot(distances)
```




    [<matplotlib.lines.Line2D at 0x7fd544656070>]




    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_367_1.png)
    



```python
tvec = TfidfVectorizer(stop_words=updated_stopwords_2,max_df=0.3,max_features=100,tokenizer=man_token_jie, norm='l2')
tvec.fit(lyrics)
X_transformed = tvec.transform(lyrics)

neigh = NearestNeighbors(n_neighbors=2)
nbrs = neigh.fit(X_transformed)
distances, indices = nbrs.kneighbors(X_transformed)

distances = np.sort(distances, axis=0)
distances = distances[:,1]
plt.plot(distances)

```




    [<matplotlib.lines.Line2D at 0x7fd5b303aa90>]




    
![png](Spotty-Linguist-Project-Notebook-V00_files/Spotty-Linguist-Project-Notebook-V00_368_1.png)
    


Info here on how to optimze DBScan: 
- https://towardsdatascience.com/machine-learning-clustering-dbscan-determine-the-optimal-value-for-epsilon-eps-python-example-3100091cfbc
- https://towardsdatascience.com/k-means-vs-dbscan-clustering-49f8e627de27

#### 9.4.1. KMeans CVecVectorizer


```python
data = df_tracks_master_clean3_lyrics_genre_hsk_v2
lyrics = data.lyrics
```


```python
lyrics = data.lyrics
```


```python
from numpy import unique
from numpy import where
from sklearn.datasets import make_classification
from sklearn.cluster import KMeans
from matplotlib import pyplot
# define dataset


cvec = CountVectorizer(stop_words=updated_stopwords,max_features=10000,tokenizer=man_token_jie)
cvec.fit(lyrics)
X_transformed = cvec.transform(lyrics)



from sklearn.cluster import KMeans
modelkmeans = KMeans(n_clusters=60, init='k-means++', n_init=100)
modelkmeans.fit(X_transformed)
```




    KMeans(n_clusters=60, n_init=100)




```python
cluster_labels = modelkmeans.labels_
```


```python
cluster_labels
```




    array([41, 41,  2, ..., 41, 57, 41], dtype=int32)




```python
data['groupings'] = cluster_labels
```


```python
labels = cvec.get_feature_names()
```


```python
lyrics_mat = pd.DataFrame.sparse.from_spmatrix(X_transformed)
lyrics_mat.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
      <th>25</th>
      <th>26</th>
      <th>27</th>
      <th>28</th>
      <th>29</th>
      <th>30</th>
      <th>31</th>
      <th>32</th>
      <th>33</th>
      <th>34</th>
      <th>35</th>
      <th>36</th>
      <th>37</th>
      <th>38</th>
      <th>39</th>
      <th>...</th>
      <th>9960</th>
      <th>9961</th>
      <th>9962</th>
      <th>9963</th>
      <th>9964</th>
      <th>9965</th>
      <th>9966</th>
      <th>9967</th>
      <th>9968</th>
      <th>9969</th>
      <th>9970</th>
      <th>9971</th>
      <th>9972</th>
      <th>9973</th>
      <th>9974</th>
      <th>9975</th>
      <th>9976</th>
      <th>9977</th>
      <th>9978</th>
      <th>9979</th>
      <th>9980</th>
      <th>9981</th>
      <th>9982</th>
      <th>9983</th>
      <th>9984</th>
      <th>9985</th>
      <th>9986</th>
      <th>9987</th>
      <th>9988</th>
      <th>9989</th>
      <th>9990</th>
      <th>9991</th>
      <th>9992</th>
      <th>9993</th>
      <th>9994</th>
      <th>9995</th>
      <th>9996</th>
      <th>9997</th>
      <th>9998</th>
      <th>9999</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 10000 columns</p>
</div>




```python
lyrics_mat.columns = labels
```


```python
data2 = pd.merge(data, lyrics_mat, left_index=True, right_index=True)
```


```python
data2.to_csv('df_tracks_master_clean3_lyrics_genre_hsk_v3.csv')
```


    ---------------------------------------------------------------------------

    KeyboardInterrupt                         Traceback (most recent call last)

    /var/folders/87/4nvcbwyx25s9fz86v2nq_xmw0000gq/T/ipykernel_13777/1216579081.py in <module>
    ----> 1 data2.to_csv('df_tracks_master_clean3_lyrics_genre_hsk_v3.csv')
    

    ~/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/pandas/core/generic.py in to_csv(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)
       3464         )
       3465 
    -> 3466         return DataFrameRenderer(formatter).to_csv(
       3467             path_or_buf,
       3468             line_terminator=line_terminator,


    ~/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/pandas/io/formats/format.py in to_csv(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)
       1103             formatter=self.fmt,
       1104         )
    -> 1105         csv_formatter.save()
       1106 
       1107         if created_buffer:


    ~/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/pandas/io/formats/csvs.py in save(self)
        255             )
        256 
    --> 257             self._save()
        258 
        259     def _save(self) -> None:


    ~/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/pandas/io/formats/csvs.py in _save(self)
        260         if self._need_to_save_header:
        261             self._save_header()
    --> 262         self._save_body()
        263 
        264     def _save_header(self) -> None:


    ~/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/pandas/io/formats/csvs.py in _save_body(self)
        298             if start_i >= end_i:
        299                 break
    --> 300             self._save_chunk(start_i, end_i)
        301 
        302     def _save_chunk(self, start_i: int, end_i: int) -> None:


    ~/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/pandas/io/formats/csvs.py in _save_chunk(self, start_i, end_i)
        305         df = self.obj.iloc[slicer]
        306 
    --> 307         res = df._mgr.to_native_types(**self._number_format)
        308         data = [res.iget_values(i) for i in range(len(res.items))]
        309 


    ~/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/pandas/core/internals/managers.py in to_native_types(self, **kwargs)
        464         in formatting (repr / csv).
        465         """
    --> 466         return self.apply("to_native_types", **kwargs)
        467 
        468     def is_consolidated(self) -> bool:


    ~/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/pandas/core/internals/managers.py in apply(self, f, align_keys, ignore_failures, **kwargs)
        325                     applied = b.apply(f, **kwargs)
        326                 else:
    --> 327                     applied = getattr(b, f)(**kwargs)
        328             except (TypeError, NotImplementedError):
        329                 if not ignore_failures:


    ~/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/pandas/core/internals/blocks.py in to_native_types(self, na_rep, quoting, **kwargs)
        639     def to_native_types(self, na_rep="nan", quoting=None, **kwargs):
        640         """convert to our native types format"""
    --> 641         result = to_native_types(self.values, na_rep=na_rep, quoting=quoting, **kwargs)
        642         return self.make_block(result)
        643 


    ~/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/pandas/core/internals/blocks.py in to_native_types(values, na_rep, quoting, float_format, decimal, **kwargs)
       2060         mask = isna(values)
       2061 
    -> 2062         new_values = np.asarray(values.astype(object))
       2063         new_values[mask] = na_rep
       2064         return new_values


    ~/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/pandas/core/arrays/sparse/array.py in astype(self, dtype, copy)
       1116         Indices: array([2, 3], dtype=int32)
       1117         """
    -> 1118         if is_dtype_equal(dtype, self._dtype):
       1119             if not copy:
       1120                 return self


    ~/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/pandas/core/dtypes/common.py in is_dtype_equal(source, target)
        615         source = get_dtype(source)
        616         target = get_dtype(target)
    --> 617         return source == target
        618     except (TypeError, AttributeError):
        619 


    ~/Desktop/Spoty-Linguist-Project/env/lib/python3.8/site-packages/pandas/core/arrays/sparse/dtype.py in __eq__(self, other)
        105         return super().__hash__()
        106 
    --> 107     def __eq__(self, other: Any) -> bool:
        108         # We have to override __eq__ to handle NA values in _metadata.
        109         # The base class does simple == checks, which fail for NA.


    KeyboardInterrupt: 



```python
data2.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>artist_href</th>
      <th>artist</th>
      <th>artist_uri</th>
      <th>artist_type</th>
      <th>song_available_markets</th>
      <th>song_duration_ms</th>
      <th>is_explicit</th>
      <th>song_href</th>
      <th>preview_url</th>
      <th>track_number</th>
      <th>song_type</th>
      <th>length</th>
      <th>popularity</th>
      <th>album</th>
      <th>album_type</th>
      <th>album_artwork_link</th>
      <th>album_tracks_num</th>
      <th>album_uri</th>
      <th>album_release_date</th>
      <th>acousticness</th>
      <th>danceability</th>
      <th>energy</th>
      <th>instrumentalness</th>
      <th>liveness</th>
      <th>loudness</th>
      <th>speechiness</th>
      <th>tempo</th>
      <th>time_signature</th>
      <th>lyrics</th>
      <th>time_stamp</th>
      <th>genres</th>
      <th>lyrics_len</th>
      <th>hsk1</th>
      <th>hsk2</th>
      <th>hsk3</th>
      <th>hsk4</th>
      <th>hsk5</th>
      <th>hsk6</th>
      <th>hsk_level</th>
      <th>...</th>
      <th>黄土</th>
      <th>黄昏</th>
      <th>黄沙</th>
      <th>黄河</th>
      <th>黄算</th>
      <th>黄色</th>
      <th>黄金</th>
      <th>黎明</th>
      <th>黎明前</th>
      <th>黏</th>
      <th>黑</th>
      <th>黑发</th>
      <th>黑夜</th>
      <th>黑暗</th>
      <th>黑洞</th>
      <th>黑白</th>
      <th>黑眼圈</th>
      <th>黑色</th>
      <th>默契</th>
      <th>默念</th>
      <th>默然</th>
      <th>默许</th>
      <th>默默</th>
      <th>默默地</th>
      <th>黯淡</th>
      <th>黯然</th>
      <th>鼓</th>
      <th>鼓励</th>
      <th>鼓和声</th>
      <th>鼓声</th>
      <th>鼓手</th>
      <th>鼓掌</th>
      <th>鼓楼</th>
      <th>鼓舞</th>
      <th>鼓起勇气</th>
      <th>鼻子</th>
      <th>齐</th>
      <th>齐来</th>
      <th>齐齐</th>
      <th>龙</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>英雄 (Live)</td>
      <td>https://api.spotify.com/v1/artists/2elBjNSdBE2...</td>
      <td>Jay Chou</td>
      <td>spotify:artist:2elBjNSdBE2Y3f0j1mjrql</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>182040.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/1i31BYKv1La3...</td>
      <td>https://p.scdn.co/mp3-preview/191037a026d9e098...</td>
      <td>1.0</td>
      <td>track</td>
      <td>182040.0</td>
      <td>31.0</td>
      <td>周杰倫地表最強世界巡迴演唱會</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273494cdb...</td>
      <td>25.0</td>
      <td>spotify:album:5lqE2qT3nGLW6FGoJYfwUT</td>
      <td>2019-11-01</td>
      <td>0.000172</td>
      <td>0.522</td>
      <td>0.908</td>
      <td>0.002500</td>
      <td>0.458</td>
      <td>-8.747</td>
      <td>0.0596</td>
      <td>118.987</td>
      <td>4.0</td>
      <td>英雄词曲人生不是一个人的游戏一起奋斗一起超越一起杀吧兄弟好战好胜战胜逆命管他天赋够不够我们都...</td>
      <td>00:00.00,00:10.81,00:21.62,00:32.43,00:34.37,0...</td>
      <td>NaN</td>
      <td>288</td>
      <td>0.280303</td>
      <td>0.469697</td>
      <td>0.583333</td>
      <td>0.666667</td>
      <td>0.856061</td>
      <td>0.946970</td>
      <td>5</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>雙截棍 (Live)</td>
      <td>https://api.spotify.com/v1/artists/2elBjNSdBE2...</td>
      <td>Jay Chou</td>
      <td>spotify:artist:2elBjNSdBE2Y3f0j1mjrql</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>145173.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/77uwbOEaj6nM...</td>
      <td>https://p.scdn.co/mp3-preview/80c09c504a54344b...</td>
      <td>2.0</td>
      <td>track</td>
      <td>145173.0</td>
      <td>30.0</td>
      <td>周杰倫地表最強世界巡迴演唱會</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273494cdb...</td>
      <td>25.0</td>
      <td>spotify:album:5lqE2qT3nGLW6FGoJYfwUT</td>
      <td>2019-11-01</td>
      <td>0.002810</td>
      <td>0.461</td>
      <td>0.953</td>
      <td>0.000000</td>
      <td>0.424</td>
      <td>-6.440</td>
      <td>0.1360</td>
      <td>101.037</td>
      <td>4.0</td>
      <td>双截棍词曲岩烧店的烟味弥漫店里面的妈妈桑教拳脚武术的老板练铁沙掌硬底子功夫最擅长还会金钟罩铁...</td>
      <td>00:00.00,00:09.79,00:19.58,00:29.37,00:31.72,0...</td>
      <td>NaN</td>
      <td>370</td>
      <td>0.320000</td>
      <td>0.460000</td>
      <td>0.670000</td>
      <td>0.810000</td>
      <td>0.870000</td>
      <td>0.980000</td>
      <td>4</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>開不了口 (Live)</td>
      <td>https://api.spotify.com/v1/artists/2elBjNSdBE2...</td>
      <td>Jay Chou</td>
      <td>spotify:artist:2elBjNSdBE2Y3f0j1mjrql</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>272973.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/5OR1Fbd7RSI1...</td>
      <td>https://p.scdn.co/mp3-preview/82a62689347391e7...</td>
      <td>3.0</td>
      <td>track</td>
      <td>272973.0</td>
      <td>32.0</td>
      <td>周杰倫地表最強世界巡迴演唱會</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273494cdb...</td>
      <td>25.0</td>
      <td>spotify:album:5lqE2qT3nGLW6FGoJYfwUT</td>
      <td>2019-11-01</td>
      <td>0.001760</td>
      <td>0.519</td>
      <td>0.612</td>
      <td>0.000002</td>
      <td>0.564</td>
      <td>-10.634</td>
      <td>0.0267</td>
      <td>139.944</td>
      <td>4.0</td>
      <td>开不了口词曲才离开没多久就开始担心今天的你过得好不好整个画面是你嘴嘟嘟那可爱的模样还有在你身...</td>
      <td>00:00.00,00:24.00,00:48.00,01:12.01,01:14.58,0...</td>
      <td>NaN</td>
      <td>299</td>
      <td>0.404762</td>
      <td>0.595238</td>
      <td>0.833333</td>
      <td>0.892857</td>
      <td>0.928571</td>
      <td>0.988095</td>
      <td>3</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>床邊故事 (Live)</td>
      <td>https://api.spotify.com/v1/artists/2elBjNSdBE2...</td>
      <td>Jay Chou</td>
      <td>spotify:artist:2elBjNSdBE2Y3f0j1mjrql</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>220240.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/7MmT3xzugKwe...</td>
      <td>https://p.scdn.co/mp3-preview/5316d5602f55d671...</td>
      <td>4.0</td>
      <td>track</td>
      <td>220240.0</td>
      <td>29.0</td>
      <td>周杰倫地表最強世界巡迴演唱會</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273494cdb...</td>
      <td>25.0</td>
      <td>spotify:album:5lqE2qT3nGLW6FGoJYfwUT</td>
      <td>2019-11-01</td>
      <td>0.039800</td>
      <td>0.640</td>
      <td>0.844</td>
      <td>0.000005</td>
      <td>0.122</td>
      <td>-9.645</td>
      <td>0.0539</td>
      <td>106.033</td>
      <td>4.0</td>
      <td>床边故事词曲从前从前有只猫头鹰它站在屋顶屋顶后面一遍森林森林很安静安静的钢琴在大厅阁楼里仔细...</td>
      <td>00:00.00,00:06.45,00:12.91,00:19.37,00:20.74,0...</td>
      <td>NaN</td>
      <td>346</td>
      <td>0.368852</td>
      <td>0.500000</td>
      <td>0.655738</td>
      <td>0.803279</td>
      <td>0.901639</td>
      <td>0.950820</td>
      <td>4</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>夜曲+竊愛 (Live)</td>
      <td>https://api.spotify.com/v1/artists/2elBjNSdBE2...</td>
      <td>Jay Chou</td>
      <td>spotify:artist:2elBjNSdBE2Y3f0j1mjrql</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>222013.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/4vlzCpNsDFBa...</td>
      <td>https://p.scdn.co/mp3-preview/446b1630b52837ae...</td>
      <td>5.0</td>
      <td>track</td>
      <td>222013.0</td>
      <td>41.0</td>
      <td>周杰倫地表最強世界巡迴演唱會</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273494cdb...</td>
      <td>25.0</td>
      <td>spotify:album:5lqE2qT3nGLW6FGoJYfwUT</td>
      <td>2019-11-01</td>
      <td>0.028300</td>
      <td>0.538</td>
      <td>0.610</td>
      <td>0.005630</td>
      <td>0.386</td>
      <td>-10.513</td>
      <td>0.1680</td>
      <td>174.093</td>
      <td>4.0</td>
      <td>夜曲夜曲词曲窃爱词曲一群嗜血的蚂蚁被腐肉所吸引我面无表情看孤独的风景失去你失去你当鸽子不再象...</td>
      <td>00:00.00,00:03.37,00:03.70,00:04.45,00:05.10,0...</td>
      <td>NaN</td>
      <td>414</td>
      <td>0.233333</td>
      <td>0.306667</td>
      <td>0.526667</td>
      <td>0.680000</td>
      <td>0.813333</td>
      <td>0.933333</td>
      <td>5</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 10063 columns</p>
</div>




```python
# # look at topics using original tvec
# topics = pd.DataFrame()
# for i in range(10):
#     display(pd.DataFrame(cvec.transform(data[data['labels']==i].lyrics).toarray(), columns =cvec.get_feature_names()).sum().sort_values(ascending=False)[:15])

```


```python
# # look at topics but recalibrating tvec on each new sub cluster
# topics = pd.DataFrame()
# for i in range(10):
#     display(pd.DataFrame(cvec.fit_transform(data[data['labels']==i].lyrics).toarray(), columns =cvec.get_feature_names()).sum().sort_values(ascending=False)[:15])

```

## 10. Playlist generation


```python
df_tracks_master_clean3_lyrics_genre_hsk_v3 = pd.read_csv(
    '/Users/stuart/Desktop/Spoty-Linguist-Project/df_tracks_master_clean3_lyrics_genre_hsk_v2.csv',
    sep=',',
    index_col=0)
```

#### 10.1. Player


```python
### Version 1
from ipywidgets import GridspecLayout
from ipywidgets import AppLayout, Button, Layout
import ipywidgets as widgets
from ipywidgets import TwoByTwoLayout
# import IPython.display 
import urllib.request
from PIL import Image
from IPython.display import clear_output

########## HSK Selection
# default HSK Choice

def create_expanded_button(description, button_style):
    return Button(description=description, button_style=button_style, layout=Layout(height='auto', width='auto'))

grid = GridspecLayout(3, 3)

def HSK1_button(x):
    global HSK_choice
    HSK_choice = 1
    return HSK_choice

def HSK2_button(x):
    global HSK_choice
    HSK_choice = 2
    return HSK_choice

def HSK3_button(x):
    global HSK_choice
    HSK_choice = 3
    return HSK_choice

def HSK4_button(x):
    global HSK_choice
    HSK_choice = 4
    return HSK_choice

def HSK5_button(x):
    global HSK_choice
    HSK_choice = 5
    print(5)
    return HSK_choice

def HSK6_button(x):
    global HSK_choice
    HSK_choice = 6
    return HSK_choice  

grid[0,0] = create_expanded_button('HSK 1', 'info')
grid[0,0].on_click(HSK1_button)

grid[0,1] = create_expanded_button('HSK 2', 'info')
grid[0,1].on_click(HSK2_button)

grid[0,2] = create_expanded_button('HSK 3', 'success')
grid[0,2].on_click(HSK3_button)

grid[1,0] = create_expanded_button('HSK 4', 'success')
grid[1,0].on_click(HSK4_button)

grid[1,1] = create_expanded_button('HSK 5', 'warning')
grid[1,1].on_click(HSK5_button)

grid[1,2] = create_expanded_button('HSK 6', 'danger')
grid[1,2].on_click(HSK6_button)


###### Album Cover
img1 = open('gfg.png', 'rb').read()
wi1 = widgets.Image(value=img1, format='png', width=300, height=400)

# Genre
checkbox_genre = widgets.RadioButtons(options=df_tracks_master_clean3_lyrics_genre_hsk_v3['Genre2'][df_tracks_master_clean3_lyrics_genre_hsk_v3['Genre2'].notnull()].unique()[:10], value = 'mandopop', description='Genre')

# Play Button
play_button = widgets.Button(description="Play Song")
create_play_list_button = widgets.Button(description="Create Playlist")

talk_slider = widgets.FloatSlider(
    value=80,
    min=0,
    max=100.0,
    step=0.1,
    description='Talkability',
    disabled=False,
    continuous_update=False,
    orientation='horizontal',
    readout=True,
    readout_format='.1f',
)

tempo_slider = widgets.FloatSlider(
    value=80,
    min=0,
    max=100.0,
    step=0.1,
    description='Tempo',
    disabled=False,
    continuous_update=False,
    orientation='horizontal',
    readout=True,
    readout_format='.1f',
)


grid_master = GridspecLayout(4, 20, height='500px')

grid_master[0,0:7] = grid
grid_master[0,12:20] = wi1
grid_master[0,7:10] = checkbox_genre
grid_master[1,0:7] = talk_slider
grid_master[2,0:7] = tempo_slider
grid_master[3,0:7] = play_button
grid_master[3,8:] = create_play_list_button

play_button.on_click(on_button_clicked)



display(grid_master)

def on_button_clicked(b):
    clear_output(wait=True)
    
    songs_found = df_tracks_master_clean3_lyrics_genre_hsk_v3[(df_tracks_master_clean3_lyrics_genre_hsk_v3['hsk_level'] == HSK_choice) & (df_tracks_master_clean3_lyrics_genre_hsk_v3['Genre2'][df_tracks_master_clean3_lyrics_genre_hsk_v3['Genre2'].notnull()] == checkbox_genre.value)].shape[0]
    song_randomizer = random.randint(1, songs_found)
    
    img1 = open('gfg.png', 'rb').read()
    wi1 = widgets.Image(value=img1, format='png', width=300, height=400)


    
    
#     grid_master = GridspecLayout(20, 20)

#     grid_master[0,0:7] = grid
#     grid_master[0,12:20] = wi1
#     grid_master[0,7:10] = hkbox_genre
#     grid_master[1,0:7] = play_button
    
#     display(grid_master)
    
    urllib.request.urlretrieve(df_tracks_master_clean3_lyrics_genre_hsk_v3.iloc[song_randomizer,16],"gfg.png")

    ## Read images from file (because this is binary, maybe you can find how to use ByteIO) but this is more easy
#     ## Side by side thanks to HBox widgets
#     sidebyside = widgets.HBox([wi1])
#     ## Finally, show.
#     display(sidebyside)   
    grid_master = GridspecLayout(4, 20, height='500px')
    
    grid_master[0,0:7] = grid
    grid_master[0,12:20] = wi1
    grid_master[0,7:10] = checkbox_genre
    grid_master[1,0:7] = talk_slider
    grid_master[2,0:7] = tempo_slider
    grid_master[3,0:7] = play_button
    grid_master[3,8:] = create_play_list_button
    
    display(grid_master)
    song_selection_href = df_tracks_master_clean3_lyrics_genre_hsk_v3[(df_tracks_master_clean3_lyrics_genre_hsk_v3['hsk_level'] == HSK_choice) & (df_tracks_master_clean3_lyrics_genre_hsk_v3['Genre2'][df_tracks_master_clean3_lyrics_genre_hsk_v3['Genre2'].notnull()] == checkbox_genre.value)].iloc[song_randomizer,8]

    song_selection = df_tracks_master_clean3_lyrics_genre_hsk_v3[(df_tracks_master_clean3_lyrics_genre_hsk_v3['hsk_level'] == HSK_choice) & (df_tracks_master_clean3_lyrics_genre_hsk_v3['Genre2'][df_tracks_master_clean3_lyrics_genre_hsk_v3['Genre2'].notnull()] == checkbox_genre.value)].iloc[song_randomizer,0]
    artist_selection = df_tracks_master_clean3_lyrics_genre_hsk_v3[(df_tracks_master_clean3_lyrics_genre_hsk_v3['hsk_level'] == HSK_choice) & (df_tracks_master_clean3_lyrics_genre_hsk_v3['Genre2'][df_tracks_master_clean3_lyrics_genre_hsk_v3['Genre2'].notnull()] == checkbox_genre.value)].iloc[song_randomizer,2]
    print('HSK Level:',HSK_choice )
    print('Number of songs found:', songs_found)
    print('Playing:',song_selection)
    print('Artist:',artist_selection )
    print('Track Name', song_selection)
    
    try:
        pull_lyrics(song_selection, song_selection_href)
    except:
        print('lyrics not avaiable')
    
    
    

```


    GridspecLayout(children=(GridspecLayout(children=(Button(button_style='info', description='HSK 1', layout=Layo…


    HSK Level: 4
    Number of songs found: 1566
    Playing: 哭過就好了
    Artist: Rachel Liang
    Track Name 哭過就好了
    Starting Playback
    Lyrics
    哭过就好了
    词



```python
def getChinese(context):
    filtrate = re.compile(u'[^\u4E00-\u9FA5]')  # non-Chinese unicode range
    context = filtrate.sub(r'', context)  # remove all non-Chinese characters
    return context
    

def song_uri_from_href(href):
    try: 
        url_search = re.compile(r'(?<=track/).*(?=.*?\?)')
        result = re.findall(url_search, href)[0]
        uri = 'spotify:track:' + result
        return uri
    except:
        url_search = re.compile(r'(?<=tracks/).*')
        result = re.findall(url_search, href)[0]
        uri = 'spotify:track:' + result
        return uri


def pull_lyrics(table_name, href):
    
    # first filter tablename variable for only chinese characters
    
    try:
        table_name = getChinese(table_name)

        song_uri = song_uri_from_href(href)
        print('Starting Playback')
        print('Lyrics')
        sp.start_playback(device_id=device_id, uris=[song_uri])

        # pull lyric table from data base and convert time to time format
        db_connection = sqlite3.connect('/Users/stuart/Desktop/Spoty-Linguist-Project/master_sql_db.db.sqlite')
        sql_lyrics_table = sql.read_sql('SELECT time, lyrics FROM %s'%table_name, con = db_connection)
        sql_lyrics_table['time'] = sql_lyrics_table.time.apply(pd.to_datetime)

        # create a column that include time between each lyrics
        sql_lyrics_table['pause_time'] = sql_lyrics_table.time
        time_temp = sql_lyrics_table.pause_time[0] # start time
        for ind,i in enumerate(sql_lyrics_table.pause_time):
            sql_lyrics_table.iloc[ind,2] = sql_lyrics_table.iloc[ind,0] - time_temp
            time_temp = sql_lyrics_table.time[ind]

        for ind,i in enumerate(sql_lyrics_table.lyrics):
            print(i)
            time.sleep(sql_lyrics_table.pause_time[ind].total_seconds()/60)


        sp.pause_playback(device_id=device_id)
        
    except:
        print('lyrics not available')
        time.sleep(3)
        sp.pause_playback(device_id=device_id)
    
```


```python
# df_tracks_master_clean3_lyrics_genre_hsk_v3 = data2

### Version 2
from ipywidgets import GridspecLayout
from ipywidgets import AppLayout, Button, Layout
import ipywidgets as widgets
from ipywidgets import TwoByTwoLayout
# import IPython.display 
import urllib.request
from PIL import Image
from IPython.display import clear_output

########## HSK Selection
# default HSK Choice

def create_expanded_button(description, button_style):
    return Button(description=description, button_style=button_style, layout=Layout(height='auto', width='auto'))

grid = GridspecLayout(3, 3)

def HSK1_button(x):
    global HSK_choice
    HSK_choice = 1
    return HSK_choice

def HSK2_button(x):
    global HSK_choice
    HSK_choice = 2
    return HSK_choice

def HSK3_button(x):
    global HSK_choice
    HSK_choice = 3
    return HSK_choice

def HSK4_button(x):
    global HSK_choice
    HSK_choice = 4
    return HSK_choice

def HSK5_button(x):
    global HSK_choice
    HSK_choice = 5
    print(5)
    return HSK_choice

def HSK6_button(x):
    global HSK_choice
    HSK_choice = 6
    return HSK_choice  

grid[0,0] = create_expanded_button('HSK 1', 'info')
grid[0,0].on_click(HSK1_button)

grid[0,1] = create_expanded_button('HSK 2', 'info')
grid[0,1].on_click(HSK2_button)

grid[0,2] = create_expanded_button('HSK 3', 'success')
grid[0,2].on_click(HSK3_button)

grid[1,0] = create_expanded_button('HSK 4', 'success')
grid[1,0].on_click(HSK4_button)

grid[1,1] = create_expanded_button('HSK 5', 'warning')
grid[1,1].on_click(HSK5_button)

grid[1,2] = create_expanded_button('HSK 6', 'danger')
grid[1,2].on_click(HSK6_button)


###### Album Cover
img1 = open('gfg.png', 'rb').read()
wi1 = widgets.Image(value=img1, format='png', width=300, height=400)

# Genre
checkbox_genre = widgets.RadioButtons(options=df_tracks_master_clean3_lyrics_genre_hsk_v3['Genre2'][df_tracks_master_clean3_lyrics_genre_hsk_v3['Genre2'].notnull()].unique()[:10], value = 'mandopop', description='Genre')

# Play Button
play_button = widgets.Button(description="Play Song")
create_play_list_button = widgets.Button(description="Create Playlist")


grid_master = GridspecLayout(4, 20, height='500px')

grid_master[0,0:7] = grid
grid_master[0,12:20] = wi1
grid_master[0,7:10] = checkbox_genre
grid_master[3,0:7] = play_button
grid_master[3,8:] = create_play_list_button

play_button.on_click(on_button_clicked)



display(grid_master)



def on_button_clicked(b):
    clear_output(wait=True)
    
    songs_found = df_tracks_master_clean3_lyrics_genre_hsk_v3[(df_tracks_master_clean3_lyrics_genre_hsk_v3['hsk_level'] == HSK_choice) & (df_tracks_master_clean3_lyrics_genre_hsk_v3['Genre2'][df_tracks_master_clean3_lyrics_genre_hsk_v3['Genre2'].notnull()] == checkbox_genre.value)].shape[0]
    song_randomizer = random.randint(1, songs_found)
    
    img1 = open('gfg.png', 'rb').read()
    wi1 = widgets.Image(value=img1, format='png', width=300, height=400)


    
    
#     grid_master = GridspecLayout(20, 20)

#     grid_master[0,0:7] = grid
#     grid_master[0,12:20] = wi1
#     grid_master[0,7:10] = hkbox_genre
#     grid_master[1,0:7] = play_button
    
#     display(grid_master)
    
    urllib.request.urlretrieve(df_tracks_master_clean3_lyrics_genre_hsk_v3.iloc[song_randomizer,16],"gfg.png")

    ## Read images from file (because this is binary, maybe you can find how to use ByteIO) but this is more easy
#     ## Side by side thanks to HBox widgets
#     sidebyside = widgets.HBox([wi1])
#     ## Finally, show.
#     display(sidebyside)   
    grid_master = GridspecLayout(4, 20, height='500px')
    
    grid_master[0,0:7] = grid
    grid_master[0,12:20] = wi1
    grid_master[0,7:10] = checkbox_genre
    grid_master[3,0:7] = play_button
    grid_master[3,8:] = create_play_list_button
    
    display(grid_master)
    song_selection_href = df_tracks_master_clean3_lyrics_genre_hsk_v3[(df_tracks_master_clean3_lyrics_genre_hsk_v3['hsk_level'] == HSK_choice) & (df_tracks_master_clean3_lyrics_genre_hsk_v3['Genre2'][df_tracks_master_clean3_lyrics_genre_hsk_v3['Genre2'].notnull()] == checkbox_genre.value)].iloc[song_randomizer,8]

    song_selection = df_tracks_master_clean3_lyrics_genre_hsk_v3[(df_tracks_master_clean3_lyrics_genre_hsk_v3['hsk_level'] == HSK_choice) & (df_tracks_master_clean3_lyrics_genre_hsk_v3['Genre2'][df_tracks_master_clean3_lyrics_genre_hsk_v3['Genre2'].notnull()] == checkbox_genre.value)].iloc[song_randomizer,0]
    artist_selection = df_tracks_master_clean3_lyrics_genre_hsk_v3[(df_tracks_master_clean3_lyrics_genre_hsk_v3['hsk_level'] == HSK_choice) & (df_tracks_master_clean3_lyrics_genre_hsk_v3['Genre2'][df_tracks_master_clean3_lyrics_genre_hsk_v3['Genre2'].notnull()] == checkbox_genre.value)].iloc[song_randomizer,2]
    print('HSK Level:',HSK_choice )
    print('Number of songs found:', songs_found)
    print('Playing:',song_selection)
    print('Artist:',artist_selection )
    print('Track Name', song_selection)
    
    try:
        pull_lyrics(song_selection, song_selection_href)
    except:
        print('lyrics not avaiable')
    
    
    

```


    GridspecLayout(children=(GridspecLayout(children=(Button(button_style='info', description='HSK 1', layout=Layo…


    HSK Level: 4
    Number of songs found: 1293
    Playing: 裙擺搖搖
    Artist: 李心潔
    Track Name 裙擺搖搖
    Starting Playback
    Lyrics
    裙摆摇摇
    词


#### 10.2. Redundant


```python
# # import data file
# df_tracks_master_clean3_lyrics_genre_hsk_v2 = pd.read_csv(
#     '/Users/stuart/Desktop/Spoty-Linguist-Project/df_tracks_master_clean3_lyrics_genre_hsk_v2.csv',
#     sep=',',
#     index_col=0)
```


```python
# import ipywidgets as widgets
# from ipywidgets import TwoByTwoLayout
# # import IPython.display 
# import urllib.request
# from PIL import Image
# from IPython.display import clear_output



# # chkbox_hsk = widgets.RadioButtons(options=df_tracks_master_clean3_lyrics_genre_hsk_v2['hsk_level'].unique(), value = 4,
# #  description='HSK Level')

# # chkbox_genre = widgets.RadioButtons(options=df_tracks_master_clean3_lyrics_genre_hsk_v2['Genre2'].unique()[:10], value = 'mandopop',
# #  description='Genre')


# # button = widgets.Button(description="Play Song")


# def on_button_clicked(b):
#     songs_found = df_tracks_master_clean3_lyrics_genre_hsk_v2[(df_tracks_master_clean3_lyrics_genre_hsk_v2['hsk_level'] == HSK_choice) & (df_tracks_master_clean3_lyrics_genre_hsk_v2['Genre2'] == chkbox_genre.value)].shape[0]
#     song_randomizer = random.randint(1, songs_found)
    
#     clear_output(wait=True)
#     display(button)
#     display(chkbox_hsk, chkbox_genre)


#     urllib.request.urlretrieve(df_tracks_master_clean3_lyrics_genre_hsk_v2.iloc[song_randomizer,16],"gfg.png")

#     ## Read images from file (because this is binary, maybe you can find how to use ByteIO) but this is more easy
#     img1 = open('gfg.png', 'rb').read()
#     wi1 = widgets.Image(value=img1, format='png', width=300, height=400)
# #     ## Side by side thanks to HBox widgets
# #     sidebyside = widgets.HBox([wi1])
# #     ## Finally, show.
# #     display(sidebyside)   
#     layout = TwoByTwoLayout(top_left=chkbox_hsk,
#                bottom_left=genre1_drop,
#                bottom_right=wi1)
    
#     display(layout)
#     song_selection_href = df_tracks_master_clean3_lyrics_genre_hsk_v2[(df_tracks_master_clean3_lyrics_genre_hsk_v2['hsk_level'] == chkbox_hsk.value) & (df_tracks_master_clean3_lyrics_genre_hsk_v2['Genre2'] == chkbox_genre.value)].iloc[song_randomizer,8]

#     song_selection = df_tracks_master_clean3_lyrics_genre_hsk_v2[(df_tracks_master_clean3_lyrics_genre_hsk_v2['hsk_level'] == chkbox_hsk.value) & (df_tracks_master_clean3_lyrics_genre_hsk_v2['Genre2'] == chkbox_genre.value)].iloc[song_randomizer,0]
#     artist_selection = df_tracks_master_clean3_lyrics_genre_hsk_v2[(df_tracks_master_clean3_lyrics_genre_hsk_v2['hsk_level'] == chkbox_hsk.value) & (df_tracks_master_clean3_lyrics_genre_hsk_v2['Genre2'] == chkbox_genre.value)].iloc[song_randomizer,2]
    
#     print('Number of songs found:', songs_found)
#     print('Playing:',song_selection)
#     print('Artist:',artist_selection )
#     print('Track Name', song_selection)
    
#     try:
#         pull_lyrics(song_selection, song_selection_href)
#     except:
#         print('lyrics not avaiable')
    




# button.on_click(on_button_clicked)

# img1 = open('gfg.png', 'rb').read()
# wi1 = widgets.Image(value=img1, format='png', width=300, height=400)


# layout = TwoByTwoLayout(top_left=grid,
#            bottom_left=genre1_drop,
#            bottom_right=wi1)




# display(layout)
```


    TwoByTwoLayout(children=(GridspecLayout(children=(Button(button_style='info', description='HSK 1', layout=Layo…



```python
from ipywidgets import GridspecLayout
from ipywidgets import AppLayout, Button, Layout
import ipywidgets as widgets
from ipywidgets import TwoByTwoLayout
# import IPython.display 
import urllib.request
from PIL import Image
from IPython.display import clear_output

########## HSK Selection
# default HSK Choice
HSK_choice = 5 
def create_expanded_button(description, button_style):
    return Button(description=description, button_style=button_style, layout=Layout(height='auto', width='auto'))

grid = GridspecLayout(4, 3)

def HSK1_button(x):
    HSK_choice = 1
    return HSK_choice

def HSK2_button(x):
    HSK_choice = 2
    return HSK_choice

def HSK3_button(x):
    HSK_choice = 3
    return HSK_choice

def HSK4_button(x):
    HSK_choice = 4
    return HSK_choice

def HSK5_button(x):
    HSK_choice = 5
    return HSK_choice

def HSK6_button(x):
    HSK_choice = 6
    return HSK_choice  

grid[0,0] = create_expanded_button('HSK 1', 'info')
grid[0,0].on_click(HSK1_button)

grid[0,1] = create_expanded_button('HSK 2', 'info')
grid[0,1].on_click(HSK2_button)

grid[0,2] = create_expanded_button('HSK 3', 'success')
grid[0,2].on_click(HSK3_button)

grid[1,0] = create_expanded_button('HSK 4', 'success')
grid[1,0].on_click(HSK4_button)

grid[1,1] = create_expanded_button('HSK 5', 'warning')
grid[1,1].on_click(HSK5_button)

grid[1,2] = create_expanded_button('HSK 6', 'danger')
grid[1,2].on_click(HSK6_button)

grid[1,2] = create_expanded_button('HSK 6', 'danger')
grid[1,2].on_click(HSK6_button)

###### Album Cover
img1 = open('gfg.png', 'rb').read()
wi1 = widgets.Image(value=img1, format='png', width=300, height=400)

# Genre
checkbox_genre = widgets.RadioButtons(options=df_tracks_master_clean3_lyrics_genre_hsk_v2['Genre2'].unique()[:10], value = 'mandopop', description='Genre')

# Play Button
play_button = widgets.Button(description="Play Song")

grid_master = GridspecLayout(2, 20)

grid_master[0,0:7] = grid
grid_master[0,12:20] = wi1
grid_master[0,7:10] = checkbox_genre
grid_master[1,0:7] = play_button

play_button.on_click(on_button_clicked)

display(grid_master)

def on_button_clicked(b):
    clear_output(wait=True)
    
    songs_found = df_tracks_master_clean3_lyrics_genre_hsk_v2[(df_tracks_master_clean3_lyrics_genre_hsk_v2['hsk_level'] == HSK_choice) & (df_tracks_master_clean3_lyrics_genre_hsk_v2['Genre2'] == checkbox_genre.value)].shape[0]
    song_randomizer = random.randint(1, songs_found)
    
    img1 = open('gfg.png', 'rb').read()
    wi1 = widgets.Image(value=img1, format='png', width=300, height=400)


    
    
#     grid_master = GridspecLayout(20, 20)

#     grid_master[0,0:7] = grid
#     grid_master[0,12:20] = wi1
#     grid_master[0,7:10] = hkbox_genre
#     grid_master[1,0:7] = play_button
    
#     display(grid_master)
    
    urllib.request.urlretrieve(df_tracks_master_clean3_lyrics_genre_hsk_v2.iloc[song_randomizer,16],"gfg.png")

    ## Read images from file (because this is binary, maybe you can find how to use ByteIO) but this is more easy
#     ## Side by side thanks to HBox widgets
#     sidebyside = widgets.HBox([wi1])
#     ## Finally, show.
#     display(sidebyside)   
#     grid_master = GridspecLayout(20, 20)
    
    grid_master = GridspecLayout(2, 20)
    grid_master[0,0:7] = grid
    grid_master[0,12:20] = wi1
    grid_master[0,7:10] = checkbox_genre
    grid_master[1,0:7] = play_button
    
    display(grid_master)
    song_selection_href = df_tracks_master_clean3_lyrics_genre_hsk_v2[(df_tracks_master_clean3_lyrics_genre_hsk_v2['hsk_level'] == chkbox_hsk.value) & (df_tracks_master_clean3_lyrics_genre_hsk_v2['Genre2'] == chkbox_genre.value)].iloc[song_randomizer,8]

    song_selection = df_tracks_master_clean3_lyrics_genre_hsk_v2[(df_tracks_master_clean3_lyrics_genre_hsk_v2['hsk_level'] == chkbox_hsk.value) & (df_tracks_master_clean3_lyrics_genre_hsk_v2['Genre2'] == chkbox_genre.value)].iloc[song_randomizer,0]
    artist_selection = df_tracks_master_clean3_lyrics_genre_hsk_v2[(df_tracks_master_clean3_lyrics_genre_hsk_v2['hsk_level'] == chkbox_hsk.value) & (df_tracks_master_clean3_lyrics_genre_hsk_v2['Genre2'] == chkbox_genre.value)].iloc[song_randomizer,2]
    
    print('Number of songs found:', songs_found)
    print('Playing:',song_selection)
    print('Artist:',artist_selection )
    print('Track Name', song_selection)
    
    try:
        pull_lyrics(song_selection, song_selection_href)
    except:
        print('lyrics not avaiable')
    
    
    

```


    GridspecLayout(children=(GridspecLayout(children=(Button(button_style='info', description='HSK 1', layout=Layo…


    Number of songs found: 1712
    Playing: Better - Dusa Remix
    Artist: Kimberley Chen
    Track Name Better - Dusa Remix
    Starting Playback
    Lyrics
    lyrics not available


How to add buttons in Jupyter
- https://medium.com/convergence-tech/ux-in-jupyter-user-input-essentials-779c9b449f2d


```python
widgets.SelectMultiple(
    options=['Apples', 'Oranges', 'Pears'],
    value=['Oranges'],
    #rows=10,
    description='Fruits',
    disabled=False
)
```


    SelectMultiple(description='Fruits', index=(1,), options=('Apples', 'Oranges', 'Pears'), value=('Oranges',))


## 11. Network Generator


```python

```


```python

```


```python
df_tracks_master_clean3_lyrics_genre_hsk_v2[df_tracks_master_clean3_lyrics_genre_hsk_v2.hsk1 > 0.7]
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>artist_href</th>
      <th>artist</th>
      <th>artist_uri</th>
      <th>artist_type</th>
      <th>song_available_markets</th>
      <th>song_duration_ms</th>
      <th>is_explicit</th>
      <th>song_href</th>
      <th>preview_url</th>
      <th>track_number</th>
      <th>song_type</th>
      <th>length</th>
      <th>popularity</th>
      <th>album</th>
      <th>album_type</th>
      <th>album_artwork_link</th>
      <th>album_tracks_num</th>
      <th>album_uri</th>
      <th>album_release_date</th>
      <th>acousticness</th>
      <th>danceability</th>
      <th>energy</th>
      <th>instrumentalness</th>
      <th>liveness</th>
      <th>loudness</th>
      <th>speechiness</th>
      <th>tempo</th>
      <th>time_signature</th>
      <th>lyrics</th>
      <th>time_stamp</th>
      <th>genres</th>
      <th>lyrics_len</th>
      <th>hsk1</th>
      <th>hsk2</th>
      <th>hsk3</th>
      <th>hsk4</th>
      <th>hsk5</th>
      <th>hsk6</th>
      <th>hsk_level</th>
      <th>hsk_level_6_80p</th>
      <th>hsk_level_5_80p</th>
      <th>hsk_level_4_80p</th>
      <th>hsk_level_3_80p</th>
      <th>hsk_level_2_80p</th>
      <th>hsk_level_1_80p</th>
      <th>hsk_level_5_75p</th>
      <th>hsk_level_4_75p</th>
      <th>hsk_level_3_75p</th>
      <th>hsk_level_2_75p</th>
      <th>hsk_level_1_75p</th>
      <th>genre_0</th>
      <th>genre_1</th>
      <th>genre_2</th>
      <th>Genre1</th>
      <th>Genre2</th>
      <th>Genre3</th>
      <th>Genre4</th>
      <th>Genre5</th>
      <th>Genre6</th>
      <th>Genre7</th>
      <th>weblink</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1088</th>
      <td>LOVE (Live)</td>
      <td>https://api.spotify.com/v1/artists/14bJhryXGk6...</td>
      <td>Hebe Tien</td>
      <td>spotify:artist:14bJhryXGk6H6qlGzwj3W5</td>
      <td>artist</td>
      <td>['HK', 'MO', 'TW']</td>
      <td>219706.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/4qKPEOARx0lE...</td>
      <td>https://p.scdn.co/mp3-preview/08bbaded95e1f3ee...</td>
      <td>2.0</td>
      <td>track</td>
      <td>219706.0</td>
      <td>11.0</td>
      <td>IF only 如果 田馥甄巡迴演唱會 (Live)</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b27372f745...</td>
      <td>26.0</td>
      <td>spotify:album:33XmR5MH1w3n3gIECukz6E</td>
      <td>2016-12-12</td>
      <td>0.010100</td>
      <td>0.551</td>
      <td>0.898</td>
      <td>0.000457</td>
      <td>0.7300</td>
      <td>-6.248</td>
      <td>0.1020</td>
      <td>99.932</td>
      <td>4.0</td>
      <td>词曲我爱你她爱她你爱我他爱他咦已经没有人相爱怎么这世界怎么这世界不爱自己我爱你她爱她你爱我他...</td>
      <td>00:00.00,00:12.20,00:24.41,00:36.62,00:40.96,0...</td>
      <td>mandopop,taiwan pop</td>
      <td>92</td>
      <td>0.705882</td>
      <td>0.764706</td>
      <td>0.941176</td>
      <td>0.941176</td>
      <td>0.941176</td>
      <td>0.941176</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>mandopop</td>
      <td>taiwan</td>
      <td>pop</td>
      <td>mandopop</td>
      <td>taiwan pop</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>https://open.spotify.com/track/4qKPEOARx0lEIkj...</td>
    </tr>
    <tr>
      <th>1148</th>
      <td>Love?</td>
      <td>https://api.spotify.com/v1/artists/14bJhryXGk6...</td>
      <td>Hebe Tien</td>
      <td>spotify:artist:14bJhryXGk6H6qlGzwj3W5</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>87720.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/0T722iRBSkCX...</td>
      <td>https://p.scdn.co/mp3-preview/1f0f0cf23f85a725...</td>
      <td>1.0</td>
      <td>track</td>
      <td>87720.0</td>
      <td>18.0</td>
      <td>To Hebe</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b2733e7c6c...</td>
      <td>11.0</td>
      <td>spotify:album:3LS8Yu3yTT5CnUvkTcISyC</td>
      <td>2010-10-20</td>
      <td>0.844000</td>
      <td>0.714</td>
      <td>0.227</td>
      <td>0.000000</td>
      <td>0.0950</td>
      <td>-11.133</td>
      <td>0.0358</td>
      <td>95.076</td>
      <td>4.0</td>
      <td>词曲我爱你她爱她你爱我他爱他怎么这世界怎么这世界怎么这世界怎么这世界怎么这世界怎么这世界</td>
      <td>00:00.00,00:03.92,00:07.84,00:11.76,00:26.13,0...</td>
      <td>mandopop,taiwan pop</td>
      <td>44</td>
      <td>0.800000</td>
      <td>0.800000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>mandopop</td>
      <td>taiwan</td>
      <td>pop</td>
      <td>mandopop</td>
      <td>taiwan pop</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>https://open.spotify.com/track/0T722iRBSkCXJyh...</td>
    </tr>
    <tr>
      <th>8889</th>
      <td>我夢想有一天還會見到你</td>
      <td>https://api.spotify.com/v1/artists/1km0aro5NJu...</td>
      <td>Joanna Wang</td>
      <td>spotify:artist:1km0aro5NJuoX5dE0Mub5C</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>159876.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/7yoiatTNgWKd...</td>
      <td>https://p.scdn.co/mp3-preview/52862c76255a1bc4...</td>
      <td>13.0</td>
      <td>track</td>
      <td>159876.0</td>
      <td>7.0</td>
      <td>摩登悲劇</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b27384a469...</td>
      <td>19.0</td>
      <td>spotify:album:5RceUqTiztLNmT5IXwZZEX</td>
      <td>2018-08-08</td>
      <td>0.143000</td>
      <td>0.480</td>
      <td>0.857</td>
      <td>0.000000</td>
      <td>0.1060</td>
      <td>-5.755</td>
      <td>0.1350</td>
      <td>151.659</td>
      <td>4.0</td>
      <td>我梦想有一天还会见到你</td>
      <td>00:00.00,00:02.23,00:04.46,00:06.69,00:09.66,0...</td>
      <td>c-pop,chinese audiophile,chinese indie,chinese...</td>
      <td>11</td>
      <td>0.777778</td>
      <td>0.888889</td>
      <td>0.888889</td>
      <td>0.888889</td>
      <td>0.888889</td>
      <td>1.000000</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>c</td>
      <td>pop</td>
      <td>chinese</td>
      <td>c-pop</td>
      <td>chinese audiophile</td>
      <td>chinese indie</td>
      <td>chinese jazz</td>
      <td>mandopop</td>
      <td>taiwan pop</td>
      <td>taiwan singer-songwriter</td>
      <td>https://open.spotify.com/track/7yoiatTNgWKdg2G...</td>
    </tr>
    <tr>
      <th>20415</th>
      <td>不遮掩</td>
      <td>https://api.spotify.com/v1/artists/39YbP9PakVw...</td>
      <td>E-Jun Lee</td>
      <td>spotify:artist:39YbP9PakVwqfXFtRdn5vI</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>235240.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/5cZcW1kzErDa...</td>
      <td>https://p.scdn.co/mp3-preview/8e628f06d0c6853e...</td>
      <td>9.0</td>
      <td>track</td>
      <td>235240.0</td>
      <td>1.0</td>
      <td>苓聽</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b27300be24...</td>
      <td>10.0</td>
      <td>spotify:album:6PueNXlHjFK7OwFHQsCFux</td>
      <td>2015-06-02</td>
      <td>0.080100</td>
      <td>0.551</td>
      <td>0.539</td>
      <td>0.000000</td>
      <td>0.2780</td>
      <td>-9.772</td>
      <td>0.0305</td>
      <td>139.954</td>
      <td>4.0</td>
      <td>不遮掩词曲一天小说昨天谢谢了我看见谁让谁再见我听见谁怪谁说再见一天小说昨天谢谢了我看见谁让谁...</td>
      <td>00:00.00,00:05.36,00:10.72,00:16.09,00:19.60,0...</td>
      <td>NaN</td>
      <td>85</td>
      <td>0.800000</td>
      <td>0.880000</td>
      <td>0.920000</td>
      <td>0.960000</td>
      <td>0.960000</td>
      <td>1.000000</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>c-pop</td>
      <td>classic mandopop</td>
      <td>mandopop</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>https://open.spotify.com/track/5cZcW1kzErDaCP9...</td>
    </tr>
    <tr>
      <th>21240</th>
      <td>北海怪兽</td>
      <td>https://api.spotify.com/v1/artists/3asy14alGCZ...</td>
      <td>新裤子乐队</td>
      <td>spotify:artist:3asy14alGCZBDd3Y6pbBbp</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>192920.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/63xfbzkcmx6H...</td>
      <td>https://p.scdn.co/mp3-preview/cc298675c57871ec...</td>
      <td>1.0</td>
      <td>track</td>
      <td>192920.0</td>
      <td>13.0</td>
      <td>野人也有爱</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273bbe4f7...</td>
      <td>11.0</td>
      <td>spotify:album:4lRp7mDRzeUdK5PKSM3ZOv</td>
      <td>2008-11-14</td>
      <td>0.061700</td>
      <td>0.531</td>
      <td>0.811</td>
      <td>0.391000</td>
      <td>0.0568</td>
      <td>-5.332</td>
      <td>0.0477</td>
      <td>120.015</td>
      <td>4.0</td>
      <td>北海怪兽我爱北海怪兽我爱北海怪兽我爱北海怪兽我爱北海怪兽</td>
      <td>00:00.00,00:24.70,00:31.06,00:32.54,00:40.80,0...</td>
      <td>chinese indie,chinese indie rock</td>
      <td>28</td>
      <td>0.750000</td>
      <td>0.750000</td>
      <td>0.750000</td>
      <td>0.750000</td>
      <td>0.750000</td>
      <td>1.000000</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>chinese</td>
      <td>indie</td>
      <td>chinese</td>
      <td>chinese indie</td>
      <td>chinese indie rock</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>https://open.spotify.com/track/63xfbzkcmx6HPzy...</td>
    </tr>
    <tr>
      <th>23648</th>
      <td>等14</td>
      <td>https://api.spotify.com/v1/artists/25rFapdPzsR...</td>
      <td>Mavis Fan</td>
      <td>spotify:artist:25rFapdPzsR0PEl8dFWL3I</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>208533.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/6TktrvPdYpGF...</td>
      <td>https://p.scdn.co/mp3-preview/0eb35346856877b4...</td>
      <td>12.0</td>
      <td>track</td>
      <td>208533.0</td>
      <td>5.0</td>
      <td>絕世名伶</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273bc1dd6...</td>
      <td>13.0</td>
      <td>spotify:album:5GyWIA1gNsqLCWEOmpJD7c</td>
      <td>2001-08-24</td>
      <td>0.715000</td>
      <td>0.738</td>
      <td>0.296</td>
      <td>0.001470</td>
      <td>0.1110</td>
      <td>-12.402</td>
      <td>0.0580</td>
      <td>145.210</td>
      <td>1.0</td>
      <td>等词曲等等等等等等等等等等等等等等你不能我不能他不能都不能再等不愿等不要等不想等不能够再等等...</td>
      <td>00:00.00,00:02.52,00:05.04,00:07.56,00:11.61,0...</td>
      <td>chinese indie,classic mandopop,mandopop,taiwan...</td>
      <td>351</td>
      <td>0.705882</td>
      <td>0.764706</td>
      <td>0.882353</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>chinese</td>
      <td>indie</td>
      <td>classic</td>
      <td>chinese indie</td>
      <td>classic mandopop</td>
      <td>mandopop</td>
      <td>taiwan pop</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>https://open.spotify.com/track/6TktrvPdYpGFmft...</td>
    </tr>
    <tr>
      <th>25332</th>
      <td>讓你飛 - REMIX版</td>
      <td>https://api.spotify.com/v1/artists/0LyfQWJT6nX...</td>
      <td>Various Artists</td>
      <td>spotify:artist:6noxsCszBEEK04kCehugOp</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>256600.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/2bsJYLBCUYSF...</td>
      <td>https://p.scdn.co/mp3-preview/922a6368d093cf7e...</td>
      <td>6.0</td>
      <td>track</td>
      <td>256600.0</td>
      <td>0.0</td>
      <td>RE 蕊</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273ec8e5a...</td>
      <td>36.0</td>
      <td>spotify:album:1RAYeYrzpWt75oOcrNe4Vj</td>
      <td>2002-07-16</td>
      <td>0.000989</td>
      <td>0.669</td>
      <td>0.967</td>
      <td>0.844000</td>
      <td>0.0844</td>
      <td>-4.859</td>
      <td>0.0561</td>
      <td>130.005</td>
      <td>4.0</td>
      <td>让你飞词曲让你飞让你飞让你飞让你飞让你飞让你飞让你飞让你飞让你飞让你飞别再爱我一遍飞飞飞飞飞...</td>
      <td>00:00.00,00:22.27,00:44.55,01:06.82,01:09.54,0...</td>
      <td>c-pop,mandopop,taiwan pop</td>
      <td>108</td>
      <td>0.733333</td>
      <td>0.800000</td>
      <td>0.933333</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>c</td>
      <td>pop</td>
      <td>mandopop</td>
      <td>c-pop</td>
      <td>mandopop</td>
      <td>taiwan pop</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>https://open.spotify.com/track/2bsJYLBCUYSFpeZ...</td>
    </tr>
    <tr>
      <th>26688</th>
      <td>他有三百塊</td>
      <td>https://api.spotify.com/v1/artists/3mgkHRW7Rgk...</td>
      <td>Wayne's so Sad</td>
      <td>spotify:artist:3mgkHRW7Rgkbx2gJb5TPl7</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>31200.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/2Nz7WBzl819d...</td>
      <td>https://p.scdn.co/mp3-preview/74ab9859e7be3dc0...</td>
      <td>6.0</td>
      <td>track</td>
      <td>31200.0</td>
      <td>15.0</td>
      <td>我愛您</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b2733407b9...</td>
      <td>10.0</td>
      <td>spotify:album:587ID1VF5vNm7IDOCKZdYj</td>
      <td>2010-11-07</td>
      <td>0.000011</td>
      <td>0.208</td>
      <td>0.600</td>
      <td>0.000541</td>
      <td>0.1040</td>
      <td>-5.812</td>
      <td>0.1110</td>
      <td>172.854</td>
      <td>3.0</td>
      <td>他有三百块词曲他有三百块他有三百块他有三百块他有三百块</td>
      <td>00:00.00,00:02.67,00:05.34,00:08.02,00:09.76,0...</td>
      <td>chinese indie,taiwan indie,taiwan pop,taiwan punk</td>
      <td>27</td>
      <td>0.800000</td>
      <td>0.800000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>chinese</td>
      <td>indie</td>
      <td>taiwan</td>
      <td>chinese indie</td>
      <td>taiwan indie</td>
      <td>taiwan pop</td>
      <td>taiwan punk</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>https://open.spotify.com/track/2Nz7WBzl819dOJS...</td>
    </tr>
    <tr>
      <th>29775</th>
      <td>Shopping Mall</td>
      <td>https://api.spotify.com/v1/artists/7ruJU7jtsDq...</td>
      <td>Peggy Hsu</td>
      <td>spotify:artist:7ruJU7jtsDqbgA23BL3VFQ</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>235640.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/6kxGoPjoHGTt...</td>
      <td>https://p.scdn.co/mp3-preview/98d9160502497446...</td>
      <td>11.0</td>
      <td>track</td>
      <td>235640.0</td>
      <td>3.0</td>
      <td>氣球</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273477d4a...</td>
      <td>14.0</td>
      <td>spotify:album:5FrPCsBhMpYx964CRqrVoP</td>
      <td>2001-11-01</td>
      <td>0.018200</td>
      <td>0.689</td>
      <td>0.699</td>
      <td>0.001050</td>
      <td>0.8320</td>
      <td>-7.779</td>
      <td>0.1130</td>
      <td>91.023</td>
      <td>4.0</td>
      <td>到三五个好朋友或是一个人很自由到到</td>
      <td>00:00.00,00:45.15,00:47.78,00:50.40,00:55.72,0...</td>
      <td>chinese indie,taiwan pop,taiwan singer-songwriter</td>
      <td>17</td>
      <td>0.727273</td>
      <td>0.818182</td>
      <td>0.909091</td>
      <td>0.909091</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>chinese</td>
      <td>indie</td>
      <td>taiwan</td>
      <td>chinese indie</td>
      <td>taiwan pop</td>
      <td>taiwan singer-songwriter</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>https://open.spotify.com/track/6kxGoPjoHGTtv0V...</td>
    </tr>
    <tr>
      <th>34673</th>
      <td>我好飽</td>
      <td>https://api.spotify.com/v1/artists/5VjRKAwMi36...</td>
      <td>punkhoo</td>
      <td>spotify:artist:5VjRKAwMi363o2WQ7D509J</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>222264.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/2iuv8yLBMMl4...</td>
      <td>https://p.scdn.co/mp3-preview/555a2be924f63281...</td>
      <td>4.0</td>
      <td>track</td>
      <td>222264.0</td>
      <td>2.0</td>
      <td>大家好我們是胖虎</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b27306aaf8...</td>
      <td>13.0</td>
      <td>spotify:album:4D82oGvccRqM4iJwlC1EAj</td>
      <td>2009-09-01</td>
      <td>0.007200</td>
      <td>0.462</td>
      <td>0.967</td>
      <td>0.000007</td>
      <td>0.3160</td>
      <td>-3.135</td>
      <td>0.0786</td>
      <td>117.439</td>
      <td>4.0</td>
      <td>我好饱告诉我爸爸说看看我吃太多管他说我不想待在这里面我不想看看这世界塞满了你全身都是油油腻腻...</td>
      <td>00:00.00,00:21.64,00:24.27,00:25.07,00:27.92,0...</td>
      <td>NaN</td>
      <td>94</td>
      <td>0.714286</td>
      <td>0.750000</td>
      <td>0.821429</td>
      <td>0.892857</td>
      <td>0.892857</td>
      <td>1.000000</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>taiwan indie</td>
      <td>taiwan punk</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>https://open.spotify.com/track/2iuv8yLBMMl4uXq...</td>
    </tr>
    <tr>
      <th>35929</th>
      <td>你见过这样的星星吗?</td>
      <td>https://api.spotify.com/v1/artists/2VsTfId3QBJ...</td>
      <td>Casino Demon</td>
      <td>spotify:artist:2VsTfId3QBJggJThhZr8qc</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>208946.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/58kFyxYcBejq...</td>
      <td>https://p.scdn.co/mp3-preview/189949ddfffd951f...</td>
      <td>1.0</td>
      <td>track</td>
      <td>208946.0</td>
      <td>2.0</td>
      <td>你见过这样的星星吗?</td>
      <td>single</td>
      <td>https://i.scdn.co/image/ab67616d0000b2739135e6...</td>
      <td>6.0</td>
      <td>spotify:album:4enuDQoYdyclnthTJLLvXS</td>
      <td>2012-11-24</td>
      <td>0.024700</td>
      <td>0.648</td>
      <td>0.769</td>
      <td>0.000541</td>
      <td>0.2320</td>
      <td>-7.773</td>
      <td>0.0312</td>
      <td>131.128</td>
      <td>4.0</td>
      <td>你见过这样的星星吗</td>
      <td>00:00.00,00:30.44,00:33.54,00:36.13,00:41.36,0...</td>
      <td>NaN</td>
      <td>9</td>
      <td>0.857143</td>
      <td>0.857143</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>chinese indie rock</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>https://open.spotify.com/track/58kFyxYcBejqJyf...</td>
    </tr>
  </tbody>
</table>
</div>




```python
start = df_tracks_master_clean3_lyrics_genre_hsk_v2.loc[1088].lyrics
start
```




    '词曲我爱你她爱她你爱我他爱他咦已经没有人相爱怎么这世界怎么这世界不爱自己我爱你她爱她你爱我他爱他咦已经没有人相爱怎么这世界怎么这世界不爱自己咦已经没有人相爱怎么这世界怎么这世界不爱自己'




```python
list(set(SnowNLP(start).words))
```




    ['没有',
     '怎么',
     '自己',
     '词曲',
     '她爱',
     '咦',
     '不爱',
     '你',
     '人相',
     '这',
     '已经',
     '不',
     '爱',
     '我',
     '她',
     '他',
     '世界']




```python
df_test = df_tracks_master_clean3_lyrics_genre_hsk_v2.copy()
```


```python
def characters(x):
    return list(set(SnowNLP(x).words))
    
```


```python
df_test['character_unique'] = df_test.lyrics
```


```python
df_test['character_unique'] = df_test['character_unique'].apply(characters)
```


```python
df_test.loc[0].lyrics
```




    '英雄词曲人生不是一个人的游戏一起奋斗一起超越一起杀吧兄弟好战好胜战胜逆命管他天赋够不够我们都还需要再努力你的剑就是我的剑艾希的箭可不可以准一点你打野我来控兵线不要随便慌张就交闪现旋转跳跃你闭着眼卡特转完会让你闭上眼悟空盖伦也转圈圈盲僧李先生一脚把你击杀扛塔击杀迎接剑圣易大师的冥想就像紧箍咒的碎念问你出门有没带眼不是骂你有没长眼皇子嘉文不就是我黄金甲那傲气的脸让你画面一直呈现卓别林的黑白默片请不要在观战模式偷窥着我我的专情不可能被那阿狸魅惑别学我跳牛仔很忙好吗狗头不要动不动逆风你们就想投旋转跳跃你闭着眼卡特转完会让你闭上眼小鱼人再跳我就把你击杀扛塔击杀迎接击杀扛塔击杀迎接'




```python
df_test[df_test.hsk_level == 1].shape
```




    (6, 63)




```python
df_hsk2 = df_test[df_test.hsk_level == 2]
```


```python
df_hsk2 = df_hsk2.sort_values(by='hsk2', ascending=False)
```


```python
start = df_hsk2.character_unique.iloc[0]
```


```python




```

Start


```python

```


```python
calc_df = df_test.copy()
```


```python
# Create calc column
calc_df['calc'] = calc_df['character_unique']
```


```python

# create Branch column
calc_df['branch'] = 0

# create Node column
calc_df['node'] = 0

# create Previous Branch
calc_df['prev_branch'] = 0

# Cumulative characters
calc_df['cumulative_characters'] = 0
```


```python
song_order = pd.DataFrame()
```


```python
branch = 0
```


```python
node = 0
```


```python
prev_branch = 0
```


```python
# get starting song
calc_df = calc_df.sort_values(by='hsk1', ascending=False)
```


```python
# Calculate scores of all songs with comparison to fist song
comparable = calc_df['calc'].iloc[0]
for ind,i in enumerate(calc_df['calc']):
    calc_df['calc'].iloc[ind] = len(i) - len(set.intersection(set(comparable),set(i)))
calc_df = calc_df.sort_values(by=['calc', 'hsk3'])
```


```python
#next closest song (row 1 is the original song)
calc_df.sort_values(by=['calc', 'hsk3']).iloc[1]
```




    name                                                                   破浪潮
    artist_href              https://api.spotify.com/v1/artists/5qChpS8zymO...
    artist                                                            AV Okubo
    artist_uri                           spotify:artist:5qChpS8zymOfQup7IJNNDx
    artist_type                                                         artist
                                                   ...                        
    calc                                                                     2
    branch                                                                   0
    node                                                                     0
    prev_branch                                                              0
    cumulative_characters                                                    0
    Name: 35602, Length: 68, dtype: object




```python
calc_df['cumulative_characters'].iloc[0] = len(calc_df.character_unique.iloc[0])
calc_df['branch'].iloc[0] = branch
calc_df['node'].iloc[0] = node
previous_branch = 0
calc_df['prev_branch'].iloc[0] = previous_branch
```


```python
# put first song into song order
song_order = song_order.append(calc_df.iloc[0])
```


```python
cumulative_characters = len(calc_df.character_unique.iloc[0])
previous_branch = 0
```


```python
# remove first song from dataframe
calc_df = calc_df.iloc[1:]
```


```python

```

Repeat


```python
# Create calc column
calc_df['calc'] = calc_df['character_unique']
```


```python
# Calculate scores of all songs with comparison to fist song
comparable = calc_df['calc'].iloc[0]
for ind,i in enumerate(calc_df['calc']):
    calc_df['calc'].iloc[ind] = len(i) - len(set.intersection(set(comparable),set(i)))
calc_df = calc_df.sort_values(by='calc')
```


```python
#next closest song (row 1 is the original song)
calc_df.sort_values(by=['calc', 'hsk3']).iloc[1]
```




    name                                                          3010 - Intro
    artist_href              https://api.spotify.com/v1/artists/0f5x4P8Sybb...
    artist                                                            Da Mouth
    artist_uri                           spotify:artist:0f5x4P8SybbRdpVr4idUwd
    artist_type                                                         artist
                                                   ...                        
    calc                                                                     3
    branch                                                                   0
    node                                                                     0
    prev_branch                                                              0
    cumulative_characters                                                    0
    Name: 26794, Length: 68, dtype: object




```python
calc_df.iloc[1,63]
```




    3




```python
calc_df['cumulative_characters'].iloc[0] = cumulative_characters + len(calc_df.character_unique.iloc[0])

cumulative_characters = cumulative_characters + len(calc_df.character_unique.iloc[0])
if calc_df.iloc[1,63] == 0:
    calc_df['branch'].iloc[0] = branch
elif calc_df.iloc[1,63] != 0:
    branch += 1
    calc_df['branch'].iloc[0] = branch

node += 1
calc_df['node'].iloc[0] = node

calc_df['prev_branch'].iloc[0] = previous_branch
previous_branch +=1
```


```python
# put first song into song order
song_order = song_order.append(calc_df.iloc[0])
```


```python
# remove first song from dataframe
calc_df = calc_df.iloc[1:]
```


```python
song_order
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>artist_href</th>
      <th>artist</th>
      <th>artist_uri</th>
      <th>artist_type</th>
      <th>song_available_markets</th>
      <th>song_duration_ms</th>
      <th>is_explicit</th>
      <th>song_href</th>
      <th>preview_url</th>
      <th>track_number</th>
      <th>song_type</th>
      <th>length</th>
      <th>popularity</th>
      <th>album</th>
      <th>album_type</th>
      <th>album_artwork_link</th>
      <th>album_tracks_num</th>
      <th>album_uri</th>
      <th>album_release_date</th>
      <th>acousticness</th>
      <th>danceability</th>
      <th>energy</th>
      <th>instrumentalness</th>
      <th>liveness</th>
      <th>loudness</th>
      <th>speechiness</th>
      <th>tempo</th>
      <th>time_signature</th>
      <th>lyrics</th>
      <th>time_stamp</th>
      <th>genres</th>
      <th>lyrics_len</th>
      <th>hsk1</th>
      <th>hsk2</th>
      <th>hsk3</th>
      <th>hsk4</th>
      <th>hsk5</th>
      <th>hsk6</th>
      <th>hsk_level</th>
      <th>hsk_level_6_80p</th>
      <th>hsk_level_5_80p</th>
      <th>hsk_level_4_80p</th>
      <th>hsk_level_3_80p</th>
      <th>hsk_level_2_80p</th>
      <th>hsk_level_1_80p</th>
      <th>hsk_level_5_75p</th>
      <th>hsk_level_4_75p</th>
      <th>hsk_level_3_75p</th>
      <th>hsk_level_2_75p</th>
      <th>hsk_level_1_75p</th>
      <th>genre_0</th>
      <th>genre_1</th>
      <th>genre_2</th>
      <th>Genre1</th>
      <th>Genre2</th>
      <th>Genre3</th>
      <th>Genre4</th>
      <th>Genre5</th>
      <th>Genre6</th>
      <th>Genre7</th>
      <th>weblink</th>
      <th>character_unique</th>
      <th>calc</th>
      <th>branch</th>
      <th>node</th>
      <th>prev_branch</th>
      <th>cumulative_characters</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>35929</th>
      <td>你见过这样的星星吗?</td>
      <td>https://api.spotify.com/v1/artists/2VsTfId3QBJ...</td>
      <td>Casino Demon</td>
      <td>spotify:artist:2VsTfId3QBJggJThhZr8qc</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>208946.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/58kFyxYcBejq...</td>
      <td>https://p.scdn.co/mp3-preview/189949ddfffd951f...</td>
      <td>1.0</td>
      <td>track</td>
      <td>208946.0</td>
      <td>2.0</td>
      <td>你见过这样的星星吗?</td>
      <td>single</td>
      <td>https://i.scdn.co/image/ab67616d0000b2739135e6...</td>
      <td>6.0</td>
      <td>spotify:album:4enuDQoYdyclnthTJLLvXS</td>
      <td>2012-11-24</td>
      <td>0.024700</td>
      <td>0.648</td>
      <td>0.769</td>
      <td>0.000541</td>
      <td>0.2320</td>
      <td>-7.773</td>
      <td>0.0312</td>
      <td>131.128</td>
      <td>4.0</td>
      <td>你见过这样的星星吗</td>
      <td>00:00.00,00:30.44,00:33.54,00:36.13,00:41.36,0...</td>
      <td>NaN</td>
      <td>9.0</td>
      <td>0.857143</td>
      <td>0.857143</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>chinese indie rock</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>https://open.spotify.com/track/58kFyxYcBejqJyf...</td>
      <td>[见, 这样, 的, 你, 过, 吗, 星星]</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>7.0</td>
    </tr>
    <tr>
      <th>35602</th>
      <td>破浪潮</td>
      <td>https://api.spotify.com/v1/artists/5qChpS8zymO...</td>
      <td>AV Okubo</td>
      <td>spotify:artist:5qChpS8zymOfQup7IJNNDx</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>316613.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/0m7rQWrvkPQq...</td>
      <td>https://p.scdn.co/mp3-preview/55570e513812c654...</td>
      <td>4.0</td>
      <td>track</td>
      <td>316613.0</td>
      <td>2.0</td>
      <td>一品國際</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b2735d7a8b...</td>
      <td>9.0</td>
      <td>spotify:album:7dFkAqqJR01lUapdCBqSjW</td>
      <td>2014-04-08</td>
      <td>0.000012</td>
      <td>0.171</td>
      <td>0.901</td>
      <td>0.657000</td>
      <td>0.1410</td>
      <td>-5.292</td>
      <td>0.0549</td>
      <td>80.409</td>
      <td>4.0</td>
      <td>破浪潮破破破破破破破破破破破破破破破破破破破破破破破破破破破破破破破破破破破破破破破破破破破...</td>
      <td>00:00.00,00:27.31,00:29.24,00:30.05,00:32.00,0...</td>
      <td>NaN</td>
      <td>67.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>4.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>chinese indie rock</td>
      <td>chinese post-punk</td>
      <td>chinese punk</td>
      <td>wuhan indie</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>https://open.spotify.com/track/0m7rQWrvkPQqrvG...</td>
      <td>[浪潮, 破]</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>9.0</td>
    </tr>
    <tr>
      <th>6873</th>
      <td>You've Got A Friend - Live</td>
      <td>https://api.spotify.com/v1/artists/1YrtUPrWcPf...</td>
      <td>Khalil Fong</td>
      <td>spotify:artist:1YrtUPrWcPfgdl9BaD9nhd</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>324240.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/7a2BK0igYYkA...</td>
      <td>https://p.scdn.co/mp3-preview/ffbe9bb588f09b21...</td>
      <td>20.0</td>
      <td>track</td>
      <td>324240.0</td>
      <td>3.0</td>
      <td>This Love Live 2007</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273d1a7e0...</td>
      <td>21.0</td>
      <td>spotify:album:3YBppQ78Ffif9iwsPimGiA</td>
      <td>2006-12-31</td>
      <td>0.865000</td>
      <td>0.404</td>
      <td>0.184</td>
      <td>0.000000</td>
      <td>0.4580</td>
      <td>-13.164</td>
      <td>0.0587</td>
      <td>181.614</td>
      <td>4.0</td>
      <td>薛薛方薛方薛薛方薛方</td>
      <td>00:00.39,00:01.41,00:02.43,00:11.78,00:12.28,0...</td>
      <td>cantopop,chinese indie,chinese r&amp;b,mandopop</td>
      <td>10.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.5</td>
      <td>0.5</td>
      <td>0.5</td>
      <td>0.5</td>
      <td>6.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>cantopop</td>
      <td>chinese</td>
      <td>indie</td>
      <td>cantopop</td>
      <td>chinese indie</td>
      <td>chinese r&amp;b</td>
      <td>mandopop</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>https://open.spotify.com/track/7a2BK0igYYkAiFq...</td>
      <td>[薛, 方]</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>11.0</td>
    </tr>
    <tr>
      <th>1698</th>
      <td>Goodnight, Goodbye</td>
      <td>https://api.spotify.com/v1/artists/6noxsCszBEE...</td>
      <td>A-Mei Chang</td>
      <td>spotify:artist:6noxsCszBEEK04kCehugOp</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>235533.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/75xxcxy05MJ7...</td>
      <td>https://p.scdn.co/mp3-preview/86d28cb95e32239b...</td>
      <td>6.0</td>
      <td>track</td>
      <td>235533.0</td>
      <td>9.0</td>
      <td>我可以抱你嗎?愛人</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b27362c24e...</td>
      <td>12.0</td>
      <td>spotify:album:3gl8g9vcEMOkrZtWNN9Bqt</td>
      <td>1999-06-07</td>
      <td>0.025400</td>
      <td>0.738</td>
      <td>0.531</td>
      <td>0.000044</td>
      <td>0.0609</td>
      <td>-10.298</td>
      <td>0.0466</td>
      <td>127.992</td>
      <td>4.0</td>
      <td>宋黄宋黄宋黄宋黄</td>
      <td>00:00.28,00:00.63,00:01.07,00:15.11,00:15.52,0...</td>
      <td>c-pop,mandopop,taiwan pop</td>
      <td>8.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.5</td>
      <td>0.5</td>
      <td>0.5</td>
      <td>0.5</td>
      <td>6.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>c</td>
      <td>pop</td>
      <td>mandopop</td>
      <td>c-pop</td>
      <td>mandopop</td>
      <td>taiwan pop</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>https://open.spotify.com/track/75xxcxy05MJ7gwz...</td>
      <td>[宋, 黄]</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>13.0</td>
    </tr>
  </tbody>
</table>
</div>




```python

```


```python
for i in tqdm(range(100)):
    # Create calc column
    calc_df['calc'] = calc_df['character_unique']


    # Calculate scores of all songs with comparison to fist song
    comparable = calc_df['calc'].iloc[0]
    for ind,i in enumerate(calc_df['calc']):
        calc_df['calc'].iloc[ind] = len(i) - len(set.intersection(set(comparable),set(i)))
    calc_df = calc_df.sort_values(by='calc')


    # update cumulative characters in table and the counter
    calc_df['cumulative_characters'].iloc[0] = cumulative_characters + len(calc_df.character_unique.iloc[0])
    cumulative_characters = cumulative_characters + len(calc_df.character_unique.iloc[0])

    # update network details
    if calc_df.iloc[1,63] == 0:
        calc_df['branch'].iloc[0] = branch
    elif calc_df.iloc[1,63] != 0:
        branch += 1
        calc_df['branch'].iloc[0] = branch

    node += 1
    calc_df['node'].iloc[0] = node

    calc_df['prev_branch'].iloc[0] = previous_branch
    previous_branch +=1

    # put first song into song order
    song_order = song_order.append(calc_df.iloc[0])

    # remove first song from dataframe
    calc_df = calc_df.iloc[1:]

song_order
```

    100%|█████████████████████████████████████████| 100/100 [19:23<00:00, 11.64s/it]





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>artist_href</th>
      <th>artist</th>
      <th>artist_uri</th>
      <th>artist_type</th>
      <th>song_available_markets</th>
      <th>song_duration_ms</th>
      <th>is_explicit</th>
      <th>song_href</th>
      <th>preview_url</th>
      <th>track_number</th>
      <th>song_type</th>
      <th>length</th>
      <th>popularity</th>
      <th>album</th>
      <th>album_type</th>
      <th>album_artwork_link</th>
      <th>album_tracks_num</th>
      <th>album_uri</th>
      <th>album_release_date</th>
      <th>acousticness</th>
      <th>danceability</th>
      <th>energy</th>
      <th>instrumentalness</th>
      <th>liveness</th>
      <th>loudness</th>
      <th>speechiness</th>
      <th>tempo</th>
      <th>time_signature</th>
      <th>lyrics</th>
      <th>time_stamp</th>
      <th>genres</th>
      <th>lyrics_len</th>
      <th>hsk1</th>
      <th>hsk2</th>
      <th>hsk3</th>
      <th>hsk4</th>
      <th>hsk5</th>
      <th>hsk6</th>
      <th>hsk_level</th>
      <th>hsk_level_6_80p</th>
      <th>hsk_level_5_80p</th>
      <th>hsk_level_4_80p</th>
      <th>hsk_level_3_80p</th>
      <th>hsk_level_2_80p</th>
      <th>hsk_level_1_80p</th>
      <th>hsk_level_5_75p</th>
      <th>hsk_level_4_75p</th>
      <th>hsk_level_3_75p</th>
      <th>hsk_level_2_75p</th>
      <th>hsk_level_1_75p</th>
      <th>genre_0</th>
      <th>genre_1</th>
      <th>genre_2</th>
      <th>Genre1</th>
      <th>Genre2</th>
      <th>Genre3</th>
      <th>Genre4</th>
      <th>Genre5</th>
      <th>Genre6</th>
      <th>Genre7</th>
      <th>weblink</th>
      <th>character_unique</th>
      <th>calc</th>
      <th>branch</th>
      <th>node</th>
      <th>prev_branch</th>
      <th>cumulative_characters</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>35929</th>
      <td>你见过这样的星星吗?</td>
      <td>https://api.spotify.com/v1/artists/2VsTfId3QBJ...</td>
      <td>Casino Demon</td>
      <td>spotify:artist:2VsTfId3QBJggJThhZr8qc</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>208946.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/58kFyxYcBejq...</td>
      <td>https://p.scdn.co/mp3-preview/189949ddfffd951f...</td>
      <td>1.0</td>
      <td>track</td>
      <td>208946.0</td>
      <td>2.0</td>
      <td>你见过这样的星星吗?</td>
      <td>single</td>
      <td>https://i.scdn.co/image/ab67616d0000b2739135e6...</td>
      <td>6.0</td>
      <td>spotify:album:4enuDQoYdyclnthTJLLvXS</td>
      <td>2012-11-24</td>
      <td>0.024700</td>
      <td>0.648</td>
      <td>0.7690</td>
      <td>0.000541</td>
      <td>0.2320</td>
      <td>-7.773</td>
      <td>0.0312</td>
      <td>131.128</td>
      <td>4.0</td>
      <td>你见过这样的星星吗</td>
      <td>00:00.00,00:30.44,00:33.54,00:36.13,00:41.36,0...</td>
      <td>NaN</td>
      <td>9.0</td>
      <td>0.857143</td>
      <td>0.857143</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>chinese indie rock</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>https://open.spotify.com/track/58kFyxYcBejqJyf...</td>
      <td>[见, 这样, 的, 你, 过, 吗, 星星]</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>7.0</td>
    </tr>
    <tr>
      <th>35602</th>
      <td>破浪潮</td>
      <td>https://api.spotify.com/v1/artists/5qChpS8zymO...</td>
      <td>AV Okubo</td>
      <td>spotify:artist:5qChpS8zymOfQup7IJNNDx</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>316613.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/0m7rQWrvkPQq...</td>
      <td>https://p.scdn.co/mp3-preview/55570e513812c654...</td>
      <td>4.0</td>
      <td>track</td>
      <td>316613.0</td>
      <td>2.0</td>
      <td>一品國際</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b2735d7a8b...</td>
      <td>9.0</td>
      <td>spotify:album:7dFkAqqJR01lUapdCBqSjW</td>
      <td>2014-04-08</td>
      <td>0.000012</td>
      <td>0.171</td>
      <td>0.9010</td>
      <td>0.657000</td>
      <td>0.1410</td>
      <td>-5.292</td>
      <td>0.0549</td>
      <td>80.409</td>
      <td>4.0</td>
      <td>破浪潮破破破破破破破破破破破破破破破破破破破破破破破破破破破破破破破破破破破破破破破破破破破...</td>
      <td>00:00.00,00:27.31,00:29.24,00:30.05,00:32.00,0...</td>
      <td>NaN</td>
      <td>67.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>4.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>chinese indie rock</td>
      <td>chinese post-punk</td>
      <td>chinese punk</td>
      <td>wuhan indie</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>https://open.spotify.com/track/0m7rQWrvkPQqrvG...</td>
      <td>[浪潮, 破]</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>9.0</td>
    </tr>
    <tr>
      <th>6873</th>
      <td>You've Got A Friend - Live</td>
      <td>https://api.spotify.com/v1/artists/1YrtUPrWcPf...</td>
      <td>Khalil Fong</td>
      <td>spotify:artist:1YrtUPrWcPfgdl9BaD9nhd</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>324240.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/7a2BK0igYYkA...</td>
      <td>https://p.scdn.co/mp3-preview/ffbe9bb588f09b21...</td>
      <td>20.0</td>
      <td>track</td>
      <td>324240.0</td>
      <td>3.0</td>
      <td>This Love Live 2007</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273d1a7e0...</td>
      <td>21.0</td>
      <td>spotify:album:3YBppQ78Ffif9iwsPimGiA</td>
      <td>2006-12-31</td>
      <td>0.865000</td>
      <td>0.404</td>
      <td>0.1840</td>
      <td>0.000000</td>
      <td>0.4580</td>
      <td>-13.164</td>
      <td>0.0587</td>
      <td>181.614</td>
      <td>4.0</td>
      <td>薛薛方薛方薛薛方薛方</td>
      <td>00:00.39,00:01.41,00:02.43,00:11.78,00:12.28,0...</td>
      <td>cantopop,chinese indie,chinese r&amp;b,mandopop</td>
      <td>10.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.500000</td>
      <td>0.500000</td>
      <td>0.500000</td>
      <td>0.500000</td>
      <td>6.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>cantopop</td>
      <td>chinese</td>
      <td>indie</td>
      <td>cantopop</td>
      <td>chinese indie</td>
      <td>chinese r&amp;b</td>
      <td>mandopop</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>https://open.spotify.com/track/7a2BK0igYYkAiFq...</td>
      <td>[薛, 方]</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>11.0</td>
    </tr>
    <tr>
      <th>1698</th>
      <td>Goodnight, Goodbye</td>
      <td>https://api.spotify.com/v1/artists/6noxsCszBEE...</td>
      <td>A-Mei Chang</td>
      <td>spotify:artist:6noxsCszBEEK04kCehugOp</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>235533.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/75xxcxy05MJ7...</td>
      <td>https://p.scdn.co/mp3-preview/86d28cb95e32239b...</td>
      <td>6.0</td>
      <td>track</td>
      <td>235533.0</td>
      <td>9.0</td>
      <td>我可以抱你嗎?愛人</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b27362c24e...</td>
      <td>12.0</td>
      <td>spotify:album:3gl8g9vcEMOkrZtWNN9Bqt</td>
      <td>1999-06-07</td>
      <td>0.025400</td>
      <td>0.738</td>
      <td>0.5310</td>
      <td>0.000044</td>
      <td>0.0609</td>
      <td>-10.298</td>
      <td>0.0466</td>
      <td>127.992</td>
      <td>4.0</td>
      <td>宋黄宋黄宋黄宋黄</td>
      <td>00:00.28,00:00.63,00:01.07,00:15.11,00:15.52,0...</td>
      <td>c-pop,mandopop,taiwan pop</td>
      <td>8.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.500000</td>
      <td>0.500000</td>
      <td>0.500000</td>
      <td>0.500000</td>
      <td>6.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>c</td>
      <td>pop</td>
      <td>mandopop</td>
      <td>c-pop</td>
      <td>mandopop</td>
      <td>taiwan pop</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>https://open.spotify.com/track/75xxcxy05MJ7gwz...</td>
      <td>[宋, 黄]</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>13.0</td>
    </tr>
    <tr>
      <th>26794</th>
      <td>3010 - Intro</td>
      <td>https://api.spotify.com/v1/artists/0f5x4P8Sybb...</td>
      <td>Da Mouth</td>
      <td>spotify:artist:0f5x4P8SybbRdpVr4idUwd</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>92733.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/2lb9xUEYMCS2...</td>
      <td>NaN</td>
      <td>1.0</td>
      <td>track</td>
      <td>92733.0</td>
      <td>4.0</td>
      <td>万凸3</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b2733d8023...</td>
      <td>12.0</td>
      <td>spotify:album:2tXAhfX4fIqnqeOEbAHPdg</td>
      <td>2010-01-01</td>
      <td>0.016400</td>
      <td>0.716</td>
      <td>0.9310</td>
      <td>0.000161</td>
      <td>0.1500</td>
      <td>-6.126</td>
      <td>0.0725</td>
      <td>100.003</td>
      <td>4.0</td>
      <td>结果咧曲结果咧结果咧结果咧结果咧结果咧结果咧结果咧结果咧结果咧结果咧结果咧结果咧结果咧结果咧...</td>
      <td>00:00.00,00:10.47,00:20.95,00:23.10,00:25.42,0...</td>
      <td>NaN</td>
      <td>76.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.666667</td>
      <td>0.666667</td>
      <td>0.666667</td>
      <td>6.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>mandopop</td>
      <td>taiwan pop</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>https://open.spotify.com/track/2lb9xUEYMCS2AI8...</td>
      <td>[结果, 咧, 咧曲]</td>
      <td>0.0</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>3.0</td>
      <td>16.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>34005</th>
      <td>一起來跳舞</td>
      <td>https://api.spotify.com/v1/artists/7tYJYd9Xr6O...</td>
      <td>八十八顆芭樂籽</td>
      <td>spotify:artist:7tYJYd9Xr6O4I0uQE2ZGd0</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>295426.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/5zJDfAfUSp6X...</td>
      <td>https://p.scdn.co/mp3-preview/fc85f4347410774f...</td>
      <td>3.0</td>
      <td>track</td>
      <td>295426.0</td>
      <td>8.0</td>
      <td>比獸還壞</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273b3af10...</td>
      <td>11.0</td>
      <td>spotify:album:2JCWudnCnaaFjcxQif7bqM</td>
      <td>2010-08-08</td>
      <td>0.076200</td>
      <td>0.572</td>
      <td>0.8670</td>
      <td>0.000009</td>
      <td>0.0959</td>
      <td>-5.814</td>
      <td>0.0620</td>
      <td>173.887</td>
      <td>4.0</td>
      <td>词曲中常識子笑芸人常識</td>
      <td>00:00.51,00:05.80,00:07.23,00:14.30,00:17.65,0...</td>
      <td>NaN</td>
      <td>11.0</td>
      <td>0.285714</td>
      <td>0.571429</td>
      <td>0.714286</td>
      <td>0.714286</td>
      <td>0.714286</td>
      <td>0.714286</td>
      <td>6.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>taiwan indie</td>
      <td>taiwan pop</td>
      <td>taiwan punk</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>https://open.spotify.com/track/5zJDfAfUSp6X0Gp...</td>
      <td>[識子, 識, 词曲, 笑, 常, 中, 芸, 人]</td>
      <td>0.0</td>
      <td>138.0</td>
      <td>139.0</td>
      <td>138.0</td>
      <td>849.0</td>
    </tr>
    <tr>
      <th>11324</th>
      <td>邪童謠 / The Song Of Experience</td>
      <td>https://api.spotify.com/v1/artists/0u3m5Sy2zsq...</td>
      <td>Serrini</td>
      <td>spotify:artist:0u3m5Sy2zsq4Gk0aduH9s7</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>338693.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/5VJAnw1JLLxQ...</td>
      <td>https://p.scdn.co/mp3-preview/3fc650863a873b62...</td>
      <td>10.0</td>
      <td>track</td>
      <td>338693.0</td>
      <td>20.0</td>
      <td>邪童謠 / Songs of Experience</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273981aa4...</td>
      <td>11.0</td>
      <td>spotify:album:2R59zEBdvbupMAfB5GTrTL</td>
      <td>2019-04-04</td>
      <td>0.601000</td>
      <td>0.288</td>
      <td>0.0613</td>
      <td>0.914000</td>
      <td>0.1310</td>
      <td>-29.655</td>
      <td>0.0351</td>
      <td>62.183</td>
      <td>3.0</td>
      <td>未经许可不得翻唱或使用</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>11.0</td>
      <td>0.142857</td>
      <td>0.285714</td>
      <td>0.428571</td>
      <td>0.714286</td>
      <td>0.857143</td>
      <td>1.000000</td>
      <td>5.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>cantopop</td>
      <td>hong kong indie</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>https://open.spotify.com/track/5VJAnw1JLLxQDUU...</td>
      <td>[不得, 许可, 使用, 翻, 未经, 唱, 或]</td>
      <td>0.0</td>
      <td>138.0</td>
      <td>140.0</td>
      <td>139.0</td>
      <td>856.0</td>
    </tr>
    <tr>
      <th>34719</th>
      <td>Song of Wine (Jiushen Qu)</td>
      <td>https://api.spotify.com/v1/artists/0LyfQWJT6nX...</td>
      <td>Various Artists</td>
      <td>spotify:artist:5ce0OfrGcsuv6PQgkd8iVM</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>168133.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/5PBC6BTuhDug...</td>
      <td>https://p.scdn.co/mp3-preview/4386ce6e29fc3db7...</td>
      <td>7.0</td>
      <td>track</td>
      <td>168133.0</td>
      <td>0.0</td>
      <td>Chinese Hits From the 1980’s: The Rebirth (Zho...</td>
      <td>compilation</td>
      <td>https://i.scdn.co/image/ab67616d0000b27342d606...</td>
      <td>15.0</td>
      <td>spotify:album:1rJReqgHie4zmkEDszHqn3</td>
      <td>2007-01-01</td>
      <td>0.314000</td>
      <td>0.661</td>
      <td>0.6060</td>
      <td>0.000004</td>
      <td>0.3270</td>
      <td>-13.137</td>
      <td>0.0612</td>
      <td>99.880</td>
      <td>4.0</td>
      <td>未经许可不得翻唱或使用未经许可不得翻唱或使用</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>22.0</td>
      <td>0.142857</td>
      <td>0.285714</td>
      <td>0.428571</td>
      <td>0.714286</td>
      <td>0.857143</td>
      <td>1.000000</td>
      <td>5.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>https://open.spotify.com/track/5PBC6BTuhDugEm9...</td>
      <td>[不得, 许可, 使用, 翻, 未经, 唱, 或]</td>
      <td>0.0</td>
      <td>139.0</td>
      <td>141.0</td>
      <td>140.0</td>
      <td>863.0</td>
    </tr>
    <tr>
      <th>9171</th>
      <td>到時候再說</td>
      <td>https://api.spotify.com/v1/artists/7fCFxj1GCRq...</td>
      <td>Will Pan</td>
      <td>spotify:artist:7fCFxj1GCRqwFZEP4iJRw0</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>265600.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/4XKyE3CDuR5K...</td>
      <td>NaN</td>
      <td>10.0</td>
      <td>track</td>
      <td>265600.0</td>
      <td>6.0</td>
      <td>我的麥克風</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b2735fc42e...</td>
      <td>10.0</td>
      <td>spotify:album:4MYmTwirh8mfdWZxVa9mgX</td>
      <td>2003-01-01</td>
      <td>0.296000</td>
      <td>0.795</td>
      <td>0.5420</td>
      <td>0.000000</td>
      <td>0.1230</td>
      <td>-9.787</td>
      <td>0.0578</td>
      <td>120.035</td>
      <td>4.0</td>
      <td>韩文词曲和声混声未经许可</td>
      <td>00:00.50,00:02.01,00:03.01,00:03.77,00:05.17,0...</td>
      <td>NaN</td>
      <td>12.0</td>
      <td>0.125000</td>
      <td>0.125000</td>
      <td>0.500000</td>
      <td>0.500000</td>
      <td>0.625000</td>
      <td>0.875000</td>
      <td>6.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>mandopop</td>
      <td>taiwan pop</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>https://open.spotify.com/track/4XKyE3CDuR5KP9U...</td>
      <td>[文, 混声, 韩, 词曲, 许可, 和, 未经, 声]</td>
      <td>0.0</td>
      <td>140.0</td>
      <td>142.0</td>
      <td>141.0</td>
      <td>871.0</td>
    </tr>
    <tr>
      <th>31892</th>
      <td>心臟痛</td>
      <td>https://api.spotify.com/v1/artists/74aw0Er4aZT...</td>
      <td>問題總部 It's Your Fault</td>
      <td>spotify:artist:74aw0Er4aZTPzPcpnFwlBF</td>
      <td>artist</td>
      <td>['AD', 'AE', 'AG', 'AL', 'AM', 'AO', 'AR', 'AT...</td>
      <td>305585.0</td>
      <td>0.0</td>
      <td>https://api.spotify.com/v1/tracks/6pAg28t4SnCw...</td>
      <td>https://p.scdn.co/mp3-preview/013988190617e04f...</td>
      <td>6.0</td>
      <td>track</td>
      <td>305585.0</td>
      <td>32.0</td>
      <td>User Guide: I</td>
      <td>album</td>
      <td>https://i.scdn.co/image/ab67616d0000b273d1adbc...</td>
      <td>8.0</td>
      <td>spotify:album:3GzmfnXiRSZrI3c7PkqXCE</td>
      <td>2020-05-28</td>
      <td>0.907000</td>
      <td>0.711</td>
      <td>0.1770</td>
      <td>0.000452</td>
      <td>0.3150</td>
      <td>-15.837</td>
      <td>0.0376</td>
      <td>82.987</td>
      <td>4.0</td>
      <td>心脏痛词曲编曲制作和声和声编写</td>
      <td>00:00.00,00:04.10,00:05.58,00:08.49,00:10.80,0...</td>
      <td>NaN</td>
      <td>15.0</td>
      <td>0.250000</td>
      <td>0.250000</td>
      <td>0.500000</td>
      <td>0.500000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>5.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>chinese indie</td>
      <td>taiwan indie</td>
      <td>taiwan pop</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>https://open.spotify.com/track/6pAg28t4SnCwh8G...</td>
      <td>[和声, 编写, 痛, 心脏, 词曲, 编曲, 和, 声, 制作]</td>
      <td>0.0</td>
      <td>141.0</td>
      <td>143.0</td>
      <td>142.0</td>
      <td>880.0</td>
    </tr>
  </tbody>
</table>
<p>144 rows × 68 columns</p>
</div>




```python

```

 ## 12. Ideas Pad


```python

```
